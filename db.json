{"meta":{"version":1,"warehouse":"1.0.2"},"models":{"Asset":[{"_id":"themes/apollo/source/scss/apollo.scss","path":"scss/apollo.scss","modified":1},{"_id":"themes/apollo/source/gen.sh","path":"gen.sh","modified":1},{"_id":"themes/apollo/source/favicon.png","path":"favicon.png","modified":1},{"_id":"themes/apollo/source/css/apollo.css.map","path":"css/apollo.css.map","modified":1},{"_id":"themes/apollo/source/css/apollo.css","path":"css/apollo.css","modified":1},{"_id":"source/CNAME","path":"CNAME","modified":1},{"_id":"source/upload/java-at-alibaba.pptx","path":"upload/java-at-alibaba.pptx","modified":1},{"_id":"source/images/wpid-nio-oio.jpg","path":"images/wpid-nio-oio.jpg","modified":1},{"_id":"source/images/wpid-Netty-thread-model3.png","path":"images/wpid-Netty-thread-model3.png","modified":1},{"_id":"source/images/wpid-Netty-ChannelPipeline-.png","path":"images/wpid-Netty-ChannelPipeline-.png","modified":1},{"_id":"source/images/wpid-Multi-reactors3.png","path":"images/wpid-Multi-reactors3.png","modified":1},{"_id":"source/images/wpid-Channel.png","path":"images/wpid-Channel.png","modified":1},{"_id":"source/images/solr-DocSet.png","path":"images/solr-DocSet.png","modified":1},{"_id":"source/images/reactors-in-threads.png","path":"images/reactors-in-threads.png","modified":1},{"_id":"source/images/reactors-in-threads-thread-pool.png","path":"images/reactors-in-threads-thread-pool.png","modified":1},{"_id":"source/images/reactor-thread-pool.png","path":"images/reactor-thread-pool.png","modified":1},{"_id":"source/images/reactor-single-thread.png","path":"images/reactor-single-thread.png","modified":1},{"_id":"source/images/ranksystem-in-social-network.png","path":"images/ranksystem-in-social-network.png","modified":1},{"_id":"source/images/facet-1.png","path":"images/facet-1.png","modified":1},{"_id":"source/images/bytebuf-priciple.png","path":"images/bytebuf-priciple.png","modified":1},{"_id":"source/images/bytebuf-diagram.jpg","path":"images/bytebuf-diagram.jpg","modified":1},{"_id":"source/images/bytebuf-combine-slice-buffer.png","path":"images/bytebuf-combine-slice-buffer.png","modified":1},{"_id":"source/images/2014/05/reactors-in-threads.png","path":"images/2014/05/reactors-in-threads.png","modified":1},{"_id":"source/images/2014/05/reactors-in-threads-thread-pool.png","path":"images/2014/05/reactors-in-threads-thread-pool.png","modified":1},{"_id":"source/images/2014/05/reactor-thread-pool.png","path":"images/2014/05/reactor-thread-pool.png","modified":1},{"_id":"source/images/2014/05/reactor-single-thread.png","path":"images/2014/05/reactor-single-thread.png","modified":1},{"_id":"source/images/2014/01/wpid-nio-oio1.jpg","path":"images/2014/01/wpid-nio-oio1.jpg","modified":1},{"_id":"source/images/2014/01/wpid-nio-oio.jpg","path":"images/2014/01/wpid-nio-oio.jpg","modified":1},{"_id":"source/images/2014/01/wpid-diagram1.jpg","path":"images/2014/01/wpid-diagram1.jpg","modified":1},{"_id":"source/images/2014/01/wpid-diagram.jpg","path":"images/2014/01/wpid-diagram.jpg","modified":1},{"_id":"source/images/2014/01/wpid-combine-slice-buffer.png","path":"images/2014/01/wpid-combine-slice-buffer.png","modified":1},{"_id":"source/images/2014/01/wpid-bytebuf.png","path":"images/2014/01/wpid-bytebuf.png","modified":1},{"_id":"source/images/2014/01/wpid-Netty-thread-model3.png","path":"images/2014/01/wpid-Netty-thread-model3.png","modified":1},{"_id":"source/images/2014/01/wpid-Netty-thread-model2.png","path":"images/2014/01/wpid-Netty-thread-model2.png","modified":1},{"_id":"source/images/2014/01/wpid-Netty-thread-model1.png","path":"images/2014/01/wpid-Netty-thread-model1.png","modified":1},{"_id":"source/images/2014/01/wpid-Netty-thread-model.png","path":"images/2014/01/wpid-Netty-thread-model.png","modified":1},{"_id":"source/images/2014/01/wpid-Netty-ChannelPipeline-1.png","path":"images/2014/01/wpid-Netty-ChannelPipeline-1.png","modified":1},{"_id":"source/images/2014/01/wpid-Netty-ChannelPipeline-.png","path":"images/2014/01/wpid-Netty-ChannelPipeline-.png","modified":1},{"_id":"source/images/2014/01/wpid-Multi-reactors3.png","path":"images/2014/01/wpid-Multi-reactors3.png","modified":1},{"_id":"source/images/2014/01/wpid-Multi-reactors2.png","path":"images/2014/01/wpid-Multi-reactors2.png","modified":1},{"_id":"source/images/2014/01/wpid-Multi-reactors1.png","path":"images/2014/01/wpid-Multi-reactors1.png","modified":1},{"_id":"source/images/2014/01/wpid-Multi-reactors.png","path":"images/2014/01/wpid-Multi-reactors.png","modified":1},{"_id":"source/images/2014/01/wpid-Channel.png","path":"images/2014/01/wpid-Channel.png","modified":1},{"_id":"source/images/2014/01/icon1.png","path":"images/2014/01/icon1.png","modified":1},{"_id":"source/images/2014/01/icon.png","path":"images/2014/01/icon.png","modified":1},{"_id":"source/images/2014/01/favicon.png","path":"images/2014/01/favicon.png","modified":1},{"_id":"source/images/2014/01/e.png","path":"images/2014/01/e.png","modified":1},{"_id":"source/images/2013/10/Image_thumb.png","path":"images/2013/10/Image_thumb.png","modified":1},{"_id":"source/images/2013/10/Image4_thumb.png","path":"images/2013/10/Image4_thumb.png","modified":1},{"_id":"source/images/2013/10/Image4.png","path":"images/2013/10/Image4.png","modified":1},{"_id":"source/images/2013/10/Image3_thumb.png","path":"images/2013/10/Image3_thumb.png","modified":1},{"_id":"source/images/2013/10/Image3.png","path":"images/2013/10/Image3.png","modified":1},{"_id":"source/images/2013/10/Image2_thumb.png","path":"images/2013/10/Image2_thumb.png","modified":1},{"_id":"source/images/2013/10/Image2.png","path":"images/2013/10/Image2.png","modified":1},{"_id":"source/images/2013/10/Image1_thumb.png","path":"images/2013/10/Image1_thumb.png","modified":1},{"_id":"source/images/2013/10/Image1.png","path":"images/2013/10/Image1.png","modified":1},{"_id":"source/images/2013/10/Image.png","path":"images/2013/10/Image.png","modified":1},{"_id":"source/images/2013/08/image_thumb.png","path":"images/2013/08/image_thumb.png","modified":1},{"_id":"source/images/2013/08/image.png","path":"images/2013/08/image.png","modified":1},{"_id":"source/images/2013/08/QQ20130831103315_thumb.jpg","path":"images/2013/08/QQ20130831103315_thumb.jpg","modified":1},{"_id":"source/images/2013/08/QQ20130831103315.jpg","path":"images/2013/08/QQ20130831103315.jpg","modified":1},{"_id":"source/images/2013/04/DocSet_thumb.png","path":"images/2013/04/DocSet_thumb.png","modified":1},{"_id":"source/images/2013/04/DocSet.png","path":"images/2013/04/DocSet.png","modified":1},{"_id":"source/images/2013/03/thumb.jpg","path":"images/2013/03/thumb.jpg","modified":1},{"_id":"source/images/2013/03/image_thumb1.png","path":"images/2013/03/image_thumb1.png","modified":1},{"_id":"source/images/2013/03/image_thumb.png","path":"images/2013/03/image_thumb.png","modified":1},{"_id":"source/images/2013/03/image1.png","path":"images/2013/03/image1.png","modified":1},{"_id":"source/images/2013/03/image.png","path":"images/2013/03/image.png","modified":1},{"_id":"source/images/2013/03/POqWJ.png","path":"images/2013/03/POqWJ.png","modified":1},{"_id":"source/images/2013/03/201201051.png","path":"images/2013/03/201201051.png","modified":1},{"_id":"source/images/2013/03/1_thumb.jpg","path":"images/2013/03/1_thumb.jpg","modified":1},{"_id":"source/images/2013/03/163306e8e326.jpg","path":"images/2013/03/163306e8e326.jpg","modified":1},{"_id":"source/images/2013/03/12.jpg","path":"images/2013/03/12.jpg","modified":1},{"_id":"source/images/2013/03/11.jpg","path":"images/2013/03/11.jpg","modified":1},{"_id":"source/images/2013/03/1.jpg","path":"images/2013/03/1.jpg","modified":1},{"_id":"source/images/2013/01/image_thumb.png","path":"images/2013/01/image_thumb.png","modified":1},{"_id":"source/images/2013/01/image.png","path":"images/2013/01/image.png","modified":1},{"_id":"source/images/2013/01/SavedPicture-2013123102218.jpg","path":"images/2013/01/SavedPicture-2013123102218.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0074_thumb1.jpg","path":"images/2012/10/clip_image0074_thumb1.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0074_thumb.jpg","path":"images/2012/10/clip_image0074_thumb.jpg","modified":1},{"_id":"source/images/2012/10/clip_image00741.jpg","path":"images/2012/10/clip_image00741.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0074.jpg","path":"images/2012/10/clip_image0074.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0054_thumb.jpg","path":"images/2012/10/clip_image0054_thumb.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0054.jpg","path":"images/2012/10/clip_image0054.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0034_thumb.jpg","path":"images/2012/10/clip_image0034_thumb.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0034.jpg","path":"images/2012/10/clip_image0034.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0014_thumb.jpg","path":"images/2012/10/clip_image0014_thumb.jpg","modified":1},{"_id":"source/images/2012/10/clip_image0014.jpg","path":"images/2012/10/clip_image0014.jpg","modified":1},{"_id":"source/images/2012/10/Image_thumb2.jpg","path":"images/2012/10/Image_thumb2.jpg","modified":1},{"_id":"source/images/2012/10/Image_thumb1.jpg","path":"images/2012/10/Image_thumb1.jpg","modified":1},{"_id":"source/images/2012/10/Image_thumb.jpg","path":"images/2012/10/Image_thumb.jpg","modified":1},{"_id":"source/images/2012/10/Image2.jpg","path":"images/2012/10/Image2.jpg","modified":1},{"_id":"source/images/2012/10/Image1.jpg","path":"images/2012/10/Image1.jpg","modified":1},{"_id":"source/images/2012/10/Image.jpg","path":"images/2012/10/Image.jpg","modified":1},{"_id":"source/images/2012/09/YARNArch_thumb1.png","path":"images/2012/09/YARNArch_thumb1.png","modified":1},{"_id":"source/images/2012/09/YARNArch_thumb.png","path":"images/2012/09/YARNArch_thumb.png","modified":1},{"_id":"source/images/2012/09/YARNArch.png","path":"images/2012/09/YARNArch.png","modified":1},{"_id":"source/images/2012/09/Image_thumb1.png","path":"images/2012/09/Image_thumb1.png","modified":1},{"_id":"source/images/2012/09/Image_thumb.png","path":"images/2012/09/Image_thumb.png","modified":1},{"_id":"source/images/2012/09/Image.png","path":"images/2012/09/Image.png","modified":1},{"_id":"source/images/2012/07/BigTablebasedRangePartitioning_thumb.png","path":"images/2012/07/BigTablebasedRangePartitioning_thumb.png","modified":1},{"_id":"source/images/2012/07/BigTablebasedRangePartitioning.png","path":"images/2012/07/BigTablebasedRangePartitioning.png","modified":1},{"_id":"source/images/2012/07/ADistributedHashTableRing_thumb1.png","path":"images/2012/07/ADistributedHashTableRing_thumb1.png","modified":1},{"_id":"source/images/2012/07/ADistributedHashTableRing_thumb.png","path":"images/2012/07/ADistributedHashTableRing_thumb.png","modified":1},{"_id":"source/images/2012/07/ADistributedHashTableRing.png","path":"images/2012/07/ADistributedHashTableRing.png","modified":1},{"_id":"source/images/2012/06/clip_image017_thumb.jpg","path":"images/2012/06/clip_image017_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image017.jpg","path":"images/2012/06/clip_image017.jpg","modified":1},{"_id":"source/images/2012/06/clip_image015_thumb.jpg","path":"images/2012/06/clip_image015_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image015.jpg","path":"images/2012/06/clip_image015.jpg","modified":1},{"_id":"source/images/2012/06/clip_image013_thumb.jpg","path":"images/2012/06/clip_image013_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image013.jpg","path":"images/2012/06/clip_image013.jpg","modified":1},{"_id":"source/images/2012/06/clip_image011_thumb.jpg","path":"images/2012/06/clip_image011_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image011.jpg","path":"images/2012/06/clip_image011.jpg","modified":1},{"_id":"source/images/2012/06/clip_image009_thumb.jpg","path":"images/2012/06/clip_image009_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image009.jpg","path":"images/2012/06/clip_image009.jpg","modified":1},{"_id":"source/images/2012/06/clip_image007_thumb.jpg","path":"images/2012/06/clip_image007_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image007.jpg","path":"images/2012/06/clip_image007.jpg","modified":1},{"_id":"source/images/2012/06/clip_image005_thumb.jpg","path":"images/2012/06/clip_image005_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image005.jpg","path":"images/2012/06/clip_image005.jpg","modified":1},{"_id":"source/images/2012/06/clip_image003_thumb1.jpg","path":"images/2012/06/clip_image003_thumb1.jpg","modified":1},{"_id":"source/images/2012/06/clip_image003_thumb.jpg","path":"images/2012/06/clip_image003_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image003.jpg","path":"images/2012/06/clip_image003.jpg","modified":1},{"_id":"source/images/2012/06/clip_image001_thumb.jpg","path":"images/2012/06/clip_image001_thumb.jpg","modified":1},{"_id":"source/images/2012/06/clip_image001.jpg","path":"images/2012/06/clip_image001.jpg","modified":1},{"_id":"source/images/2012/05/image_thumb2.png","path":"images/2012/05/image_thumb2.png","modified":1},{"_id":"source/images/2012/05/image_thumb1.png","path":"images/2012/05/image_thumb1.png","modified":1},{"_id":"source/images/2012/05/image_thumb.png","path":"images/2012/05/image_thumb.png","modified":1},{"_id":"source/images/2012/05/image1.png","path":"images/2012/05/image1.png","modified":1},{"_id":"source/images/2012/05/image.png","path":"images/2012/05/image.png","modified":1},{"_id":"source/images/2012/04/clip_image011.jpg","path":"images/2012/04/clip_image011.jpg","modified":1},{"_id":"source/images/2012/04/clip_image009.jpg","path":"images/2012/04/clip_image009.jpg","modified":1},{"_id":"source/images/2012/04/clip_image007.jpg","path":"images/2012/04/clip_image007.jpg","modified":1},{"_id":"source/images/2012/04/clip_image005_thumb1.jpg","path":"images/2012/04/clip_image005_thumb1.jpg","modified":1},{"_id":"source/images/2012/04/clip_image005_thumb.jpg","path":"images/2012/04/clip_image005_thumb.jpg","modified":1},{"_id":"source/images/2012/04/clip_image005.jpg","path":"images/2012/04/clip_image005.jpg","modified":1},{"_id":"source/images/2012/04/clip_image004.jpg","path":"images/2012/04/clip_image004.jpg","modified":1},{"_id":"source/images/2012/04/clip_image002.jpg","path":"images/2012/04/clip_image002.jpg","modified":1},{"_id":"source/images/2012/04/201201051.png","path":"images/2012/04/201201051.png","modified":1},{"_id":"source/images/2012/03/image_thumb9.png","path":"images/2012/03/image_thumb9.png","modified":1},{"_id":"source/images/2012/03/image_thumb8.png","path":"images/2012/03/image_thumb8.png","modified":1},{"_id":"source/images/2012/03/image_thumb7.png","path":"images/2012/03/image_thumb7.png","modified":1},{"_id":"source/images/2012/03/image_thumb61.png","path":"images/2012/03/image_thumb61.png","modified":1},{"_id":"source/images/2012/03/image_thumb6.png","path":"images/2012/03/image_thumb6.png","modified":1},{"_id":"source/images/2012/03/image_thumb5.png","path":"images/2012/03/image_thumb5.png","modified":1},{"_id":"source/images/2012/03/image_thumb4.png","path":"images/2012/03/image_thumb4.png","modified":1},{"_id":"source/images/2012/03/image_thumb3.png","path":"images/2012/03/image_thumb3.png","modified":1},{"_id":"source/images/2012/03/image_thumb2.png","path":"images/2012/03/image_thumb2.png","modified":1},{"_id":"source/images/2012/03/image_thumb11.png","path":"images/2012/03/image_thumb11.png","modified":1},{"_id":"source/images/2012/03/image_thumb10.png","path":"images/2012/03/image_thumb10.png","modified":1},{"_id":"source/images/2012/03/image_thumb1.png","path":"images/2012/03/image_thumb1.png","modified":1},{"_id":"source/images/2012/03/image_thumb.png","path":"images/2012/03/image_thumb.png","modified":1},{"_id":"source/images/2012/03/image9.png","path":"images/2012/03/image9.png","modified":1},{"_id":"source/images/2012/03/image8.png","path":"images/2012/03/image8.png","modified":1},{"_id":"source/images/2012/03/image7.png","path":"images/2012/03/image7.png","modified":1},{"_id":"source/images/2012/03/image6.png","path":"images/2012/03/image6.png","modified":1},{"_id":"source/images/2012/03/image5.png","path":"images/2012/03/image5.png","modified":1},{"_id":"source/images/2012/03/image4.png","path":"images/2012/03/image4.png","modified":1},{"_id":"source/images/2012/03/image3.png","path":"images/2012/03/image3.png","modified":1},{"_id":"source/images/2012/03/image2.png","path":"images/2012/03/image2.png","modified":1},{"_id":"source/images/2012/03/image10.png","path":"images/2012/03/image10.png","modified":1},{"_id":"source/images/2012/03/image1.png","path":"images/2012/03/image1.png","modified":1},{"_id":"source/images/2012/03/image.png","path":"images/2012/03/image.png","modified":1},{"_id":"source/images/2012/03/clip_image006_thumb1.jpg","path":"images/2012/03/clip_image006_thumb1.jpg","modified":1},{"_id":"source/images/2012/03/clip_image006_thumb.jpg","path":"images/2012/03/clip_image006_thumb.jpg","modified":1},{"_id":"source/images/2012/03/clip_image006.jpg","path":"images/2012/03/clip_image006.jpg","modified":1},{"_id":"source/images/2012/03/clip_image005_thumb.jpg","path":"images/2012/03/clip_image005_thumb.jpg","modified":1},{"_id":"source/images/2012/03/clip_image005.jpg","path":"images/2012/03/clip_image005.jpg","modified":1},{"_id":"source/images/2012/03/clip_image003_thumb1.jpg","path":"images/2012/03/clip_image003_thumb1.jpg","modified":1},{"_id":"source/images/2012/03/clip_image003_thumb.jpg","path":"images/2012/03/clip_image003_thumb.jpg","modified":1},{"_id":"source/images/2012/03/clip_image003.jpg","path":"images/2012/03/clip_image003.jpg","modified":1},{"_id":"source/images/2012/03/clip_image002_thumb.jpg","path":"images/2012/03/clip_image002_thumb.jpg","modified":1},{"_id":"source/images/2012/03/clip_image002.jpg","path":"images/2012/03/clip_image002.jpg","modified":1},{"_id":"source/images/2012/03/SQLHadoop_thumb.jpg","path":"images/2012/03/SQLHadoop_thumb.jpg","modified":1},{"_id":"source/images/2012/03/SQLHadoop.jpg","path":"images/2012/03/SQLHadoop.jpg","modified":1},{"_id":"source/images/2012/03/POqWJ.png","path":"images/2012/03/POqWJ.png","modified":1},{"_id":"source/images/2012/03/NoSQL_thumb.png","path":"images/2012/03/NoSQL_thumb.png","modified":1},{"_id":"source/images/2012/03/CAP_thumb.png","path":"images/2012/03/CAP_thumb.png","modified":1},{"_id":"source/images/2012/02/image_thumb3.png","path":"images/2012/02/image_thumb3.png","modified":1},{"_id":"source/images/2012/02/image_thumb2.png","path":"images/2012/02/image_thumb2.png","modified":1},{"_id":"source/images/2012/02/image_thumb11.png","path":"images/2012/02/image_thumb11.png","modified":1},{"_id":"source/images/2012/02/image_thumb1.png","path":"images/2012/02/image_thumb1.png","modified":1},{"_id":"source/images/2012/02/image_thumb.png","path":"images/2012/02/image_thumb.png","modified":1},{"_id":"source/images/2012/02/image2.png","path":"images/2012/02/image2.png","modified":1},{"_id":"source/images/2012/02/image1.png","path":"images/2012/02/image1.png","modified":1},{"_id":"source/images/2012/02/image.png","path":"images/2012/02/image.png","modified":1},{"_id":"source/images/2012/02/clip_image004_thumb3.jpg","path":"images/2012/02/clip_image004_thumb3.jpg","modified":1},{"_id":"source/images/2012/02/clip_image004_thumb2.jpg","path":"images/2012/02/clip_image004_thumb2.jpg","modified":1},{"_id":"source/images/2012/02/clip_image004_thumb1.jpg","path":"images/2012/02/clip_image004_thumb1.jpg","modified":1},{"_id":"source/images/2012/02/clip_image004_thumb.jpg","path":"images/2012/02/clip_image004_thumb.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0046_thumb.jpg","path":"images/2012/02/clip_image0046_thumb.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0046.jpg","path":"images/2012/02/clip_image0046.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0043.jpg","path":"images/2012/02/clip_image0043.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0042.jpg","path":"images/2012/02/clip_image0042.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0041.jpg","path":"images/2012/02/clip_image0041.jpg","modified":1},{"_id":"source/images/2012/02/clip_image004.jpg","path":"images/2012/02/clip_image004.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb5.jpg","path":"images/2012/02/clip_image002_thumb5.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb41.jpg","path":"images/2012/02/clip_image002_thumb41.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb4.jpg","path":"images/2012/02/clip_image002_thumb4.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb31.jpg","path":"images/2012/02/clip_image002_thumb31.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb3.jpg","path":"images/2012/02/clip_image002_thumb3.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb21.jpg","path":"images/2012/02/clip_image002_thumb21.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb2.jpg","path":"images/2012/02/clip_image002_thumb2.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb1.jpg","path":"images/2012/02/clip_image002_thumb1.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb.jpg","path":"images/2012/02/clip_image002_thumb.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002_thumb.gif","path":"images/2012/02/clip_image002_thumb.gif","modified":1},{"_id":"source/images/2012/02/clip_image0026_thumb1.jpg","path":"images/2012/02/clip_image0026_thumb1.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0026_thumb.jpg","path":"images/2012/02/clip_image0026_thumb.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0026.jpg","path":"images/2012/02/clip_image0026.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0024.jpg","path":"images/2012/02/clip_image0024.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0023.jpg","path":"images/2012/02/clip_image0023.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0022.jpg","path":"images/2012/02/clip_image0022.jpg","modified":1},{"_id":"source/images/2012/02/clip_image0021.jpg","path":"images/2012/02/clip_image0021.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002.jpg","path":"images/2012/02/clip_image002.jpg","modified":1},{"_id":"source/images/2012/02/clip_image002.gif","path":"images/2012/02/clip_image002.gif","modified":1},{"_id":"source/images/2011/09/cem3Ejwh9cAac.gif","path":"images/2011/09/cem3Ejwh9cAac.gif","modified":1},{"_id":"source/images/2011/09/MUXFRHT_O3VBF79.jpg","path":"images/2011/09/MUXFRHT_O3VBF79.jpg","modified":1},{"_id":"source/images/2011/09/4d0ebb45de203b32cefca30d_thumb1.jpg","path":"images/2011/09/4d0ebb45de203b32cefca30d_thumb1.jpg","modified":1},{"_id":"source/images/2011/09/4d0ebb45de203b32cefca30d_thumb.jpg","path":"images/2011/09/4d0ebb45de203b32cefca30d_thumb.jpg","modified":1},{"_id":"source/images/2011/09/4d0ebb45de203b32cefca30d.jpg","path":"images/2011/09/4d0ebb45de203b32cefca30d.jpg","modified":1},{"_id":"source/images/2011/07/Android-Res11.png","path":"images/2011/07/Android-Res11.png","modified":1},{"_id":"source/images/2011/07/Android-Res1.png","path":"images/2011/07/Android-Res1.png","modified":1},{"_id":"source/images/2011/07/Android-Res.png","path":"images/2011/07/Android-Res.png","modified":1},{"_id":"source/images/2011/03/image_thumb2.png","path":"images/2011/03/image_thumb2.png","modified":1},{"_id":"source/images/2011/03/image_thumb1.png","path":"images/2011/03/image_thumb1.png","modified":1},{"_id":"source/images/2011/03/image_thumb.png","path":"images/2011/03/image_thumb.png","modified":1},{"_id":"source/images/2011/03/image1.png","path":"images/2011/03/image1.png","modified":1},{"_id":"source/images/2011/03/image.png","path":"images/2011/03/image.png","modified":1},{"_id":"source/images/2011/03/clip_image004_thumb.jpg","path":"images/2011/03/clip_image004_thumb.jpg","modified":1},{"_id":"source/images/2011/03/clip_image004.jpg","path":"images/2011/03/clip_image004.jpg","modified":1},{"_id":"source/images/2011/03/clip_image003_thumb.png","path":"images/2011/03/clip_image003_thumb.png","modified":1},{"_id":"source/images/2011/03/clip_image003.png","path":"images/2011/03/clip_image003.png","modified":1},{"_id":"source/images/2011/02/mypic.jpg","path":"images/2011/02/mypic.jpg","modified":1},{"_id":"source/images/2011/02/h_large_goGg_25210000773d2f75.jpg","path":"images/2011/02/h_large_goGg_25210000773d2f75.jpg","modified":1},{"_id":"source/images/2011/02/about.jpg","path":"images/2011/02/about.jpg","modified":1},{"_id":"source/favicon.png","path":"favicon.png","modified":1}],"Cache":[{"_id":"source/CNAME","shasum":"0ac7ddcf1b989fbcc91f6ca144101324ffac8de4","modified":1451395815000},{"_id":"source/_posts/2011/03/english-punctuate.md","shasum":"45fff5dca933ce55fb6d748ad0d49fe0f05d1c4f","modified":1451401954000},{"_id":"source/_posts/2011/03/java-probability.md","shasum":"129ee8b37b92e59ba933107f7c252476a2817ba8","modified":1451477458000},{"_id":"source/_posts/2011/03/ranksystem-in-social-network.md","shasum":"fa44485671d4b193860dbb2a3d847d34e9cf475d","modified":1451402118000},{"_id":"source/_posts/2011/03/study-school-sociey.md","shasum":"2d6a8777e101b726ec2277c97a46a7863daea0fb","modified":1451399713000},{"_id":"source/_posts/2011/10/dist-filesystem-metadata-server.md","shasum":"b9884b93f9b5533f78694490af8a29a80d855e05","modified":1451399210000},{"_id":"source/_posts/2012/01/python-chat.md","shasum":"c2d71fa0a41429396d4ebef0bf5ec1c49b6ca553","modified":1451402546000},{"_id":"source/_posts/2012/02/algorithm-dynammic-programming.md","shasum":"ddcdc980a3a1652c999d93a94e02dfed4774ce34","modified":1451402894000},{"_id":"source/_posts/2012/02/algorithm-probability.md","shasum":"7a0d5559089c0f29cff9035902af97e11b365d3b","modified":1451403050000},{"_id":"source/_posts/2012/02/algorithm-search.md","shasum":"c1a23fa025d64933121197e6822e7a1180b91ed3","modified":1451403182000},{"_id":"source/_posts/2012/02/hadoop-ipc-client.md","shasum":"6d9f28d8ec301445ff3257eabbcc14f498adfac4","modified":1451474229000},{"_id":"source/_posts/2012/02/hadoop-ipc-rpc.md","shasum":"5586010e30f5170037fae4862494fc01db8d3897","modified":1451474396000},{"_id":"source/_posts/2012/02/hadoop-ipc-server.md","shasum":"c6e7196d9549e885cc259102884c9ed40b6d0933","modified":1451474728000},{"_id":"source/_posts/2012/02/iterative-mapred-distcache.md","shasum":"39ed55573731900f4c499728442192290a09d5cc","modified":1451403905000},{"_id":"source/_posts/2012/02/iterative-mapred-summary-haloop.md","shasum":"3e2741520b4db9f01b405024935dfda4719d8f95","modified":1451403708000},{"_id":"source/_posts/2012/02/mapred-optimize.md","shasum":"593f133f7c57783bb1958e75d821bcf66a3b42d0","modified":1451404160000},{"_id":"source/_posts/2012/02/iterative-mapred.md","shasum":"2dd4ce726d35a26a50df52d77817d07de3f33ab2","modified":1451403558000},{"_id":"source/_posts/2012/02/jvm-structure.md","shasum":"cf84ce68f17ce1891f17e01d7e7e9b685aa721f1","modified":1451477790000},{"_id":"source/_posts/2012/03/data-structure-bitmap.md","shasum":"28ab4ffa8011da3729f9605ce59fe2238c859200","modified":1451474904000},{"_id":"source/_posts/2012/03/data-structure-disjoint-set.md","shasum":"88007ae13d5fa2c6801da564b557cb9b1faa5062","modified":1451475159000},{"_id":"source/_posts/2012/03/data-structure-skiplists.md","shasum":"b99a956f0b67bbeb71502b21fbe80518cb27b136","modified":1451475357000},{"_id":"source/_posts/2012/03/java-boxing.md","shasum":"9ac72eddb4d5823e0d2be1779f8a795cccbc5902","modified":1451475462000},{"_id":"source/_posts/2012/03/redis-data-strutrue.md","shasum":"280cbffbf67d447efdf65f39449cb4e29ef4083a","modified":1451475813000},{"_id":"source/_posts/2012/04/nlp-repost-segmentation.md","shasum":"e401e39ecaf65f94e5353a0a5d91a82a34ff781c","modified":1451555426000},{"_id":"source/_posts/2012/04/nlp-repost-semantic.md","shasum":"9d72e1b74dcb3c8f5e0985eeb674ef144a82750a","modified":1451555359000},{"_id":"source/_posts/2012/04/nlp-say-hi.md","shasum":"1ccde694f771702d1dcab5ed031391e6b7a19df6","modified":1451555609000},{"_id":"source/_posts/2012/05/hadoop-pipes-src.md","shasum":"51d077995f5afafc2bf4ca1a2ff35e98185af0e8","modified":1451751330000},{"_id":"source/_posts/2012/05/hadoop-pipes.md","shasum":"2ed2e5c0b40b71ea59d17cb27634f15fe311d416","modified":1451751318000},{"_id":"source/_posts/2012/06/google-doodle-for-turing.md","shasum":"2db0745ddab754327fa1a58003ebb0468505870a","modified":1451564208000},{"_id":"source/_posts/2012/09/apache-hadoop-yarn-background-and-an-overview.md","shasum":"363c5c55f4f10e2645114c1a955e4ee9a1caf33d","modified":1451564154000},{"_id":"source/_posts/2012/09/hadoop-bug-in-text.md","shasum":"9413e1e4e560d5a519738f22d64515bb1bf37803","modified":1451564197000},{"_id":"source/_posts/2012/09/mapred-optimize-writable.md","shasum":"03789f7ff4ca1be80e2196215ebc239e792905a7","modified":1451564372000},{"_id":"source/_posts/2012/10/mapreduce-task-src-analysis.md","shasum":"6cd2e9bbcf0eae0a5d8b95660c4b22edf8441612","modified":1451564690000},{"_id":"source/_posts/2012/11/zookeeper-ephemeral-nodes-experience.md","shasum":"735726edc4e129ca1dd64dc57d95d41aad027a51","modified":1451713618000},{"_id":"source/_posts/2013/01/2013-learning-plan.md","shasum":"e72c42466e6973c2c70aca45c26c6970723da1ef","modified":1451714630000},{"_id":"source/_posts/2013/01/linux-file-directory-shell.md","shasum":"7b9d7503052e444640753d069dc8215cb2735d95","modified":1451714640000},{"_id":"source/_posts/2013/01/linux-process-shell.md","shasum":"0989239a5cc47376df6486cbb0d434fc04d38609","modified":1451714729000},{"_id":"source/_posts/2013/01/linux-shell-term-tuning.md","shasum":"14f135ba9a7e718cfb583b8a718cda23435c0786","modified":1451714991000},{"_id":"source/_posts/2013/03/apache-solr-facet-introduction.md","shasum":"504e22b7d7a847db4560a00f0a4ccadb4cecdb62","modified":1451399210000},{"_id":"source/_posts/2013/01/linux-text-shell.md","shasum":"b4ab9010f3707ad1d45efe3c31f1bfef2ff35075","modified":1451751271000},{"_id":"source/_posts/2013/03/apache-solr-facet-pivot-implementation-tranplant.md","shasum":"39dba09d46ab464c725f1317de2b410745e9e187","modified":1451399210000},{"_id":"source/_posts/2013/03/maven-coordinates-dependencies.md","shasum":"d3bcc08ef574eb910150175a421875546cecebc4","modified":1451401492000},{"_id":"source/_posts/2013/03/maven-repositories.md","shasum":"0604635fd352640f7d2a0690fc8506817978cb48","modified":1451477297000},{"_id":"source/_posts/2013/04/apache-solr-data-structrue-part-1.md","shasum":"e5a00ab8073ada89915dd9bfd03a4a4ccc23d0be","modified":1451486098000},{"_id":"source/_posts/2013/04/apache-solr-data-structrue-part-2.md","shasum":"763f5958778d0bdb55be678133f11f46a5bbf59b","modified":1451486104000},{"_id":"source/_posts/2013/04/apache-solr-distributed-search.md","shasum":"585bbeb4d8acbdbd18252c9b3cd0fcb599111700","modified":1451399210000},{"_id":"source/_posts/2013/08/solr-switch-query-parser.md","shasum":"ef5ff00ae7ccaabea5c39f92cc69e66a2a6147f2","modified":1451407271000},{"_id":"source/_posts/2013/10/once-java-profiling.md","shasum":"c2c814cb2dfe34bb3472a049122296be672e0a74","modified":1451401305000},{"_id":"source/_posts/2013/11/what-i-think-about-1111.md","shasum":"91e108be838b24231ec9ab29588a130eb02e55f3","modified":1451486237000},{"_id":"source/_posts/2014/01/netty-4-x-bytebuf.md","shasum":"cf5be6da4996618547c4ebb96abcbd4dd435bb77","modified":1451399210000},{"_id":"source/_posts/2014/01/netty-4-x-channel-pipeline.md","shasum":"dc4638a1e3046b17fe979c28995208d7f4489780","modified":1451399210000},{"_id":"source/_posts/2014/01/netty-4-x-thread-model.md","shasum":"6e902100ff218758cb15c5c1200f82e3fa95fb1b","modified":1451399210000},{"_id":"source/_posts/2014/04/forget-your-lusts.md","shasum":"1fe47f1162bb8099a521c552ebd3c472c73f009d","modified":1451492138000},{"_id":"source/_posts/2014/04/rework-digest.md","shasum":"9802fd98973bc3d72987620533b12aaf4d6d2547","modified":1451484860000},{"_id":"source/_posts/2014/05/netty-mina-in-depth-1.md","shasum":"4bdaa9ac00fe395b951fdc0a3285d0948da84d6f","modified":1451399210000},{"_id":"source/_posts/2014/05/netty-mina-in-depth-2.md","shasum":"959f1dc0d5a16b48aa38a6a8bd67ab6402146fc4","modified":1451399210000},{"_id":"source/_posts/2014/10/i-need-an-answer-now-from-remote.md","shasum":"827ffe21654555420e3fc149cf920347ca436fc5","modified":1451486066000},{"_id":"source/_posts/2015/03/oom-killer-1.md","shasum":"b48bac6ad708228f6647c738c244894f9a62714f","modified":1451399210000},{"_id":"source/_posts/2015/08/css-practice.md","shasum":"e521d07dd5be3a7747133ab6e8c5afcfb5142565","modified":1451478276000},{"_id":"source/_posts/2015/08/git-practice.md","shasum":"e53449bd37919c6f8a1454b7e110f6bea0fd040c","modified":1451478312000},{"_id":"source/_posts/2015/08/java-practice.md","shasum":"eea28bdacd7b16985f09d1adccda4b82f4eee513","modified":1451399210000},{"_id":"source/_posts/2015/09/java-source-code-practice.md","shasum":"0f61c80b2c757e9946eb0d2933e23b44b3a9eec1","modified":1451484831000},{"_id":"source/_posts/2015/09/remoting-practice.md","shasum":"c07d005871ab910bf398e841b3a22096559e5232","modified":1451478232000},{"_id":"source/_posts/2015/10/docker-compose-pratice.md","shasum":"81d723d410f9fb2fa4cc0796a182272ddbd292ef","modified":1451399210000},{"_id":"source/_posts/2015/10/docker-volume-plugin.md","shasum":"3967ae1a2d2c17241c312defdd7292f33915f2b8","modified":1451399210000},{"_id":"source/_posts/2015/12/SSH.md","shasum":"77f3a99bbed7be55bcfa85b378dbc44370f72019","modified":1451478130000},{"_id":"source/images/2011/02/about.jpg","shasum":"9c4dc81ca2c7f421b2decb7a47069830f54a0508","modified":1451399281000},{"_id":"source/images/2011/02/mypic.jpg","shasum":"6ff67032870dfb7e1aa0ec90f0d7d26547848685","modified":1451399281000},{"_id":"source/images/2011/03/clip_image003.png","shasum":"8d15e68e537662a1492f58d657f54eefc0499c23","modified":1451399281000},{"_id":"source/images/2011/03/clip_image003_thumb.png","shasum":"8d15e68e537662a1492f58d657f54eefc0499c23","modified":1451399281000},{"_id":"source/images/2011/03/clip_image004.jpg","shasum":"72be51f60526a32129cd1cfd399106e2087bd246","modified":1451399281000},{"_id":"source/images/2011/03/clip_image004_thumb.jpg","shasum":"72be51f60526a32129cd1cfd399106e2087bd246","modified":1451399281000},{"_id":"source/images/2011/03/image1.png","shasum":"3660b360e73275b3e59a715c609fb71932ee5c35","modified":1451399281000},{"_id":"source/images/2011/03/image_thumb1.png","shasum":"fa2a8141c763f42cc2b99e307989d0e896243a86","modified":1451399281000},{"_id":"source/images/2011/07/Android-Res.png","shasum":"dfe817cc2d9e50c9fc8c97cc7d092fde5600e8cc","modified":1451399281000},{"_id":"source/images/2011/07/Android-Res1.png","shasum":"141c50584be61aaa984ce0be0b65a7c2ca53db80","modified":1451399281000},{"_id":"source/images/2011/07/Android-Res11.png","shasum":"141c50584be61aaa984ce0be0b65a7c2ca53db80","modified":1451399281000},{"_id":"source/images/2011/09/4d0ebb45de203b32cefca30d.jpg","shasum":"080e358e8e67e087abad69b70e18ae908bbc5380","modified":1451399281000},{"_id":"source/images/2011/09/4d0ebb45de203b32cefca30d_thumb.jpg","shasum":"f7ad545e02d5ad2b2d7fae16ea94c16050d941d2","modified":1451399281000},{"_id":"source/images/2011/09/4d0ebb45de203b32cefca30d_thumb1.jpg","shasum":"f7ad545e02d5ad2b2d7fae16ea94c16050d941d2","modified":1451399281000},{"_id":"source/images/2011/09/MUXFRHT_O3VBF79.jpg","shasum":"06df5e3d88f12f65755cbdb4ce51f170a83521a7","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002.gif","shasum":"99b0acc08c43b2476a60325bb416ef1b6045cbf5","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0021.jpg","shasum":"aa3d36e4dde0e8d1b999e86d0983ac1b20ab7e2c","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0022.jpg","shasum":"8d22940cd711972e2a6db714fc3bd332b5de9fb5","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0023.jpg","shasum":"3dd68989dd90210f6ae3e9e1d983ddc2f988c57e","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0026.jpg","shasum":"ab4dc27026bb1c747e61fd01f81dcf612c5eacc3","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0026_thumb.jpg","shasum":"ab4dc27026bb1c747e61fd01f81dcf612c5eacc3","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0026_thumb1.jpg","shasum":"ab4dc27026bb1c747e61fd01f81dcf612c5eacc3","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb.gif","shasum":"99b0acc08c43b2476a60325bb416ef1b6045cbf5","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb1.jpg","shasum":"aa3d36e4dde0e8d1b999e86d0983ac1b20ab7e2c","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb2.jpg","shasum":"8d22940cd711972e2a6db714fc3bd332b5de9fb5","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb21.jpg","shasum":"8d22940cd711972e2a6db714fc3bd332b5de9fb5","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb3.jpg","shasum":"3dd68989dd90210f6ae3e9e1d983ddc2f988c57e","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb31.jpg","shasum":"3dd68989dd90210f6ae3e9e1d983ddc2f988c57e","modified":1451399281000},{"_id":"source/images/2012/02/clip_image004.jpg","shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0041.jpg","shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0042.jpg","shasum":"bbd80e60d21d82b4cc39a4aba095a8c838981c34","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0043.jpg","shasum":"6e875577a50bcd7dee405243e91d69ca4ccb57d9","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0046.jpg","shasum":"54cec04992d1202e349a6182ebabfc6a63429bd7","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0046_thumb.jpg","shasum":"54cec04992d1202e349a6182ebabfc6a63429bd7","modified":1451399281000},{"_id":"source/images/2012/02/clip_image004_thumb.jpg","shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4","modified":1451399281000},{"_id":"source/images/2012/02/clip_image004_thumb1.jpg","shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4","modified":1451399281000},{"_id":"source/images/2012/02/clip_image004_thumb2.jpg","shasum":"bbd80e60d21d82b4cc39a4aba095a8c838981c34","modified":1451399281000},{"_id":"source/images/2012/02/clip_image004_thumb3.jpg","shasum":"6e875577a50bcd7dee405243e91d69ca4ccb57d9","modified":1451399281000},{"_id":"source/images/2012/02/image.png","shasum":"02b1528c983b043cef0127a93dc3101338189e9b","modified":1451399281000},{"_id":"source/images/2012/02/image1.png","shasum":"541f1ea432ffa3d7d813331fdfe0c0f31352790e","modified":1451399281000},{"_id":"source/images/2012/02/image2.png","shasum":"a72f4d771698d16a3b6f7a318145241d8ca3c81f","modified":1451399281000},{"_id":"source/images/2012/02/image_thumb.png","shasum":"02b1528c983b043cef0127a93dc3101338189e9b","modified":1451399281000},{"_id":"source/images/2012/02/image_thumb1.png","shasum":"541f1ea432ffa3d7d813331fdfe0c0f31352790e","modified":1451399281000},{"_id":"source/images/2012/02/image_thumb11.png","shasum":"541f1ea432ffa3d7d813331fdfe0c0f31352790e","modified":1451399281000},{"_id":"source/images/2012/02/image_thumb2.png","shasum":"a72f4d771698d16a3b6f7a318145241d8ca3c81f","modified":1451399281000},{"_id":"source/images/2012/02/image_thumb3.png","shasum":"02b1528c983b043cef0127a93dc3101338189e9b","modified":1451399281000},{"_id":"source/images/2012/03/CAP_thumb.png","shasum":"e45054b100e84dc10c528d24205c122192895196","modified":1451399281000},{"_id":"source/images/2012/03/SQLHadoop.jpg","shasum":"a1417b1af81fdc84f6e49bb62b87443f75bff70d","modified":1451399281000},{"_id":"source/images/2012/03/SQLHadoop_thumb.jpg","shasum":"4439674c2bac37078115fae030cc3e5fbbc4e55f","modified":1451399281000},{"_id":"source/images/2012/03/clip_image002.jpg","shasum":"325780e332a50f6c02797b7fc3c7233b080aee60","modified":1451399281000},{"_id":"source/images/2012/03/clip_image002_thumb.jpg","shasum":"325780e332a50f6c02797b7fc3c7233b080aee60","modified":1451399281000},{"_id":"source/images/2012/03/clip_image003.jpg","shasum":"04d2f4dc0101bf9a379a0b79bf7f141ea0e83c40","modified":1451399281000},{"_id":"source/images/2012/03/clip_image003_thumb.jpg","shasum":"04d2f4dc0101bf9a379a0b79bf7f141ea0e83c40","modified":1451399281000},{"_id":"source/images/2012/03/clip_image003_thumb1.jpg","shasum":"04d2f4dc0101bf9a379a0b79bf7f141ea0e83c40","modified":1451399281000},{"_id":"source/images/2012/03/clip_image005.jpg","shasum":"8dbed96cd7c32f0fa95bbcccacad144519971bf6","modified":1451399281000},{"_id":"source/images/2012/03/clip_image005_thumb.jpg","shasum":"8dbed96cd7c32f0fa95bbcccacad144519971bf6","modified":1451399281000},{"_id":"source/images/2012/03/clip_image006.jpg","shasum":"b06606bec34138a24d08a0e1f4944cbec2cafe2a","modified":1451399281000},{"_id":"source/images/2012/03/clip_image006_thumb.jpg","shasum":"b06606bec34138a24d08a0e1f4944cbec2cafe2a","modified":1451399281000},{"_id":"source/images/2012/03/clip_image006_thumb1.jpg","shasum":"b06606bec34138a24d08a0e1f4944cbec2cafe2a","modified":1451399281000},{"_id":"source/images/2012/03/image.png","shasum":"b9a5c4d09646c6a7ad2a01f346be9a3179566d16","modified":1451399281000},{"_id":"source/images/2012/03/image1.png","shasum":"8a84a4f2a100170eda650f76023bea3c36db979b","modified":1451399281000},{"_id":"source/images/2012/03/image10.png","shasum":"859f13eef8fcfeb85c2b4311aeaf9e69aa3de4f0","modified":1451399281000},{"_id":"source/images/2012/03/image2.png","shasum":"cb958188a7a0c04bf2477b4a9ae336cf34938551","modified":1451399281000},{"_id":"source/images/2012/03/image3.png","shasum":"c4b9438398ff0665f1f4ce0c6c392a6fa5c7ab11","modified":1451399281000},{"_id":"source/images/2012/03/image4.png","shasum":"8ad5195a1a96c47a988a925b9d37c7ce27538bb4","modified":1451399281000},{"_id":"source/images/2012/03/image5.png","shasum":"5f8293231fb845e5c27ad4366c89b07d3d0f53b3","modified":1451399281000},{"_id":"source/images/2012/03/image6.png","shasum":"52efe11382790cf209ed173036345af8540f008b","modified":1451399281000},{"_id":"source/images/2012/03/image7.png","shasum":"7ad5e28317aaf8bfdedb1c614e8884fa5fccd49f","modified":1451399281000},{"_id":"source/images/2012/03/image8.png","shasum":"15f7b86b844beb2e8b910c7c7b64bcb7c379f442","modified":1451399281000},{"_id":"source/images/2012/03/image9.png","shasum":"d57c64f9ad1bbcffbe9c97a55dcd75850573b473","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb.png","shasum":"7585ee5ed3df2715a13c7c68182d5249b5747842","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb10.png","shasum":"859f13eef8fcfeb85c2b4311aeaf9e69aa3de4f0","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb2.png","shasum":"cb958188a7a0c04bf2477b4a9ae336cf34938551","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb3.png","shasum":"c4b9438398ff0665f1f4ce0c6c392a6fa5c7ab11","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb4.png","shasum":"bd5b82b3353b73dc010af4b4d769e08c8ea4a8ab","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb5.png","shasum":"637d07fdc4705d41f55cd5ec997f51548525bfda","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb6.png","shasum":"52efe11382790cf209ed173036345af8540f008b","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb61.png","shasum":"52efe11382790cf209ed173036345af8540f008b","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb7.png","shasum":"7ad5e28317aaf8bfdedb1c614e8884fa5fccd49f","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb8.png","shasum":"15f7b86b844beb2e8b910c7c7b64bcb7c379f442","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb9.png","shasum":"d57c64f9ad1bbcffbe9c97a55dcd75850573b473","modified":1451399281000},{"_id":"source/images/2012/04/201201051.png","shasum":"03896f69e1f4742c999c4a915e47d9b712d9e262","modified":1451399281000},{"_id":"source/images/2012/04/clip_image002.jpg","shasum":"d9dbb846d1a6a63beaa284a88c02f0e684debad9","modified":1451399281000},{"_id":"source/images/2012/04/clip_image004.jpg","shasum":"1a8c0450e687ce88e41c64d3304067b1df652a5e","modified":1451399281000},{"_id":"source/images/2012/04/clip_image005.jpg","shasum":"c2f5e532c5888f197d3418fe33350e7fe93b5549","modified":1451399281000},{"_id":"source/images/2012/04/clip_image005_thumb.jpg","shasum":"7c1ab1ac0bda4ccd03c5af62f4bc5a7c1c567359","modified":1451399281000},{"_id":"source/images/2012/04/clip_image005_thumb1.jpg","shasum":"7c1ab1ac0bda4ccd03c5af62f4bc5a7c1c567359","modified":1451399281000},{"_id":"source/images/2012/04/clip_image007.jpg","shasum":"20f7ffb693a7703de79b04932b20814284e1aabc","modified":1451399281000},{"_id":"source/images/2012/04/clip_image009.jpg","shasum":"8a5ffee5f2a711636b8cfd16fab8cd839fbcc1f2","modified":1451399281000},{"_id":"source/images/2012/04/clip_image011.jpg","shasum":"5fe890d2f1a75bb14aec5ce18fb26f17d7d26d64","modified":1451399281000},{"_id":"source/images/2012/05/image.png","shasum":"99a2adc8c84be74a923d4b4f4db422921ee1948d","modified":1451399281000},{"_id":"source/images/2012/05/image1.png","shasum":"3d2a6e55d6f76a10216693f4aabf68cb864e9e24","modified":1451399281000},{"_id":"source/images/2012/05/image_thumb.png","shasum":"99a2adc8c84be74a923d4b4f4db422921ee1948d","modified":1451399281000},{"_id":"source/images/2012/05/image_thumb1.png","shasum":"3d2a6e55d6f76a10216693f4aabf68cb864e9e24","modified":1451399281000},{"_id":"source/images/2012/05/image_thumb2.png","shasum":"99a2adc8c84be74a923d4b4f4db422921ee1948d","modified":1451399281000},{"_id":"source/images/2012/06/clip_image001.jpg","shasum":"d965ab04b3611481137d6c701a743e0149a801ac","modified":1451399281000},{"_id":"source/images/2012/06/clip_image001_thumb.jpg","shasum":"d965ab04b3611481137d6c701a743e0149a801ac","modified":1451399281000},{"_id":"source/images/2012/06/clip_image003.jpg","shasum":"b3f70e473e626b041c5ab20a1f1d5c5885c80759","modified":1451399281000},{"_id":"source/images/2012/06/clip_image003_thumb.jpg","shasum":"b3f70e473e626b041c5ab20a1f1d5c5885c80759","modified":1451399281000},{"_id":"source/images/2012/06/clip_image003_thumb1.jpg","shasum":"b3f70e473e626b041c5ab20a1f1d5c5885c80759","modified":1451399281000},{"_id":"source/images/2012/06/clip_image005.jpg","shasum":"48d59fc9c6042dfdb52ef3060821bec3762f0c51","modified":1451399281000},{"_id":"source/images/2012/06/clip_image005_thumb.jpg","shasum":"48d59fc9c6042dfdb52ef3060821bec3762f0c51","modified":1451399281000},{"_id":"source/images/2012/06/clip_image007.jpg","shasum":"1fa5711dfc217d445f6c6a8f2ee876e4f0ddbcfc","modified":1451399281000},{"_id":"source/images/2012/06/clip_image007_thumb.jpg","shasum":"1fa5711dfc217d445f6c6a8f2ee876e4f0ddbcfc","modified":1451399281000},{"_id":"source/images/2012/06/clip_image009.jpg","shasum":"d9d3a25bec46e34a583326cf0fc27007edb0b12d","modified":1451399281000},{"_id":"source/images/2012/06/clip_image009_thumb.jpg","shasum":"d9d3a25bec46e34a583326cf0fc27007edb0b12d","modified":1451399281000},{"_id":"source/images/2012/06/clip_image011.jpg","shasum":"e149bec5527843850753e916a446d35c12d90732","modified":1451399281000},{"_id":"source/images/2012/06/clip_image011_thumb.jpg","shasum":"e149bec5527843850753e916a446d35c12d90732","modified":1451399281000},{"_id":"source/images/2012/06/clip_image013.jpg","shasum":"15ff24c36b2445e25fd042764d6162c21b27a941","modified":1451399281000},{"_id":"source/images/2012/06/clip_image013_thumb.jpg","shasum":"15ff24c36b2445e25fd042764d6162c21b27a941","modified":1451399281000},{"_id":"source/images/2012/06/clip_image015.jpg","shasum":"10dbcadc81aa1fc38b92c56e04130fd6cc8024ee","modified":1451399281000},{"_id":"source/images/2012/06/clip_image015_thumb.jpg","shasum":"10dbcadc81aa1fc38b92c56e04130fd6cc8024ee","modified":1451399281000},{"_id":"source/images/2012/06/clip_image017.jpg","shasum":"072ccce367e88ff765490ac1650a5e859a974ffa","modified":1451399281000},{"_id":"source/images/2012/06/clip_image017_thumb.jpg","shasum":"072ccce367e88ff765490ac1650a5e859a974ffa","modified":1451399281000},{"_id":"source/images/2012/07/ADistributedHashTableRing.png","shasum":"fd23b6fd24140414f34e730f24b3589fe952065f","modified":1451399281000},{"_id":"source/images/2012/07/ADistributedHashTableRing_thumb.png","shasum":"fd23b6fd24140414f34e730f24b3589fe952065f","modified":1451399281000},{"_id":"source/images/2012/07/ADistributedHashTableRing_thumb1.png","shasum":"fd23b6fd24140414f34e730f24b3589fe952065f","modified":1451399281000},{"_id":"source/images/2012/07/BigTablebasedRangePartitioning.png","shasum":"5d6d2edfc4119b10ebaf987fffdfc542e6bb4c35","modified":1451399281000},{"_id":"source/images/2012/07/BigTablebasedRangePartitioning_thumb.png","shasum":"d718d1c24300ac26d354c4014865cf54ee0c498c","modified":1451399281000},{"_id":"source/images/2012/09/Image.png","shasum":"fe48fa243251e3e2eaec878e7ba8c60e4da5fcbb","modified":1451399281000},{"_id":"source/images/2012/09/Image_thumb.png","shasum":"08e9ffbea67f103f6b4c535caf22d23aacdaf482","modified":1451399281000},{"_id":"source/images/2012/09/Image_thumb1.png","shasum":"08e9ffbea67f103f6b4c535caf22d23aacdaf482","modified":1451399281000},{"_id":"source/images/2012/10/Image.jpg","shasum":"ee2a4583b4d17a3acee44176dcb7a0b2dd3ef399","modified":1451399281000},{"_id":"source/images/2012/10/Image1.jpg","shasum":"d3e7624b85605e875ec1fa8361ae36d0d380309d","modified":1451399281000},{"_id":"source/images/2012/10/Image2.jpg","shasum":"941934f73c977a4eff64f941617bdcd3922b38bc","modified":1451399281000},{"_id":"source/images/2012/10/Image_thumb.jpg","shasum":"7d2a4856c78b1f5bf49ff794b1c3710a864a703c","modified":1451399281000},{"_id":"source/images/2012/10/Image_thumb1.jpg","shasum":"7784cda2ee30d9ea440d5a8253614505d9dde6b1","modified":1451399281000},{"_id":"source/images/2012/10/Image_thumb2.jpg","shasum":"68a48f2e700de8ed3bbb355b3c7ccbd8d628023a","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0014.jpg","shasum":"b4f1adcf8a0c8435a93a8de13cda8d6e2cdf8222","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0014_thumb.jpg","shasum":"7dca37fb15bcf57f60b52ccce1a24080c203dbfd","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0034.jpg","shasum":"24f6ab225443c52c619410e9d276b6ef54d3b14a","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0034_thumb.jpg","shasum":"cc83cfdbfe92568e82d60e384221858566d39ec2","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0054.jpg","shasum":"6e44f6676958342ecefce17604a83b6f4d57feaa","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0054_thumb.jpg","shasum":"3f80076600d98cf0d584e11dbbe3d913174cf266","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0074.jpg","shasum":"8678cec0c56cfebf5529b7561c72436263354bde","modified":1451399281000},{"_id":"source/images/2012/10/clip_image00741.jpg","shasum":"8678cec0c56cfebf5529b7561c72436263354bde","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0074_thumb.jpg","shasum":"4574179180a12f170bd1744afcf7fef7b6e964f2","modified":1451399281000},{"_id":"source/images/2013/01/image.png","shasum":"2de71610aff991609f60397bc7d64e4c293f1c1f","modified":1451399281000},{"_id":"source/images/2012/10/clip_image0074_thumb1.jpg","shasum":"5a2c832cb3366482f250989a078ef598efcea533","modified":1451399281000},{"_id":"source/images/2013/01/image_thumb.png","shasum":"83031ca8e39e0fbc66eac549a3bbdd05a52df0c6","modified":1451399281000},{"_id":"source/images/2013/03/1.jpg","shasum":"6ef5c815e053159040d2a32132e8a26b5be06cbc","modified":1451399281000},{"_id":"source/images/2013/03/11.jpg","shasum":"313f0ffca4496858ec127799f602cde0690a803c","modified":1451399281000},{"_id":"source/images/2013/03/12.jpg","shasum":"fceaccd5b97d79ae7e2bd3b3bd90ba6eb45af218","modified":1451399281000},{"_id":"source/images/2013/03/1_thumb.jpg","shasum":"b7091eb744aafbf947895a6f0791706ce5d6fb8f","modified":1451399281000},{"_id":"source/images/2013/03/201201051.png","shasum":"03896f69e1f4742c999c4a915e47d9b712d9e262","modified":1451399281000},{"_id":"source/images/2013/03/image.png","shasum":"93d094420b3319853eff3887f15ecbdaf88970db","modified":1451399281000},{"_id":"source/images/2013/03/image1.png","shasum":"938bd19a87b24de5485960176c817bdac85b6093","modified":1451399281000},{"_id":"source/images/2013/03/image_thumb.png","shasum":"93d094420b3319853eff3887f15ecbdaf88970db","modified":1451399281000},{"_id":"source/images/2013/03/image_thumb1.png","shasum":"938bd19a87b24de5485960176c817bdac85b6093","modified":1451399281000},{"_id":"source/images/2013/08/QQ20130831103315.jpg","shasum":"70d5654d4d6a393e7d49d56bf5c642899dcb8148","modified":1451399281000},{"_id":"source/images/2013/08/QQ20130831103315_thumb.jpg","shasum":"9f65d0e3ec9ad186766da8fe25097e34355f0807","modified":1451399281000},{"_id":"source/favicon.png","shasum":"ff06c4acc220f1126650f1bdff6b00cdb70699af","modified":1451478743000},{"_id":"source/images/2011/02/h_large_goGg_25210000773d2f75.jpg","shasum":"b6d6800e44218ed44f789634540e94b13815e0da","modified":1451399281000},{"_id":"source/images/2012/02/DistributedCache.html","shasum":"9501d7adbfa87b4094b43c6625b00662287579b7","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002.jpg","shasum":"c7ec9214889032bb9ad8344d7abcd026a3378a7d","modified":1451399281000},{"_id":"source/images/2012/02/clip_image0024.jpg","shasum":"275b69d040221d290e2d168f2c6a6a6b1c845ce6","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb.jpg","shasum":"c7ec9214889032bb9ad8344d7abcd026a3378a7d","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb4.jpg","shasum":"275b69d040221d290e2d168f2c6a6a6b1c845ce6","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb41.jpg","shasum":"275b69d040221d290e2d168f2c6a6a6b1c845ce6","modified":1451399281000},{"_id":"source/images/2012/02/clip_image002_thumb5.jpg","shasum":"c7ec9214889032bb9ad8344d7abcd026a3378a7d","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb1.png","shasum":"bdc4785520eef15621503d27c552a9757bb3f55b","modified":1451399281000},{"_id":"source/images/2012/03/image_thumb11.png","shasum":"bdc4785520eef15621503d27c552a9757bb3f55b","modified":1451399281000},{"_id":"source/images/2013/01/SavedPicture-2013123102218.jpg","shasum":"f0c7730e07986abf5b71fb0c9cc979c132d56fa1","modified":1451399281000},{"_id":"source/images/2013/03/163306e8e326.jpg","shasum":"6d79054352936d30197e3fab9ef0846ebd7465cd","modified":1451399281000},{"_id":"source/images/2013/03/thumb.jpg","shasum":"63d503070b625764aff6a829b4b33235b5c9e548","modified":1451399281000},{"_id":"source/images/2013/08/image.png","shasum":"553edaa340cae996da369b9a2830af99b8053778","modified":1451399281000},{"_id":"source/images/2012/03/NoSQL_thumb.png","shasum":"fec0eb5d77a5b76d8ad875d4f5cda0498fd36bf1","modified":1451399281000},{"_id":"source/images/2012/09/YARNArch.png","shasum":"d78fbdab15ec4dc68d4b9068c8f3003657de1a6a","modified":1451399281000},{"_id":"source/images/2012/09/YARNArch_thumb.png","shasum":"d78fbdab15ec4dc68d4b9068c8f3003657de1a6a","modified":1451399281000},{"_id":"source/images/2012/09/YARNArch_thumb1.png","shasum":"d78fbdab15ec4dc68d4b9068c8f3003657de1a6a","modified":1451399281000},{"_id":"source/images/2013/04/DocSet.png","shasum":"08891f7f7c7f3ee40161f9b8a01012249e5d9151","modified":1451399281000},{"_id":"source/images/2011/09/cem3Ejwh9cAac.gif","shasum":"090944ce94f1dcd17261844ae413738739c23f82","modified":1451399281000},{"_id":"source/images/2013/04/DocSet_thumb.png","shasum":"3fdcb297cb89b11a71563e93953edd0a363538e3","modified":1451399281000},{"_id":"source/images/2013/08/image_thumb.png","shasum":"b70606cdab5cceeffd98740b6ee18d9fda954c25","modified":1451399281000},{"_id":"source/images/2013/10/Image1.png","shasum":"f2ec479480662ce8da00dcaa93a4d3b18c280efc","modified":1451399281000},{"_id":"source/images/2013/10/Image.png","shasum":"0a9571d46098c17aa1ae400465cd609943b35ad2","modified":1451399281000},{"_id":"source/images/2013/10/Image1_thumb.png","shasum":"f2ec479480662ce8da00dcaa93a4d3b18c280efc","modified":1451399281000},{"_id":"source/images/2013/10/Image2.png","shasum":"5912532a4058c8a90da1774ed2ccf6f63b878f15","modified":1451399281000},{"_id":"source/images/2013/10/Image2_thumb.png","shasum":"5912532a4058c8a90da1774ed2ccf6f63b878f15","modified":1451399281000},{"_id":"source/images/2013/10/Image3.png","shasum":"cef190cdde4f0879911fc6d1805791ef0cfeab2e","modified":1451399281000},{"_id":"source/images/2013/10/Image3_thumb.png","shasum":"cef190cdde4f0879911fc6d1805791ef0cfeab2e","modified":1451399281000},{"_id":"source/images/2013/10/Image4.png","shasum":"c0f75c34f88dda8dc1e83b93c4ece6ecacaafffb","modified":1451399281000},{"_id":"source/images/2013/10/Image4_thumb.png","shasum":"c0f75c34f88dda8dc1e83b93c4ece6ecacaafffb","modified":1451399281000},{"_id":"source/images/2013/10/Image_thumb.png","shasum":"0a9571d46098c17aa1ae400465cd609943b35ad2","modified":1451399281000},{"_id":"source/images/2014/01/favicon.png","shasum":"6cdd78df2d91e397f08a981df6e5bb58896f1f80","modified":1451399281000},{"_id":"source/images/2014/01/icon.png","shasum":"ec9a4bb4b2dcf5ae30a74710ae623a394af94165","modified":1451399281000},{"_id":"source/images/2014/01/e.png","shasum":"f38f376c44ab4c307cf5208a521e59b7fd1c22a6","modified":1451399281000},{"_id":"source/images/2014/01/icon1.png","shasum":"716496939c0b0a965fb3c7ec9096f2e7a4fb1810","modified":1451399281000},{"_id":"source/images/2014/01/wpid-diagram.jpg","shasum":"81f2c3c1b19f01d6af05b2de19124c93ea3cad23","modified":1451399281000},{"_id":"source/images/2014/01/wpid-diagram1.jpg","shasum":"81f2c3c1b19f01d6af05b2de19124c93ea3cad23","modified":1451399281000},{"_id":"source/images/2014/05/reactor-single-thread.png","shasum":"799dbea22e802bf51877500c9d7a80621baadc44","modified":1451399282000},{"_id":"source/images/2014/01/wpid-bytebuf.png","shasum":"f2abcaa081d341f8b6aaa1c7c19e7c562329475b","modified":1451399281000},{"_id":"source/images/bytebuf-diagram.jpg","shasum":"81f2c3c1b19f01d6af05b2de19124c93ea3cad23","modified":1451395815000},{"_id":"source/images/facet-1.png","shasum":"938bd19a87b24de5485960176c817bdac85b6093","modified":1451395815000},{"_id":"source/images/bytebuf-priciple.png","shasum":"f2abcaa081d341f8b6aaa1c7c19e7c562329475b","modified":1451395815000},{"_id":"source/images/reactor-single-thread.png","shasum":"799dbea22e802bf51877500c9d7a80621baadc44","modified":1451395815000},{"_id":"source/tags/index.md","shasum":"3d1d7c6bc9e211ff0ed7fc76d0489c8944b1e064","modified":1451493530000},{"_id":"themes/apollo/LICENSE","shasum":"6e31ac9076bfc8f09ae47977419eee4edfb63e5b","modified":1451406050000},{"_id":"themes/apollo/README.md","shasum":"ee179bad009e0a0d1795d25b31cf591a9342dab5","modified":1451406050000},{"_id":"themes/apollo/_config.yml","shasum":"fccf637141d74fb3643bacf7f80206b797813d53","modified":1451493544000},{"_id":"themes/apollo/doc/custom-blocks.md","shasum":"78e9400714d0ff7c9b272d3ccc80fb18c3bf208f","modified":1451406050000},{"_id":"themes/apollo/gulpfile.babel.js","shasum":"bab2bbe4543fa2d096b06488c1c62494535eaa72","modified":1451406050000},{"_id":"themes/apollo/layout/archive.jade","shasum":"0e8805f041192fbc5e86cfd1474909e58059b946","modified":1451492510000},{"_id":"themes/apollo/layout/index.jade","shasum":"88446eccbbe45e11356e1853778e481318b69a85","modified":1451483039000},{"_id":"themes/apollo/layout/mixins/paginator.jade","shasum":"f10066422337a33e0d8030d3277892907ff45f0d","modified":1451492714000},{"_id":"themes/apollo/layout/mixins/post.jade","shasum":"db51425a02a4f5a1a360378b1aa4b864d8085eab","modified":1451493356000},{"_id":"themes/apollo/layout/partial/comment.jade","shasum":"ff0a2c269c2434da2ac5529872f1d6184a71f96d","modified":1451406050000},{"_id":"themes/apollo/layout/partial/head.jade","shasum":"2ed7b73ad13b184e2bda9c80ba078ea33bbbbe4d","modified":1451406050000},{"_id":"themes/apollo/layout/partial/layout.jade","shasum":"d9c4f8933f6740f5159713ec69ab943db5fb7cae","modified":1451406050000},{"_id":"themes/apollo/layout/partial/scripts.jade","shasum":"4c83fec1e2fc5cffefafc2e31835e28122c0fdfd","modified":1451406050000},{"_id":"themes/apollo/layout/partial/nav.jade","shasum":"a2f37ff652a0609fb5d9817bb04433c0619cba97","modified":1451494185000},{"_id":"themes/apollo/layout/post.jade","shasum":"80e5c236c7f0beee2110777e62f02e9a5d4c6727","modified":1451493433000},{"_id":"themes/apollo/layout/tag.jade","shasum":"093892fbf972b40673a72650f7f1d750b55d1c84","modified":1451493453000},{"_id":"themes/apollo/package.json","shasum":"a872d0158d522612ccc0b300bdf27d0228de8428","modified":1451406050000},{"_id":"themes/apollo/source/css/apollo.css","shasum":"eaad806068ee494ff6eeabafa8b430ba9eae499f","modified":1451490216000},{"_id":"themes/apollo/source/css/apollo.css.map","shasum":"44cd7c88ef9aa2fd70cfd29942d0ae6267349a76","modified":1451490216000},{"_id":"themes/apollo/source/favicon.png","shasum":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1451406050000},{"_id":"themes/apollo/source/gen.sh","shasum":"9388b3ff4d753551234340fef45dae5842f46858","modified":1451485100000},{"_id":"themes/apollo/source/scss/_partial/base.scss","shasum":"88b361e68475caddbab763feed5e1db788ac2cd7","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/footer.scss","shasum":"60cf365489c0d93cd7e9f10eedd4aee702e1ef27","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/header.scss","shasum":"d24cc6520f3faa7bb80610b858a92639eadcc289","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/mq.scss","shasum":"0b9c7097136ac8e4a07d9702fc4dbe0345ac7596","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/normalize.scss","shasum":"fd0b27bed6f103ea95b08f698ea663ff576dbcf1","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/post.scss","shasum":"8ca0ffea647c28ed0b158635a142e43ae052b2f2","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/posts.scss","shasum":"92858015b8f3dcb4eb91b6dc41563b7aaa91b376","modified":1451406050000},{"_id":"themes/apollo/source/scss/_partial/tags.scss","shasum":"f3bad1b9b6920d3d6531daefb2cb5a2ec48807ea","modified":1451490208000},{"_id":"themes/apollo/source/scss/apollo.scss","shasum":"fb21abe0987aeded9f4ea88e4eb3c9c0a90f9b2b","modified":1451481070000},{"_id":"source/images/2014/01/wpid-nio-oio.jpg","shasum":"ac7df496f41f1b549b41a4a4404a437ea18460c9","modified":1451399282000},{"_id":"source/images/2014/01/wpid-nio-oio1.jpg","shasum":"6b6e43e7a96dd83b11e4f63b1bcfb9668ddefbf2","modified":1451399282000},{"_id":"source/images/2014/05/reactor-thread-pool.png","shasum":"2698dc114f10fdddef468dd49f23338f743c319e","modified":1451399282000},{"_id":"source/images/2014/01/wpid-combine-slice-buffer.png","shasum":"a073deecce1516d12e2270f373ccfa8bc57b87fd","modified":1451399281000},{"_id":"source/images/bytebuf-combine-slice-buffer.png","shasum":"a073deecce1516d12e2270f373ccfa8bc57b87fd","modified":1451395815000},{"_id":"source/images/reactor-thread-pool.png","shasum":"2698dc114f10fdddef468dd49f23338f743c319e","modified":1451395815000},{"_id":"source/images/wpid-nio-oio.jpg","shasum":"ac7df496f41f1b549b41a4a4404a437ea18460c9","modified":1451395815000},{"_id":"source/images/2014/01/wpid-Channel.png","shasum":"06acf8025d55d644ecb8d153a62c438ce6f07b8f","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Netty-ChannelPipeline-.png","shasum":"3b02ee73f26fcc6a782a8cb6b3af22b6e7d2be7d","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Netty-ChannelPipeline-1.png","shasum":"8fb9f8bfe8c3c82012789e93c9ed0d3742cf2998","modified":1451399281000},{"_id":"source/images/2014/05/reactors-in-threads-thread-pool.png","shasum":"cf1805741cd76ae36b85f51e3b5c455c418fa910","modified":1451399282000},{"_id":"source/images/2014/05/reactors-in-threads.png","shasum":"31e20bc84b78d60af27998c0a154014babfb09dd","modified":1451399282000},{"_id":"source/images/reactors-in-threads-thread-pool.png","shasum":"cf1805741cd76ae36b85f51e3b5c455c418fa910","modified":1451395815000},{"_id":"source/images/solr-DocSet.png","shasum":"08891f7f7c7f3ee40161f9b8a01012249e5d9151","modified":1451395815000},{"_id":"source/images/reactors-in-threads.png","shasum":"31e20bc84b78d60af27998c0a154014babfb09dd","modified":1451395815000},{"_id":"source/images/wpid-Channel.png","shasum":"06acf8025d55d644ecb8d153a62c438ce6f07b8f","modified":1451395815000},{"_id":"source/images/wpid-Netty-ChannelPipeline-.png","shasum":"3b02ee73f26fcc6a782a8cb6b3af22b6e7d2be7d","modified":1451395815000},{"_id":"source/images/2011/03/image.png","shasum":"6250cb26042719803f5c641cc14ef10401c8675e","modified":1451399281000},{"_id":"source/images/2011/03/image_thumb.png","shasum":"4eaf9814bf07e4956ddc6ac1d25e556696b592c0","modified":1451399281000},{"_id":"source/images/2011/03/image_thumb2.png","shasum":"4eaf9814bf07e4956ddc6ac1d25e556696b592c0","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Multi-reactors.png","shasum":"7220e5db283093ba134a0b8ac93f45c4b0ebcbdc","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Multi-reactors2.png","shasum":"720760fd57dafcff8ef831e39ce7e179d7d52f2f","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Multi-reactors1.png","shasum":"192bf67e9fd4a26900c129d4db4b8e611b5629b0","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Multi-reactors3.png","shasum":"c18acca0c970ff38beb9605f03819eae331a0d1b","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Netty-thread-model1.png","shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Netty-thread-model.png","shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79","modified":1451399281000},{"_id":"source/images/2014/01/wpid-Netty-thread-model2.png","shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79","modified":1451399282000},{"_id":"source/images/2014/01/wpid-Netty-thread-model3.png","shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79","modified":1451399282000},{"_id":"source/images/wpid-Multi-reactors3.png","shasum":"c18acca0c970ff38beb9605f03819eae331a0d1b","modified":1451395815000},{"_id":"source/images/wpid-Netty-thread-model3.png","shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79","modified":1451395815000},{"_id":"source/images/ranksystem-in-social-network.png","shasum":"6250cb26042719803f5c641cc14ef10401c8675e","modified":1451396159000},{"_id":"source/images/2012/03/POqWJ.png","shasum":"0eae0d7a73b610190aadc8932c09434fb8abf98b","modified":1451399281000},{"_id":"source/images/2013/03/POqWJ.png","shasum":"0eae0d7a73b610190aadc8932c09434fb8abf98b","modified":1451399281000},{"_id":"source/upload/java-at-alibaba.pptx","shasum":"bd213366da7aeac6bb98721ed84972ac62070298","modified":1451395815000},{"_id":"public/scss/apollo.scss","modified":1451751375880,"shasum":"fb21abe0987aeded9f4ea88e4eb3c9c0a90f9b2b"},{"_id":"public/gen.sh","modified":1451751375883,"shasum":"9388b3ff4d753551234340fef45dae5842f46858"},{"_id":"public/favicon.png","modified":1451751375885,"shasum":"ff06c4acc220f1126650f1bdff6b00cdb70699af"},{"_id":"public/css/apollo.css.map","modified":1451751375886,"shasum":"44cd7c88ef9aa2fd70cfd29942d0ae6267349a76"},{"_id":"public/css/apollo.css","modified":1451751375888,"shasum":"eaad806068ee494ff6eeabafa8b430ba9eae499f"},{"_id":"public/CNAME","modified":1451751375889,"shasum":"0ac7ddcf1b989fbcc91f6ca144101324ffac8de4"},{"_id":"public/upload/java-at-alibaba.pptx","modified":1451751375891,"shasum":"bd213366da7aeac6bb98721ed84972ac62070298"},{"_id":"public/images/wpid-nio-oio.jpg","modified":1451751375894,"shasum":"ac7df496f41f1b549b41a4a4404a437ea18460c9"},{"_id":"public/images/wpid-Netty-thread-model3.png","modified":1451751375895,"shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79"},{"_id":"public/images/wpid-Netty-ChannelPipeline-.png","modified":1451751375898,"shasum":"3b02ee73f26fcc6a782a8cb6b3af22b6e7d2be7d"},{"_id":"public/images/wpid-Multi-reactors3.png","modified":1451751375900,"shasum":"c18acca0c970ff38beb9605f03819eae331a0d1b"},{"_id":"public/images/wpid-Channel.png","modified":1451751375902,"shasum":"06acf8025d55d644ecb8d153a62c438ce6f07b8f"},{"_id":"public/images/solr-DocSet.png","modified":1451751375912,"shasum":"08891f7f7c7f3ee40161f9b8a01012249e5d9151"},{"_id":"public/images/reactors-in-threads.png","modified":1451751375914,"shasum":"31e20bc84b78d60af27998c0a154014babfb09dd"},{"_id":"public/images/reactors-in-threads-thread-pool.png","modified":1451751375916,"shasum":"cf1805741cd76ae36b85f51e3b5c455c418fa910"},{"_id":"public/images/reactor-thread-pool.png","modified":1451751375918,"shasum":"2698dc114f10fdddef468dd49f23338f743c319e"},{"_id":"public/images/reactor-single-thread.png","modified":1451751375920,"shasum":"799dbea22e802bf51877500c9d7a80621baadc44"},{"_id":"public/images/ranksystem-in-social-network.png","modified":1451751375922,"shasum":"6250cb26042719803f5c641cc14ef10401c8675e"},{"_id":"public/images/facet-1.png","modified":1451751375926,"shasum":"938bd19a87b24de5485960176c817bdac85b6093"},{"_id":"public/images/bytebuf-priciple.png","modified":1451751375928,"shasum":"f2abcaa081d341f8b6aaa1c7c19e7c562329475b"},{"_id":"public/images/bytebuf-diagram.jpg","modified":1451751375930,"shasum":"81f2c3c1b19f01d6af05b2de19124c93ea3cad23"},{"_id":"public/images/bytebuf-combine-slice-buffer.png","modified":1451751375932,"shasum":"a073deecce1516d12e2270f373ccfa8bc57b87fd"},{"_id":"public/images/2014/05/reactors-in-threads.png","modified":1451751375936,"shasum":"31e20bc84b78d60af27998c0a154014babfb09dd"},{"_id":"public/images/2014/05/reactors-in-threads-thread-pool.png","modified":1451751375938,"shasum":"cf1805741cd76ae36b85f51e3b5c455c418fa910"},{"_id":"public/images/2014/05/reactor-thread-pool.png","modified":1451751375939,"shasum":"2698dc114f10fdddef468dd49f23338f743c319e"},{"_id":"public/images/2014/05/reactor-single-thread.png","modified":1451751375941,"shasum":"799dbea22e802bf51877500c9d7a80621baadc44"},{"_id":"public/images/2014/01/wpid-nio-oio1.jpg","modified":1451751375943,"shasum":"6b6e43e7a96dd83b11e4f63b1bcfb9668ddefbf2"},{"_id":"public/images/2014/01/wpid-nio-oio.jpg","modified":1451751375947,"shasum":"ac7df496f41f1b549b41a4a4404a437ea18460c9"},{"_id":"public/images/2014/01/wpid-diagram1.jpg","modified":1451751375949,"shasum":"81f2c3c1b19f01d6af05b2de19124c93ea3cad23"},{"_id":"public/images/2014/01/wpid-diagram.jpg","modified":1451751375950,"shasum":"81f2c3c1b19f01d6af05b2de19124c93ea3cad23"},{"_id":"public/images/2014/01/wpid-combine-slice-buffer.png","modified":1451751375951,"shasum":"a073deecce1516d12e2270f373ccfa8bc57b87fd"},{"_id":"public/images/2014/01/wpid-bytebuf.png","modified":1451751375953,"shasum":"f2abcaa081d341f8b6aaa1c7c19e7c562329475b"},{"_id":"public/images/2014/01/wpid-Netty-thread-model3.png","modified":1451751375954,"shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79"},{"_id":"public/images/2014/01/wpid-Netty-thread-model2.png","modified":1451751375956,"shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79"},{"_id":"public/images/2014/01/wpid-Netty-thread-model1.png","modified":1451751375959,"shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79"},{"_id":"public/images/2014/01/wpid-Netty-thread-model.png","modified":1451751375961,"shasum":"c55d7b5776528103b4e6e207d8d44aa09d7faf79"},{"_id":"public/images/2014/01/wpid-Netty-ChannelPipeline-1.png","modified":1451751375963,"shasum":"8fb9f8bfe8c3c82012789e93c9ed0d3742cf2998"},{"_id":"public/images/2014/01/wpid-Netty-ChannelPipeline-.png","modified":1451751375965,"shasum":"3b02ee73f26fcc6a782a8cb6b3af22b6e7d2be7d"},{"_id":"public/images/2014/01/wpid-Multi-reactors3.png","modified":1451751375967,"shasum":"c18acca0c970ff38beb9605f03819eae331a0d1b"},{"_id":"public/images/2014/01/wpid-Multi-reactors2.png","modified":1451751375969,"shasum":"720760fd57dafcff8ef831e39ce7e179d7d52f2f"},{"_id":"public/images/2014/01/wpid-Multi-reactors1.png","modified":1451751375971,"shasum":"192bf67e9fd4a26900c129d4db4b8e611b5629b0"},{"_id":"public/images/2014/01/wpid-Multi-reactors.png","modified":1451751375973,"shasum":"7220e5db283093ba134a0b8ac93f45c4b0ebcbdc"},{"_id":"public/images/2014/01/wpid-Channel.png","modified":1451751375975,"shasum":"06acf8025d55d644ecb8d153a62c438ce6f07b8f"},{"_id":"public/images/2014/01/icon1.png","modified":1451751375976,"shasum":"716496939c0b0a965fb3c7ec9096f2e7a4fb1810"},{"_id":"public/images/2014/01/icon.png","modified":1451751375977,"shasum":"ec9a4bb4b2dcf5ae30a74710ae623a394af94165"},{"_id":"public/images/2014/01/favicon.png","modified":1451751375979,"shasum":"6cdd78df2d91e397f08a981df6e5bb58896f1f80"},{"_id":"public/images/2014/01/e.png","modified":1451751375980,"shasum":"f38f376c44ab4c307cf5208a521e59b7fd1c22a6"},{"_id":"public/images/2013/10/Image_thumb.png","modified":1451751375981,"shasum":"0a9571d46098c17aa1ae400465cd609943b35ad2"},{"_id":"public/images/2013/10/Image4_thumb.png","modified":1451751375982,"shasum":"c0f75c34f88dda8dc1e83b93c4ece6ecacaafffb"},{"_id":"public/images/2013/10/Image4.png","modified":1451751375984,"shasum":"c0f75c34f88dda8dc1e83b93c4ece6ecacaafffb"},{"_id":"public/images/2013/10/Image3_thumb.png","modified":1451751375985,"shasum":"cef190cdde4f0879911fc6d1805791ef0cfeab2e"},{"_id":"public/images/2013/10/Image3.png","modified":1451751375986,"shasum":"cef190cdde4f0879911fc6d1805791ef0cfeab2e"},{"_id":"public/images/2013/10/Image2_thumb.png","modified":1451751375987,"shasum":"5912532a4058c8a90da1774ed2ccf6f63b878f15"},{"_id":"public/images/2013/10/Image2.png","modified":1451751375988,"shasum":"5912532a4058c8a90da1774ed2ccf6f63b878f15"},{"_id":"public/images/2013/10/Image1_thumb.png","modified":1451751375989,"shasum":"f2ec479480662ce8da00dcaa93a4d3b18c280efc"},{"_id":"public/images/2013/10/Image1.png","modified":1451751375996,"shasum":"f2ec479480662ce8da00dcaa93a4d3b18c280efc"},{"_id":"public/images/2013/10/Image.png","modified":1451751375998,"shasum":"0a9571d46098c17aa1ae400465cd609943b35ad2"},{"_id":"public/images/2013/08/image_thumb.png","modified":1451751375999,"shasum":"b70606cdab5cceeffd98740b6ee18d9fda954c25"},{"_id":"public/images/2013/08/image.png","modified":1451751376001,"shasum":"553edaa340cae996da369b9a2830af99b8053778"},{"_id":"public/images/2013/08/QQ20130831103315_thumb.jpg","modified":1451751376003,"shasum":"9f65d0e3ec9ad186766da8fe25097e34355f0807"},{"_id":"public/images/2013/08/QQ20130831103315.jpg","modified":1451751376004,"shasum":"70d5654d4d6a393e7d49d56bf5c642899dcb8148"},{"_id":"public/images/2013/04/DocSet_thumb.png","modified":1451751376006,"shasum":"3fdcb297cb89b11a71563e93953edd0a363538e3"},{"_id":"public/images/2013/04/DocSet.png","modified":1451751376010,"shasum":"08891f7f7c7f3ee40161f9b8a01012249e5d9151"},{"_id":"public/images/2013/03/thumb.jpg","modified":1451751376012,"shasum":"63d503070b625764aff6a829b4b33235b5c9e548"},{"_id":"public/images/2013/03/image_thumb1.png","modified":1451751376074,"shasum":"938bd19a87b24de5485960176c817bdac85b6093"},{"_id":"public/images/2013/03/image_thumb.png","modified":1451751376077,"shasum":"93d094420b3319853eff3887f15ecbdaf88970db"},{"_id":"public/images/2013/03/image1.png","modified":1451751376079,"shasum":"938bd19a87b24de5485960176c817bdac85b6093"},{"_id":"public/images/2013/03/image.png","modified":1451751376083,"shasum":"93d094420b3319853eff3887f15ecbdaf88970db"},{"_id":"public/images/2013/03/POqWJ.png","modified":1451751376086,"shasum":"0eae0d7a73b610190aadc8932c09434fb8abf98b"},{"_id":"public/images/2013/03/201201051.png","modified":1451751376087,"shasum":"03896f69e1f4742c999c4a915e47d9b712d9e262"},{"_id":"public/images/2013/03/1_thumb.jpg","modified":1451751376089,"shasum":"b7091eb744aafbf947895a6f0791706ce5d6fb8f"},{"_id":"public/images/2013/03/163306e8e326.jpg","modified":1451751376090,"shasum":"6d79054352936d30197e3fab9ef0846ebd7465cd"},{"_id":"public/images/2013/03/12.jpg","modified":1451751376091,"shasum":"fceaccd5b97d79ae7e2bd3b3bd90ba6eb45af218"},{"_id":"public/images/2013/03/11.jpg","modified":1451751376091,"shasum":"313f0ffca4496858ec127799f602cde0690a803c"},{"_id":"public/images/2013/03/1.jpg","modified":1451751376092,"shasum":"6ef5c815e053159040d2a32132e8a26b5be06cbc"},{"_id":"public/images/2013/01/image_thumb.png","modified":1451751376093,"shasum":"83031ca8e39e0fbc66eac549a3bbdd05a52df0c6"},{"_id":"public/images/2013/01/image.png","modified":1451751376094,"shasum":"2de71610aff991609f60397bc7d64e4c293f1c1f"},{"_id":"public/images/2013/01/SavedPicture-2013123102218.jpg","modified":1451751376095,"shasum":"f0c7730e07986abf5b71fb0c9cc979c132d56fa1"},{"_id":"public/images/2012/10/clip_image0074_thumb1.jpg","modified":1451751376096,"shasum":"5a2c832cb3366482f250989a078ef598efcea533"},{"_id":"public/images/2012/10/clip_image0074_thumb.jpg","modified":1451751376097,"shasum":"4574179180a12f170bd1744afcf7fef7b6e964f2"},{"_id":"public/images/2012/10/clip_image00741.jpg","modified":1451751376098,"shasum":"8678cec0c56cfebf5529b7561c72436263354bde"},{"_id":"public/images/2012/10/clip_image0074.jpg","modified":1451751376099,"shasum":"8678cec0c56cfebf5529b7561c72436263354bde"},{"_id":"public/images/2012/10/clip_image0054_thumb.jpg","modified":1451751376100,"shasum":"3f80076600d98cf0d584e11dbbe3d913174cf266"},{"_id":"public/images/2012/10/clip_image0054.jpg","modified":1451751376101,"shasum":"6e44f6676958342ecefce17604a83b6f4d57feaa"},{"_id":"public/images/2012/10/clip_image0034_thumb.jpg","modified":1451751376102,"shasum":"cc83cfdbfe92568e82d60e384221858566d39ec2"},{"_id":"public/images/2012/10/clip_image0034.jpg","modified":1451751376103,"shasum":"24f6ab225443c52c619410e9d276b6ef54d3b14a"},{"_id":"public/images/2012/10/clip_image0014_thumb.jpg","modified":1451751376104,"shasum":"7dca37fb15bcf57f60b52ccce1a24080c203dbfd"},{"_id":"public/images/2012/10/clip_image0014.jpg","modified":1451751376105,"shasum":"b4f1adcf8a0c8435a93a8de13cda8d6e2cdf8222"},{"_id":"public/images/2012/10/Image_thumb2.jpg","modified":1451751376106,"shasum":"68a48f2e700de8ed3bbb355b3c7ccbd8d628023a"},{"_id":"public/images/2012/10/Image_thumb1.jpg","modified":1451751376106,"shasum":"7784cda2ee30d9ea440d5a8253614505d9dde6b1"},{"_id":"public/images/2012/10/Image_thumb.jpg","modified":1451751376107,"shasum":"7d2a4856c78b1f5bf49ff794b1c3710a864a703c"},{"_id":"public/images/2012/10/Image2.jpg","modified":1451751376108,"shasum":"941934f73c977a4eff64f941617bdcd3922b38bc"},{"_id":"public/images/2012/10/Image1.jpg","modified":1451751376109,"shasum":"d3e7624b85605e875ec1fa8361ae36d0d380309d"},{"_id":"public/images/2012/10/Image.jpg","modified":1451751376111,"shasum":"ee2a4583b4d17a3acee44176dcb7a0b2dd3ef399"},{"_id":"public/images/2012/09/YARNArch_thumb1.png","modified":1451751376112,"shasum":"d78fbdab15ec4dc68d4b9068c8f3003657de1a6a"},{"_id":"public/images/2012/09/YARNArch_thumb.png","modified":1451751376114,"shasum":"d78fbdab15ec4dc68d4b9068c8f3003657de1a6a"},{"_id":"public/images/2012/09/YARNArch.png","modified":1451751376115,"shasum":"d78fbdab15ec4dc68d4b9068c8f3003657de1a6a"},{"_id":"public/images/2012/09/Image_thumb1.png","modified":1451751376116,"shasum":"08e9ffbea67f103f6b4c535caf22d23aacdaf482"},{"_id":"public/images/2012/09/Image_thumb.png","modified":1451751376117,"shasum":"08e9ffbea67f103f6b4c535caf22d23aacdaf482"},{"_id":"public/images/2012/09/Image.png","modified":1451751376118,"shasum":"fe48fa243251e3e2eaec878e7ba8c60e4da5fcbb"},{"_id":"public/images/2012/07/BigTablebasedRangePartitioning_thumb.png","modified":1451751376119,"shasum":"d718d1c24300ac26d354c4014865cf54ee0c498c"},{"_id":"public/images/2012/07/BigTablebasedRangePartitioning.png","modified":1451751376120,"shasum":"5d6d2edfc4119b10ebaf987fffdfc542e6bb4c35"},{"_id":"public/images/2012/07/ADistributedHashTableRing_thumb1.png","modified":1451751376121,"shasum":"fd23b6fd24140414f34e730f24b3589fe952065f"},{"_id":"public/images/2012/07/ADistributedHashTableRing_thumb.png","modified":1451751376122,"shasum":"fd23b6fd24140414f34e730f24b3589fe952065f"},{"_id":"public/images/2012/07/ADistributedHashTableRing.png","modified":1451751376123,"shasum":"fd23b6fd24140414f34e730f24b3589fe952065f"},{"_id":"public/images/2012/06/clip_image017_thumb.jpg","modified":1451751376133,"shasum":"072ccce367e88ff765490ac1650a5e859a974ffa"},{"_id":"public/images/2012/06/clip_image017.jpg","modified":1451751376134,"shasum":"072ccce367e88ff765490ac1650a5e859a974ffa"},{"_id":"public/images/2012/06/clip_image015_thumb.jpg","modified":1451751376135,"shasum":"10dbcadc81aa1fc38b92c56e04130fd6cc8024ee"},{"_id":"public/images/2012/06/clip_image015.jpg","modified":1451751376137,"shasum":"10dbcadc81aa1fc38b92c56e04130fd6cc8024ee"},{"_id":"public/images/2012/06/clip_image013_thumb.jpg","modified":1451751376137,"shasum":"15ff24c36b2445e25fd042764d6162c21b27a941"},{"_id":"public/images/2012/06/clip_image013.jpg","modified":1451751376138,"shasum":"15ff24c36b2445e25fd042764d6162c21b27a941"},{"_id":"public/images/2012/06/clip_image011_thumb.jpg","modified":1451751376139,"shasum":"e149bec5527843850753e916a446d35c12d90732"},{"_id":"public/images/2012/06/clip_image011.jpg","modified":1451751376140,"shasum":"e149bec5527843850753e916a446d35c12d90732"},{"_id":"public/images/2012/06/clip_image009_thumb.jpg","modified":1451751376142,"shasum":"d9d3a25bec46e34a583326cf0fc27007edb0b12d"},{"_id":"public/images/2012/06/clip_image009.jpg","modified":1451751376143,"shasum":"d9d3a25bec46e34a583326cf0fc27007edb0b12d"},{"_id":"public/images/2012/06/clip_image007_thumb.jpg","modified":1451751376145,"shasum":"1fa5711dfc217d445f6c6a8f2ee876e4f0ddbcfc"},{"_id":"public/images/2012/06/clip_image007.jpg","modified":1451751376146,"shasum":"1fa5711dfc217d445f6c6a8f2ee876e4f0ddbcfc"},{"_id":"public/images/2012/06/clip_image005_thumb.jpg","modified":1451751376147,"shasum":"48d59fc9c6042dfdb52ef3060821bec3762f0c51"},{"_id":"public/images/2012/06/clip_image005.jpg","modified":1451751376148,"shasum":"48d59fc9c6042dfdb52ef3060821bec3762f0c51"},{"_id":"public/images/2012/06/clip_image003_thumb1.jpg","modified":1451751376149,"shasum":"b3f70e473e626b041c5ab20a1f1d5c5885c80759"},{"_id":"public/images/2012/06/clip_image003_thumb.jpg","modified":1451751376150,"shasum":"b3f70e473e626b041c5ab20a1f1d5c5885c80759"},{"_id":"public/images/2012/06/clip_image003.jpg","modified":1451751376151,"shasum":"b3f70e473e626b041c5ab20a1f1d5c5885c80759"},{"_id":"public/images/2012/06/clip_image001_thumb.jpg","modified":1451751376151,"shasum":"d965ab04b3611481137d6c701a743e0149a801ac"},{"_id":"public/images/2012/06/clip_image001.jpg","modified":1451751376152,"shasum":"d965ab04b3611481137d6c701a743e0149a801ac"},{"_id":"public/images/2012/05/image_thumb2.png","modified":1451751376153,"shasum":"99a2adc8c84be74a923d4b4f4db422921ee1948d"},{"_id":"public/images/2012/05/image_thumb1.png","modified":1451751376154,"shasum":"3d2a6e55d6f76a10216693f4aabf68cb864e9e24"},{"_id":"public/images/2012/05/image_thumb.png","modified":1451751376155,"shasum":"99a2adc8c84be74a923d4b4f4db422921ee1948d"},{"_id":"public/images/2012/05/image1.png","modified":1451751376156,"shasum":"3d2a6e55d6f76a10216693f4aabf68cb864e9e24"},{"_id":"public/images/2012/05/image.png","modified":1451751376157,"shasum":"99a2adc8c84be74a923d4b4f4db422921ee1948d"},{"_id":"public/images/2012/04/clip_image011.jpg","modified":1451751376170,"shasum":"5fe890d2f1a75bb14aec5ce18fb26f17d7d26d64"},{"_id":"public/images/2012/04/clip_image009.jpg","modified":1451751376171,"shasum":"8a5ffee5f2a711636b8cfd16fab8cd839fbcc1f2"},{"_id":"public/images/2012/04/clip_image007.jpg","modified":1451751376175,"shasum":"20f7ffb693a7703de79b04932b20814284e1aabc"},{"_id":"public/images/2012/04/clip_image005_thumb1.jpg","modified":1451751376177,"shasum":"7c1ab1ac0bda4ccd03c5af62f4bc5a7c1c567359"},{"_id":"public/images/2012/04/clip_image005_thumb.jpg","modified":1451751376178,"shasum":"7c1ab1ac0bda4ccd03c5af62f4bc5a7c1c567359"},{"_id":"public/images/2012/04/clip_image005.jpg","modified":1451751376179,"shasum":"c2f5e532c5888f197d3418fe33350e7fe93b5549"},{"_id":"public/images/2012/04/clip_image004.jpg","modified":1451751376180,"shasum":"1a8c0450e687ce88e41c64d3304067b1df652a5e"},{"_id":"public/images/2012/04/clip_image002.jpg","modified":1451751376181,"shasum":"d9dbb846d1a6a63beaa284a88c02f0e684debad9"},{"_id":"public/images/2012/04/201201051.png","modified":1451751376182,"shasum":"03896f69e1f4742c999c4a915e47d9b712d9e262"},{"_id":"public/images/2012/03/image_thumb9.png","modified":1451751376183,"shasum":"d57c64f9ad1bbcffbe9c97a55dcd75850573b473"},{"_id":"public/images/2012/03/image_thumb8.png","modified":1451751376184,"shasum":"15f7b86b844beb2e8b910c7c7b64bcb7c379f442"},{"_id":"public/images/2012/03/image_thumb7.png","modified":1451751376185,"shasum":"7ad5e28317aaf8bfdedb1c614e8884fa5fccd49f"},{"_id":"public/images/2012/03/image_thumb61.png","modified":1451751376186,"shasum":"52efe11382790cf209ed173036345af8540f008b"},{"_id":"public/images/2012/03/image_thumb6.png","modified":1451751376188,"shasum":"52efe11382790cf209ed173036345af8540f008b"},{"_id":"public/images/2012/03/image_thumb5.png","modified":1451751376189,"shasum":"637d07fdc4705d41f55cd5ec997f51548525bfda"},{"_id":"public/images/2012/03/image_thumb4.png","modified":1451751376190,"shasum":"bd5b82b3353b73dc010af4b4d769e08c8ea4a8ab"},{"_id":"public/images/2012/03/image_thumb3.png","modified":1451751376191,"shasum":"c4b9438398ff0665f1f4ce0c6c392a6fa5c7ab11"},{"_id":"public/images/2012/03/image_thumb2.png","modified":1451751376193,"shasum":"cb958188a7a0c04bf2477b4a9ae336cf34938551"},{"_id":"public/images/2012/03/image_thumb11.png","modified":1451751376194,"shasum":"bdc4785520eef15621503d27c552a9757bb3f55b"},{"_id":"public/images/2012/03/image_thumb10.png","modified":1451751376197,"shasum":"859f13eef8fcfeb85c2b4311aeaf9e69aa3de4f0"},{"_id":"public/images/2012/03/image_thumb1.png","modified":1451751376198,"shasum":"bdc4785520eef15621503d27c552a9757bb3f55b"},{"_id":"public/images/2012/03/image_thumb.png","modified":1451751376199,"shasum":"7585ee5ed3df2715a13c7c68182d5249b5747842"},{"_id":"public/images/2012/03/image9.png","modified":1451751376201,"shasum":"d57c64f9ad1bbcffbe9c97a55dcd75850573b473"},{"_id":"public/images/2012/03/image8.png","modified":1451751376202,"shasum":"15f7b86b844beb2e8b910c7c7b64bcb7c379f442"},{"_id":"public/images/2012/03/image7.png","modified":1451751376204,"shasum":"7ad5e28317aaf8bfdedb1c614e8884fa5fccd49f"},{"_id":"public/images/2012/03/image6.png","modified":1451751376205,"shasum":"52efe11382790cf209ed173036345af8540f008b"},{"_id":"public/images/2012/03/image5.png","modified":1451751376206,"shasum":"5f8293231fb845e5c27ad4366c89b07d3d0f53b3"},{"_id":"public/images/2012/03/image4.png","modified":1451751376207,"shasum":"8ad5195a1a96c47a988a925b9d37c7ce27538bb4"},{"_id":"public/images/2012/03/image3.png","modified":1451751376208,"shasum":"c4b9438398ff0665f1f4ce0c6c392a6fa5c7ab11"},{"_id":"public/images/2012/03/image2.png","modified":1451751376209,"shasum":"cb958188a7a0c04bf2477b4a9ae336cf34938551"},{"_id":"public/images/2012/03/image10.png","modified":1451751376210,"shasum":"859f13eef8fcfeb85c2b4311aeaf9e69aa3de4f0"},{"_id":"public/images/2012/03/image1.png","modified":1451751376211,"shasum":"8a84a4f2a100170eda650f76023bea3c36db979b"},{"_id":"public/images/2012/03/image.png","modified":1451751376212,"shasum":"b9a5c4d09646c6a7ad2a01f346be9a3179566d16"},{"_id":"public/images/2012/03/clip_image006_thumb1.jpg","modified":1451751376213,"shasum":"b06606bec34138a24d08a0e1f4944cbec2cafe2a"},{"_id":"public/images/2012/03/clip_image006_thumb.jpg","modified":1451751376215,"shasum":"b06606bec34138a24d08a0e1f4944cbec2cafe2a"},{"_id":"public/images/2012/03/clip_image006.jpg","modified":1451751376216,"shasum":"b06606bec34138a24d08a0e1f4944cbec2cafe2a"},{"_id":"public/images/2012/03/clip_image005_thumb.jpg","modified":1451751376217,"shasum":"8dbed96cd7c32f0fa95bbcccacad144519971bf6"},{"_id":"public/images/2012/03/clip_image005.jpg","modified":1451751376218,"shasum":"8dbed96cd7c32f0fa95bbcccacad144519971bf6"},{"_id":"public/images/2012/03/clip_image003_thumb1.jpg","modified":1451751376219,"shasum":"04d2f4dc0101bf9a379a0b79bf7f141ea0e83c40"},{"_id":"public/images/2012/03/clip_image003_thumb.jpg","modified":1451751376220,"shasum":"04d2f4dc0101bf9a379a0b79bf7f141ea0e83c40"},{"_id":"public/images/2012/03/clip_image003.jpg","modified":1451751376231,"shasum":"04d2f4dc0101bf9a379a0b79bf7f141ea0e83c40"},{"_id":"public/images/2012/03/clip_image002_thumb.jpg","modified":1451751376232,"shasum":"325780e332a50f6c02797b7fc3c7233b080aee60"},{"_id":"public/images/2012/03/clip_image002.jpg","modified":1451751376233,"shasum":"325780e332a50f6c02797b7fc3c7233b080aee60"},{"_id":"public/images/2012/03/SQLHadoop_thumb.jpg","modified":1451751376234,"shasum":"4439674c2bac37078115fae030cc3e5fbbc4e55f"},{"_id":"public/images/2012/03/SQLHadoop.jpg","modified":1451751376235,"shasum":"a1417b1af81fdc84f6e49bb62b87443f75bff70d"},{"_id":"public/images/2012/03/POqWJ.png","modified":1451751376238,"shasum":"0eae0d7a73b610190aadc8932c09434fb8abf98b"},{"_id":"public/images/2012/03/NoSQL_thumb.png","modified":1451751376240,"shasum":"fec0eb5d77a5b76d8ad875d4f5cda0498fd36bf1"},{"_id":"public/images/2012/03/CAP_thumb.png","modified":1451751376241,"shasum":"e45054b100e84dc10c528d24205c122192895196"},{"_id":"public/images/2012/02/image_thumb3.png","modified":1451751376242,"shasum":"02b1528c983b043cef0127a93dc3101338189e9b"},{"_id":"public/images/2012/02/image_thumb2.png","modified":1451751376244,"shasum":"a72f4d771698d16a3b6f7a318145241d8ca3c81f"},{"_id":"public/images/2012/02/image_thumb11.png","modified":1451751376245,"shasum":"541f1ea432ffa3d7d813331fdfe0c0f31352790e"},{"_id":"public/images/2012/02/image_thumb1.png","modified":1451751376246,"shasum":"541f1ea432ffa3d7d813331fdfe0c0f31352790e"},{"_id":"public/images/2012/02/image_thumb.png","modified":1451751376247,"shasum":"02b1528c983b043cef0127a93dc3101338189e9b"},{"_id":"public/images/2012/02/image2.png","modified":1451751376248,"shasum":"a72f4d771698d16a3b6f7a318145241d8ca3c81f"},{"_id":"public/images/2012/02/image1.png","modified":1451751376249,"shasum":"541f1ea432ffa3d7d813331fdfe0c0f31352790e"},{"_id":"public/images/2012/02/image.png","modified":1451751376249,"shasum":"02b1528c983b043cef0127a93dc3101338189e9b"},{"_id":"public/images/2012/02/clip_image004_thumb3.jpg","modified":1451751376250,"shasum":"6e875577a50bcd7dee405243e91d69ca4ccb57d9"},{"_id":"public/images/2012/02/clip_image004_thumb2.jpg","modified":1451751376251,"shasum":"bbd80e60d21d82b4cc39a4aba095a8c838981c34"},{"_id":"public/images/2012/02/clip_image004_thumb1.jpg","modified":1451751376252,"shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4"},{"_id":"public/images/2012/02/clip_image004_thumb.jpg","modified":1451751376253,"shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4"},{"_id":"public/images/2012/02/clip_image0046_thumb.jpg","modified":1451751376254,"shasum":"54cec04992d1202e349a6182ebabfc6a63429bd7"},{"_id":"public/images/2012/02/clip_image0046.jpg","modified":1451751376264,"shasum":"54cec04992d1202e349a6182ebabfc6a63429bd7"},{"_id":"public/images/2012/02/clip_image0043.jpg","modified":1451751376265,"shasum":"6e875577a50bcd7dee405243e91d69ca4ccb57d9"},{"_id":"public/images/2012/02/clip_image0042.jpg","modified":1451751376270,"shasum":"bbd80e60d21d82b4cc39a4aba095a8c838981c34"},{"_id":"public/images/2012/02/clip_image0041.jpg","modified":1451751376271,"shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4"},{"_id":"public/images/2012/02/clip_image004.jpg","modified":1451751376272,"shasum":"c074c2cf0ad7f44ee08777e810f6fa5489b25bc4"},{"_id":"public/images/2012/02/clip_image002_thumb5.jpg","modified":1451751376274,"shasum":"c7ec9214889032bb9ad8344d7abcd026a3378a7d"},{"_id":"public/images/2012/02/clip_image002_thumb41.jpg","modified":1451751376276,"shasum":"275b69d040221d290e2d168f2c6a6a6b1c845ce6"},{"_id":"public/images/2012/02/clip_image002_thumb4.jpg","modified":1451751376277,"shasum":"275b69d040221d290e2d168f2c6a6a6b1c845ce6"},{"_id":"public/images/2012/02/clip_image002_thumb31.jpg","modified":1451751376279,"shasum":"3dd68989dd90210f6ae3e9e1d983ddc2f988c57e"},{"_id":"public/images/2012/02/clip_image002_thumb3.jpg","modified":1451751376280,"shasum":"3dd68989dd90210f6ae3e9e1d983ddc2f988c57e"},{"_id":"public/images/2012/02/clip_image002_thumb21.jpg","modified":1451751376281,"shasum":"8d22940cd711972e2a6db714fc3bd332b5de9fb5"},{"_id":"public/images/2012/02/clip_image002_thumb2.jpg","modified":1451751376282,"shasum":"8d22940cd711972e2a6db714fc3bd332b5de9fb5"},{"_id":"public/images/2012/02/clip_image002_thumb1.jpg","modified":1451751376283,"shasum":"aa3d36e4dde0e8d1b999e86d0983ac1b20ab7e2c"},{"_id":"public/images/2012/02/clip_image002_thumb.jpg","modified":1451751376284,"shasum":"c7ec9214889032bb9ad8344d7abcd026a3378a7d"},{"_id":"public/images/2012/02/clip_image002_thumb.gif","modified":1451751376285,"shasum":"99b0acc08c43b2476a60325bb416ef1b6045cbf5"},{"_id":"public/images/2012/02/clip_image0026_thumb1.jpg","modified":1451751376285,"shasum":"ab4dc27026bb1c747e61fd01f81dcf612c5eacc3"},{"_id":"public/images/2012/02/clip_image0026_thumb.jpg","modified":1451751376286,"shasum":"ab4dc27026bb1c747e61fd01f81dcf612c5eacc3"},{"_id":"public/images/2012/02/clip_image0026.jpg","modified":1451751376287,"shasum":"ab4dc27026bb1c747e61fd01f81dcf612c5eacc3"},{"_id":"public/images/2012/02/clip_image0024.jpg","modified":1451751376289,"shasum":"275b69d040221d290e2d168f2c6a6a6b1c845ce6"},{"_id":"public/images/2012/02/clip_image0023.jpg","modified":1451751376289,"shasum":"3dd68989dd90210f6ae3e9e1d983ddc2f988c57e"},{"_id":"public/images/2012/02/clip_image0022.jpg","modified":1451751376290,"shasum":"8d22940cd711972e2a6db714fc3bd332b5de9fb5"},{"_id":"public/images/2012/02/clip_image0021.jpg","modified":1451751376291,"shasum":"aa3d36e4dde0e8d1b999e86d0983ac1b20ab7e2c"},{"_id":"public/images/2012/02/clip_image002.jpg","modified":1451751376292,"shasum":"c7ec9214889032bb9ad8344d7abcd026a3378a7d"},{"_id":"public/images/2012/02/clip_image002.gif","modified":1451751376293,"shasum":"99b0acc08c43b2476a60325bb416ef1b6045cbf5"},{"_id":"public/images/2011/09/cem3Ejwh9cAac.gif","modified":1451751376295,"shasum":"090944ce94f1dcd17261844ae413738739c23f82"},{"_id":"public/images/2011/09/MUXFRHT_O3VBF79.jpg","modified":1451751376296,"shasum":"06df5e3d88f12f65755cbdb4ce51f170a83521a7"},{"_id":"public/images/2011/09/4d0ebb45de203b32cefca30d_thumb1.jpg","modified":1451751376297,"shasum":"f7ad545e02d5ad2b2d7fae16ea94c16050d941d2"},{"_id":"public/images/2011/09/4d0ebb45de203b32cefca30d_thumb.jpg","modified":1451751376298,"shasum":"f7ad545e02d5ad2b2d7fae16ea94c16050d941d2"},{"_id":"public/images/2011/09/4d0ebb45de203b32cefca30d.jpg","modified":1451751376299,"shasum":"080e358e8e67e087abad69b70e18ae908bbc5380"},{"_id":"public/images/2011/07/Android-Res11.png","modified":1451751376300,"shasum":"141c50584be61aaa984ce0be0b65a7c2ca53db80"},{"_id":"public/images/2011/07/Android-Res1.png","modified":1451751376301,"shasum":"141c50584be61aaa984ce0be0b65a7c2ca53db80"},{"_id":"public/images/2011/07/Android-Res.png","modified":1451751376302,"shasum":"dfe817cc2d9e50c9fc8c97cc7d092fde5600e8cc"},{"_id":"public/images/2011/03/image_thumb2.png","modified":1451751376303,"shasum":"4eaf9814bf07e4956ddc6ac1d25e556696b592c0"},{"_id":"public/images/2011/03/image_thumb1.png","modified":1451751376305,"shasum":"fa2a8141c763f42cc2b99e307989d0e896243a86"},{"_id":"public/images/2011/03/image_thumb.png","modified":1451751376307,"shasum":"4eaf9814bf07e4956ddc6ac1d25e556696b592c0"},{"_id":"public/images/2011/03/image1.png","modified":1451751376308,"shasum":"3660b360e73275b3e59a715c609fb71932ee5c35"},{"_id":"public/images/2011/03/image.png","modified":1451751376310,"shasum":"6250cb26042719803f5c641cc14ef10401c8675e"},{"_id":"public/images/2011/03/clip_image004_thumb.jpg","modified":1451751376312,"shasum":"72be51f60526a32129cd1cfd399106e2087bd246"},{"_id":"public/images/2011/03/clip_image004.jpg","modified":1451751376312,"shasum":"72be51f60526a32129cd1cfd399106e2087bd246"},{"_id":"public/images/2011/03/clip_image003_thumb.png","modified":1451751376313,"shasum":"8d15e68e537662a1492f58d657f54eefc0499c23"},{"_id":"public/images/2011/03/clip_image003.png","modified":1451751376314,"shasum":"8d15e68e537662a1492f58d657f54eefc0499c23"},{"_id":"public/images/2011/02/mypic.jpg","modified":1451751376315,"shasum":"6ff67032870dfb7e1aa0ec90f0d7d26547848685"},{"_id":"public/images/2011/02/h_large_goGg_25210000773d2f75.jpg","modified":1451751376329,"shasum":"b6d6800e44218ed44f789634540e94b13815e0da"},{"_id":"public/images/2011/02/about.jpg","modified":1451751376330,"shasum":"9c4dc81ca2c7f421b2decb7a47069830f54a0508"},{"_id":"public/tags/index.html","modified":1451751376468,"shasum":"8df315a6fbb8f8b106b74403624f2372c67a4c5a"},{"_id":"public/images/2012/02/DistributedCache.html","modified":1451751376522,"shasum":"81be680b963cd3080680e249118242643d847b96"},{"_id":"public/2015/12/SSH/index.html","modified":1451751376571,"shasum":"c32aac2db2b984461d8b27d923d7d6502ec34b2a"},{"_id":"public/2015/10/docker-volume-plugin/index.html","modified":1451751376619,"shasum":"00fdce159ffab0ad6a8ca71a8051ad1317674bf6"},{"_id":"public/2015/10/docker-compose-pratice/index.html","modified":1451751376652,"shasum":"1b15b363cb27260c0d690d5f99cd833c7bc016f6"},{"_id":"public/2015/09/remoting-practice/index.html","modified":1451751376694,"shasum":"1f805fb0d64e01147e09c9e75c29a31ab9a8859a"},{"_id":"public/2015/09/java-source-code-practice/index.html","modified":1451751376728,"shasum":"d70148211e757448f396ac6d425f1d017feb0e3b"},{"_id":"public/2015/08/css-practice/index.html","modified":1451751376766,"shasum":"64b967b9c3cc6c9b0021ae0ca42847ae31e0b84d"},{"_id":"public/2015/08/java-practice/index.html","modified":1451751376807,"shasum":"272ad1bdfa1f03066fde43fc13b7416e6664ebad"},{"_id":"public/2015/08/git-practice/index.html","modified":1451751376841,"shasum":"d730a68fabe6ccea934bf243e5c93f7ce8ada181"},{"_id":"public/2015/03/oom-killer-1/index.html","modified":1451751376877,"shasum":"0b0cdbcc61918d9bfa0e8c9040b287eb00c50636"},{"_id":"public/2014/10/i-need-an-answer-now-from-remote/index.html","modified":1451751376912,"shasum":"eb58880b8e5e1fe02844a0137e5b6cbe0750f71d"},{"_id":"public/2014/05/netty-mina-in-depth-2/index.html","modified":1451751376944,"shasum":"633a00f4139562ead23eaf24e73ec638cb8b0b16"},{"_id":"public/2014/05/netty-mina-in-depth-1/index.html","modified":1451751376977,"shasum":"702804ae0f24774faf56a7c4f712f7494e8b17fd"},{"_id":"public/2014/04/rework-digest/index.html","modified":1451751377012,"shasum":"0faad6f41deb87d0cbe166b427c16ed6e544f165"},{"_id":"public/2014/04/forget-your-lusts/index.html","modified":1451751377055,"shasum":"f52026a960eadff886291b177deef631b2e2f451"},{"_id":"public/2014/01/netty-4-x-thread-model/index.html","modified":1451751377102,"shasum":"2767b62abfab89bc0a47edcb5a0a8f9bd6274157"},{"_id":"public/2014/01/netty-4-x-channel-pipeline/index.html","modified":1451751377144,"shasum":"8a35a40baee2abc900552275f91295aac1e32117"},{"_id":"public/2014/01/netty-4-x-bytebuf/index.html","modified":1451751377190,"shasum":"cc8f506b1e3ec01a8cc24bf8e852893b484f2783"},{"_id":"public/2013/11/what-i-think-about-1111/index.html","modified":1451751377238,"shasum":"2590ba6c280a443d2641ac09b9cf0ad7949c1ab0"},{"_id":"public/2013/10/once-java-profiling/index.html","modified":1451751377281,"shasum":"96fb357e67ee94e078a1c49763861fb3e53a4876"},{"_id":"public/2013/08/solr-switch-query-parser/index.html","modified":1451751377330,"shasum":"dffcc2b2a13ba3e68eebc48accc908af4a4960a1"},{"_id":"public/2013/04/apache-solr-data-structrue-part-2/index.html","modified":1451751377385,"shasum":"5fa2e15e4408e542c94a175eeb74bbf24b751a4b"},{"_id":"public/2013/04/apache-solr-data-structrue-part-1/index.html","modified":1451751377414,"shasum":"a6fbf31b8cab527d914afeecbc9917bbced8faf2"},{"_id":"public/2013/04/apache-solr-distributed-search/index.html","modified":1451751377444,"shasum":"ace327f58d20aa5cf8ce0cc16be179632bbaf306"},{"_id":"public/2013/03/apache-solr-facet-pivot-implementation-tranplant/index.html","modified":1451751377477,"shasum":"a83138d3e79f2f77028a95e3b3305dc3d1732693"},{"_id":"public/2013/03/apache-solr-facet-introduction/index.html","modified":1451751377510,"shasum":"8ac790ad918a19ad6c40503c323b90a5ea03cf39"},{"_id":"public/2013/03/maven-repositories/index.html","modified":1451751377543,"shasum":"ba669b1da922194b92477764eb080474df3ef0d7"},{"_id":"public/2013/03/maven-coordinates-dependencies/index.html","modified":1451751377575,"shasum":"cfff43a0bec696640e37a1b439f14b871ba85891"},{"_id":"public/2013/01/linux-process-shell/index.html","modified":1451751377609,"shasum":"54af100587e5bf4b3303aa71053fd93ed3eedba6"},{"_id":"public/2013/01/linux-text-shell/index.html","modified":1451751377638,"shasum":"9b9085842dba8a9f02d9e54b7bb16beb59c2cacf"},{"_id":"public/2013/01/linux-file-directory-shell/index.html","modified":1451751377672,"shasum":"2a13381560ff73998999d98464f1b596d0be323f"},{"_id":"public/2013/01/linux-shell-term-tuning/index.html","modified":1451751377701,"shasum":"dc810e3dbe4923701be2fbaeec711b03522779b2"},{"_id":"public/2013/01/2013-learning-plan/index.html","modified":1451751377738,"shasum":"16dae6d0ef53abeca1d0ca14392bc2293ae67de1"},{"_id":"public/2012/11/zookeeper-ephemeral-nodes-experience/index.html","modified":1451751377766,"shasum":"27a346e04be539bd178cf69b648a00b869964e0d"},{"_id":"public/2012/10/mapreduce-task-src-analysis/index.html","modified":1451751377800,"shasum":"a67583cca511d6af093014f77e42b63ac1fa55e8"},{"_id":"public/2012/09/mapred-optimize-writable/index.html","modified":1451751377831,"shasum":"8f887cc3124b6b8acc35dcc9eeb016d9ecf2babb"},{"_id":"public/2012/09/hadoop-bug-in-text/index.html","modified":1451751377860,"shasum":"5b81b756f0c85ab39499e3b4634dd243aec70992"},{"_id":"public/2012/09/apache-hadoop-yarn-background-and-an-overview/index.html","modified":1451751377894,"shasum":"39d299f5cd98d94c7b042954e1d03fd7aaff4111"},{"_id":"public/2012/06/google-doodle-for-turing/index.html","modified":1451751377924,"shasum":"f92d50960eb2a5943e1eb1ae730bec79b024cf6b"},{"_id":"public/2012/05/hadoop-pipes-src/index.html","modified":1451751377958,"shasum":"84a0580c2077a926aa8c961c55cd8d120991b7c1"},{"_id":"public/2012/05/hadoop-pipes/index.html","modified":1451751377990,"shasum":"3248ed1618c3caf5c9e60e9cd63bcaa37f08b4e8"},{"_id":"public/2012/04/nlp-say-hi/index.html","modified":1451751378023,"shasum":"1dc20f4b22abd38c298542b402c284ab603458d7"},{"_id":"public/2012/04/nlp-repost-semantic/index.html","modified":1451751378056,"shasum":"98cab2098d767af42d353086eae6080cfe0af792"},{"_id":"public/2012/04/nlp-repost-segmentation/index.html","modified":1451751378088,"shasum":"a92540560e1bbc8932c0a4ea83ff16138771ccbd"},{"_id":"public/2012/03/data-structure-disjoint-set/index.html","modified":1451751378120,"shasum":"fe6d13911b500e45c2eacb6d7015b61e32f0c2e3"},{"_id":"public/2012/03/data-structure-skiplists/index.html","modified":1451751378153,"shasum":"9e49e943bd050676c0f08cec365a86c60421ace2"},{"_id":"public/2012/03/redis-data-strutrue/index.html","modified":1451751378187,"shasum":"6f9bd4eb32cc03436650869523ea2993878d7b79"},{"_id":"public/2012/03/data-structure-bitmap/index.html","modified":1451751378215,"shasum":"945effc31bc1e308d4e15800794bd2b310c8294d"},{"_id":"public/2012/03/java-boxing/index.html","modified":1451751378248,"shasum":"b667aaca77a552db17acc3090b376ac83d2371a0"},{"_id":"public/2012/02/iterative-mapred-summary-haloop/index.html","modified":1451751378276,"shasum":"9a0cd9166616f068884af4c58077d1875ea20591"},{"_id":"public/2012/02/hadoop-ipc-server/index.html","modified":1451751378309,"shasum":"e5d35325e9d0e8565e2b1ea925271c893867710b"},{"_id":"public/2012/02/hadoop-ipc-client/index.html","modified":1451751378338,"shasum":"9983e0f90ead78338b1f7f6839cb14900fcce373"},{"_id":"public/2012/02/hadoop-ipc-rpc/index.html","modified":1451751378368,"shasum":"be5e55b53730acdb9996167036c5105c91cf5661"},{"_id":"public/2012/02/iterative-mapred-distcache/index.html","modified":1451751378398,"shasum":"4b8e8afe3c76d71343e32cdeea5cd6341ce72385"},{"_id":"public/2012/02/algorithm-search/index.html","modified":1451751378430,"shasum":"0df41bcd7c13a7a9a33c9d9af635135b4fc48eb5"},{"_id":"public/2012/02/algorithm-probability/index.html","modified":1451751378460,"shasum":"6669c1615265a93924e9f8edf17e9c8188efc558"},{"_id":"public/2012/02/algorithm-dynammic-programming/index.html","modified":1451751378489,"shasum":"f9274181cfe90f625666902ed3741ab53c00ef45"},{"_id":"public/2012/02/jvm-structure/index.html","modified":1451751378521,"shasum":"392313f4ec20434f48fd56bbb3772acb0f052d1a"},{"_id":"public/2012/02/mapred-optimize/index.html","modified":1451751378549,"shasum":"8582bef8c822869fbf301775727fbdcd65264b57"},{"_id":"public/2012/02/iterative-mapred/index.html","modified":1451751378585,"shasum":"3bd64d4408ff6516fb86d3f0454c631a39b181c1"},{"_id":"public/2012/01/python-chat/index.html","modified":1451751378614,"shasum":"29cbb54daa83bb821b060439eceb875bda17d74b"},{"_id":"public/2011/10/dist-filesystem-metadata-server/index.html","modified":1451751378647,"shasum":"6882df0a993e143afe5bbcb8a925624a48b0cc8b"},{"_id":"public/2011/03/java-probability/index.html","modified":1451751378676,"shasum":"c74e986950392af7653108cd319aa6972bc179c9"},{"_id":"public/2011/03/study-school-sociey/index.html","modified":1451751378709,"shasum":"014fd431ed1e97d617bb5e3d72f45de0cc22e8ea"},{"_id":"public/2011/03/english-punctuate/index.html","modified":1451751378744,"shasum":"db90477206fda5edd4bf32790b17a11f4e2aae8e"},{"_id":"public/2011/03/ranksystem-in-social-network/index.html","modified":1451751378774,"shasum":"858abf8f0b150a1d612af75306f7657e6a92bf0e"},{"_id":"public/archives/index.html","modified":1451751378798,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/2/index.html","modified":1451751378830,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/3/index.html","modified":1451751378863,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/4/index.html","modified":1451751378888,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/5/index.html","modified":1451751378917,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/6/index.html","modified":1451751378946,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/7/index.html","modified":1451751378976,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/8/index.html","modified":1451751379014,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/9/index.html","modified":1451751379041,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/page/10/index.html","modified":1451751379067,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2011/index.html","modified":1451751379101,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2011/03/index.html","modified":1451751379129,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2011/10/index.html","modified":1451751379169,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/index.html","modified":1451751379189,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/page/2/index.html","modified":1451751379213,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/page/3/index.html","modified":1451751379235,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/page/4/index.html","modified":1451751379255,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/01/index.html","modified":1451751379281,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/02/index.html","modified":1451751379303,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/02/page/2/index.html","modified":1451751379321,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/03/index.html","modified":1451751379342,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/04/index.html","modified":1451751379365,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/05/index.html","modified":1451751379387,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/06/index.html","modified":1451751379406,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/09/index.html","modified":1451751379424,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/10/index.html","modified":1451751379446,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2012/11/index.html","modified":1451751379470,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/index.html","modified":1451751379488,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/page/2/index.html","modified":1451751379510,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/page/3/index.html","modified":1451751379532,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/01/index.html","modified":1451751379552,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/03/index.html","modified":1451751379576,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/04/index.html","modified":1451751379598,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/08/index.html","modified":1451751379621,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/10/index.html","modified":1451751379646,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2013/11/index.html","modified":1451751379664,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2014/index.html","modified":1451751379682,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2014/page/2/index.html","modified":1451751379703,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2014/01/index.html","modified":1451751379725,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2014/04/index.html","modified":1451751379745,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2014/05/index.html","modified":1451751379766,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2014/10/index.html","modified":1451751379788,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/index.html","modified":1451751379810,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/page/2/index.html","modified":1451751379830,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/03/index.html","modified":1451751379856,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/08/index.html","modified":1451751379875,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/09/index.html","modified":1451751379896,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/10/index.html","modified":1451751379919,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/archives/2015/12/index.html","modified":1451751379938,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/自学资料/index.html","modified":1451751379962,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/最佳实践/index.html","modified":1451751379981,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/生活点滴/index.html","modified":1451751380003,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/index.html","modified":1451751380030,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/page/2/index.html","modified":1451751380049,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/page/3/index.html","modified":1451751380071,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/page/4/index.html","modified":1451751380093,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/page/5/index.html","modified":1451751380113,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/page/6/index.html","modified":1451751380136,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/技术分享/page/7/index.html","modified":1451751380158,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/categories/生活分享/index.html","modified":1451751380177,"shasum":"37c4e82f3220f4a4dc4054facaac3ce4c6b118fb"},{"_id":"public/atom.xml","modified":1451751380179,"shasum":"c8fe1b8d5423a75587cda6e2099ea26a7bb56436"},{"_id":"public/sitemap.xml","modified":1451751380180,"shasum":"8255afa9fa9402b3476eb647a050cce78a1f0918"},{"_id":"public/index.html","modified":1451751380228,"shasum":"270a1eca0f420686ab1dee7a691c8d3c6f2d8058"},{"_id":"public/page/2/index.html","modified":1451751380260,"shasum":"6698a1d724c7aab04c47842483ee5ab7add86000"},{"_id":"public/page/3/index.html","modified":1451751380300,"shasum":"b5b23e1f427fa4e6874e06febd7c38b16fdbc361"},{"_id":"public/page/4/index.html","modified":1451751380339,"shasum":"179f418b6861f8596b24e3fa748126f61f4062b4"},{"_id":"public/page/5/index.html","modified":1451751380375,"shasum":"9b03be15f3427a43515a5e39ef6acff982aaa4ea"},{"_id":"public/page/6/index.html","modified":1451751380414,"shasum":"51df804d37cf20fc057b1d60e5de8ed8d9854951"},{"_id":"public/page/7/index.html","modified":1451751380459,"shasum":"5c6817c1bfbd2820dae37f5b68dc826c52742e9d"},{"_id":"public/page/8/index.html","modified":1451751380506,"shasum":"cdfd0f1ded56aa44b99864cac4e95272b2f2f6d2"},{"_id":"public/page/9/index.html","modified":1451751380554,"shasum":"4ad236054566d2462b89ca7961f0b30bf6182b1d"},{"_id":"public/page/10/index.html","modified":1451751380602,"shasum":"63799a2cec47d525f528c9cd45c2ba756dcd66ef"},{"_id":"public/tags/SSH/index.html","modified":1451751380635,"shasum":"336c64a00bcaeb8353fab8766ec38b41383b450b"},{"_id":"public/tags/Docker/index.html","modified":1451751380661,"shasum":"c9a91d0d03ba904c4473f14a63f2e1818809f648"},{"_id":"public/tags/Docker-plugins/index.html","modified":1451751380692,"shasum":"33a1ff48c5f8cc85046b34ca1d0b0302db9e0cc6"},{"_id":"public/tags/Docker-volume/index.html","modified":1451751380721,"shasum":"ce3badc312865a9917943d84940bd8f27b6c8163"},{"_id":"public/tags/Docker-Compose/index.html","modified":1451751380761,"shasum":"05b20fc4b31b361aa7828fdd73b168020cf08860"},{"_id":"public/tags/Java/index.html","modified":1451751380788,"shasum":"e0cbc8b7e14ed3cd798cb0f7a99fbc1fe745e3e9"},{"_id":"public/tags/Java/page/2/index.html","modified":1451751380824,"shasum":"e0cbc8b7e14ed3cd798cb0f7a99fbc1fe745e3e9"},{"_id":"public/tags/Network/index.html","modified":1451751380842,"shasum":"1f7ed3ff91f8681705edb87e736ef75c119f73cd"},{"_id":"public/tags/源码/index.html","modified":1451751380861,"shasum":"fb037d0f8842ba218452848510bd3140c70aa547"},{"_id":"public/tags/GIT/index.html","modified":1451751380882,"shasum":"2d55449aa919175597a3403b9d10dc7c2006fe0b"},{"_id":"public/tags/CSS/index.html","modified":1451751380901,"shasum":"cc180c831b5bbf29777c5511de4f8aad4c7eb370"},{"_id":"public/tags/Frontend/index.html","modified":1451751380919,"shasum":"95998bb8dc05cd3ea0c280883e2c99cb7c272c39"},{"_id":"public/tags/JVM/index.html","modified":1451751380941,"shasum":"ba2cf6ca5f9751dd93dd5d491e7a7e512dbfafb4"},{"_id":"public/tags/OOM-Killer/index.html","modified":1451751380961,"shasum":"40ae3c680dc7f21decf8eb8132f721e0e1af11d5"},{"_id":"public/tags/Remote/index.html","modified":1451751380979,"shasum":"014cc5dd4176d5ca403d60a6ceb483a3e83d1d44"},{"_id":"public/tags/书摘/index.html","modified":1451751381004,"shasum":"9ac2bc1da2251c6d0c3fe1ae0b9c4a03b03fae4d"},{"_id":"public/tags/生活/index.html","modified":1451751381024,"shasum":"3628a2440a4a37ea8d9488bb682cd191d9e5adca"},{"_id":"public/tags/Mina/index.html","modified":1451751381046,"shasum":"80989b076e1a870cd02ea3f5ec3c506d8ad75950"},{"_id":"public/tags/Netty/index.html","modified":1451751381070,"shasum":"be4657f2f3aea2f8ee82c9279737c21fa4078230"},{"_id":"public/tags/线程模型/index.html","modified":1451751381088,"shasum":"c2a43f261f4958af63cfb29f913c8e6eb901a332"},{"_id":"public/tags/REWORK/index.html","modified":1451751381110,"shasum":"55ea6511339c56ca4615617aa9cc42382755a463"},{"_id":"public/tags/37signals/index.html","modified":1451751381130,"shasum":"eab29146149727f5f15eae3090da2fb28c5580e6"},{"_id":"public/tags/价值观/index.html","modified":1451751381149,"shasum":"857e60baed45ca48260adf0436917f41a5377a17"},{"_id":"public/tags/双11/index.html","modified":1451751381170,"shasum":"c457847bd63e871b3a76c6f28120b1c8062689c5"},{"_id":"public/tags/Solr/index.html","modified":1451751381195,"shasum":"ac011bda2989057ba8b7f1027de2d28a1572220e"},{"_id":"public/tags/数据结构/index.html","modified":1451751381220,"shasum":"3802025a0549c6ba43de65db047ded6bb7e33fa3"},{"_id":"public/tags/Maven/index.html","modified":1451751381238,"shasum":"3bcd0d823ca81d2613e1bead9f5e1126d9416448"},{"_id":"public/tags/Linux/index.html","modified":1451751381260,"shasum":"449b48bdf9f6799d24f01e0132bfca7d34fbaef6"},{"_id":"public/tags/Shell/index.html","modified":1451751381283,"shasum":"6cb075ad3ee29f5ca9c5bf6ca803479c3fa33ca8"},{"_id":"public/tags/ZooKeeper/index.html","modified":1451751381305,"shasum":"1698c8f4e11d4866c3fbe29f09515417e4f802e3"},{"_id":"public/tags/Hadoop/index.html","modified":1451751381338,"shasum":"3140e86b7f7996e5ff762758b994e4df7c7bbf8f"},{"_id":"public/tags/Hadoop/page/2/index.html","modified":1451751381366,"shasum":"3140e86b7f7996e5ff762758b994e4df7c7bbf8f"},{"_id":"public/tags/MapReduce/index.html","modified":1451751381398,"shasum":"5953e29e4f2c43cfb4653bdec402094cf6bd7d06"},{"_id":"public/tags/MapReduce/page/2/index.html","modified":1451751381421,"shasum":"5953e29e4f2c43cfb4653bdec402094cf6bd7d06"},{"_id":"public/tags/YARN/index.html","modified":1451751381443,"shasum":"f2079886741e396a860aecd3a197185e0217f9a4"},{"_id":"public/tags/Google/index.html","modified":1451751381463,"shasum":"e5f478aaf21d49d2bc7871346908e5dc93381199"},{"_id":"public/tags/Hadoop-Pipes/index.html","modified":1451751381483,"shasum":"8345c2a2e4c15e9dba371c263edfd913b460eb40"},{"_id":"public/tags/自然语言处理/index.html","modified":1451751381507,"shasum":"e7ec20fd928c0eb86cabcc58a4a6874fd03a4914"},{"_id":"public/tags/算法/index.html","modified":1451751381532,"shasum":"9f48ede73379b6dd2785a61e8c41d3315c3b8462"},{"_id":"public/tags/语义/index.html","modified":1451751381553,"shasum":"bb738998e445ead66a78eecee13a45f5b82b7b48"},{"_id":"public/tags/NoSQL/index.html","modified":1451751381570,"shasum":"49421a584460b74cbb8de36e1b5329b084f158a9"},{"_id":"public/tags/Redis/index.html","modified":1451751381588,"shasum":"90ab0455d1b61cd6ffa17a94afa1f62f76f76f27"},{"_id":"public/tags/Haloop/index.html","modified":1451751381609,"shasum":"61ac26179c97d98e1558951e90b791d6ef6b0de8"},{"_id":"public/tags/RPC/index.html","modified":1451751381628,"shasum":"ac82627bed3ab2784ca73d0ca396a0a99ac16997"},{"_id":"public/tags/Python/index.html","modified":1451751381646,"shasum":"af4bd0225a180983c712bd1605f61fd1bd1e81e4"},{"_id":"public/tags/分布式/index.html","modified":1451751381667,"shasum":"0be31c401a37aa2cf5b5bfad657c7479776d83d7"},{"_id":"public/tags/电影/index.html","modified":1451751381686,"shasum":"0789b74eba51cc39ed3f0dee55c57be293dc1b73"},{"_id":"public/tags/英语/index.html","modified":1451751381705,"shasum":"dc082220f20fbe5aeeea6ffb709ffe957ed2485d"}],"Category":[{"name":"自学资料","_id":"ciixba91600033x8fst7yyl58"},{"name":"最佳实践","_id":"ciixba91b000g3x8fk3t3bvpx"},{"name":"生活点滴","_id":"ciixba91o001h3x8fb87criik"},{"name":"技术分享","_id":"ciixba91q001q3x8fh9x4ielv"},{"name":"生活分享","_id":"ciixba92w004d3x8fky8i432j"}],"Data":[],"Page":[{"title":"Tags","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"title: Tags\ntype: \"tags\"\nlayout: tags\n---\n","date":"2015-12-30T16:38:50.000Z","updated":"2015-12-30T16:38:50.000Z","path":"tags/index.html","comments":1,"_id":"ciixba8z200003x8fpoobfr82"},{"_content":"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<!--NewPage-->\n<HTML>\n<HEAD>\n<!-- Generated by javadoc (build 1.6.0_21) on Wed May 04 07:58:32 PDT 2011 -->\n<TITLE>\nDistributedCache (Hadoop 0.20.203.0 API)\n</TITLE>\n\n<META NAME=\"date\" CONTENT=\"2011-05-04\">\n\n<LINK REL =\"stylesheet\" TYPE=\"text/css\" HREF=\"../../../../stylesheet.css\" TITLE=\"Style\">\n\n<SCRIPT type=\"text/javascript\">\nfunction windowTitle()\n{\n    if (location.href.indexOf('is-external=true') == -1) {\n        parent.document.title=\"DistributedCache (Hadoop 0.20.203.0 API)\";\n    }\n}\n</SCRIPT>\n<NOSCRIPT>\n</NOSCRIPT>\n\n</HEAD>\n\n<BODY BGCOLOR=\"white\" onload=\"windowTitle();\">\n<HR>\n\n\n<!-- ========= START OF TOP NAVBAR ======= -->\n<A NAME=\"navbar_top\"><!-- --></A>\n<A HREF=\"#skip-navbar_top\" title=\"Skip navigation links\"></A>\n<TABLE BORDER=\"0\" WIDTH=\"100%\" CELLPADDING=\"1\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR>\n<TD COLSPAN=2 BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">\n<A NAME=\"navbar_top_firstrow\"><!-- --></A>\n<TABLE BORDER=\"0\" CELLPADDING=\"0\" CELLSPACING=\"3\" SUMMARY=\"\">\n  <TR ALIGN=\"center\" VALIGN=\"top\">\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../overview-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Overview</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Package</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#FFFFFF\" CLASS=\"NavBarCell1Rev\"> &nbsp;<FONT CLASS=\"NavBarFont1Rev\"><B>Class</B></FONT>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"class-use/DistributedCache.html\"><FONT CLASS=\"NavBarFont1\"><B>Use</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-tree.html\"><FONT CLASS=\"NavBarFont1\"><B>Tree</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../deprecated-list.html\"><FONT CLASS=\"NavBarFont1\"><B>Deprecated</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../index-all.html\"><FONT CLASS=\"NavBarFont1\"><B>Index</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../help-doc.html\"><FONT CLASS=\"NavBarFont1\"><B>Help</B></FONT></A>&nbsp;</TD>\n  </TR>\n</TABLE>\n</TD>\n<TD ALIGN=\"right\" VALIGN=\"top\" ROWSPAN=3><EM>\n</EM>\n</TD>\n</TR>\n\n<TR>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n&nbsp;PREV CLASS&nbsp;\n&nbsp;<A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><B>NEXT CLASS</B></A></FONT></TD>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n  <A HREF=\"../../../../index.html?org/apache/hadoop/filecache/DistributedCache.html\" target=\"_top\"><B>FRAMES</B></A>  &nbsp;\n&nbsp;<A HREF=\"DistributedCache.html\" target=\"_top\"><B>NO FRAMES</B></A>  &nbsp;\n&nbsp;<SCRIPT type=\"text/javascript\">\n  <!--\n  if(window==top) {\n    document.writeln('<A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>');\n  }\n  //-->\n</SCRIPT>\n<NOSCRIPT>\n  <A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>\n</NOSCRIPT>\n\n\n</FONT></TD>\n</TR>\n<TR>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\n  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;<A HREF=\"#field_summary\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_summary\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_summary\">METHOD</A></FONT></TD>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\nDETAIL:&nbsp;<A HREF=\"#field_detail\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_detail\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_detail\">METHOD</A></FONT></TD>\n</TR>\n</TABLE>\n<A NAME=\"skip-navbar_top\"></A>\n<!-- ========= END OF TOP NAVBAR ========= -->\n\n<HR>\n<!-- ======== START OF CLASS DATA ======== -->\n<H2>\n<FONT SIZE=\"-1\">\norg.apache.hadoop.filecache</FONT>\n<BR>\nClass DistributedCache</H2>\n<PRE>\n<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true\" title=\"class or interface in java.lang\">java.lang.Object</A>\n  <IMG SRC=\"../../../../resources/inherit.gif\" ALT=\"extended by \"><B>org.apache.hadoop.filecache.DistributedCache</B>\n</PRE>\n<HR>\n<DL>\n<DT><PRE>public class <B>DistributedCache</B><DT>extends <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true\" title=\"class or interface in java.lang\">Object</A></DL>\n</PRE>\n\n<P>\nDistribute application-specific large, read-only files efficiently.\n \n <p><code>DistributedCache</code> is a facility provided by the Map-Reduce\n framework to cache files (text, archives, jars etc.) needed by applications.\n </p>\n \n <p>Applications specify the files, via urls (hdfs:// or http://) to be cached \n via the <A HREF=\"../../../../org/apache/hadoop/mapred/JobConf.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobConf</CODE></A>.\n The <code>DistributedCache</code> assumes that the\n files specified via hdfs:// urls are already present on the \n <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\"><CODE>FileSystem</CODE></A> at the path specified by the url.</p>\n \n <p>The framework will copy the necessary files on to the slave node before \n any tasks for the job are executed on that node. Its efficiency stems from \n the fact that the files are only copied once per job and the ability to \n cache archives which are un-archived on the slaves.</p> \n\n <p><code>DistributedCache</code> can be used to distribute simple, read-only\n data/text files and/or more complex types such as archives, jars etc. \n Archives (zip, tar and tgz/tar.gz files) are un-archived at the slave nodes. \n Jars may be optionally added to the classpath of the tasks, a rudimentary \n software distribution mechanism.  Files have execution permissions.\n Optionally users can also direct it to symlink the distributed cache file(s)\n into the working directory of the task.</p>\n \n <p><code>DistributedCache</code> tracks modification timestamps of the cache \n files. Clearly the cache files should not be modified by the application \n or externally while the job is executing.</p>\n \n <p>Here is an illustrative example on how to use the \n <code>DistributedCache</code>:</p>\n <p><blockquote><pre>\n     // Setting up the cache for the application\n     \n     1. Copy the requisite files to the <code>FileSystem</code>:\n     \n     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat  \n     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip  \n     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar\n     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar\n     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz\n     $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz\n     \n     2. Setup the application's <code>JobConf</code>:\n     \n     JobConf job = new JobConf();\n     DistributedCache.addCacheFile(new URI(\"/myapp/lookup.dat#lookup.dat\"), \n                                   job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/map.zip\", job);\n     DistributedCache.addFileToClassPath(new Path(\"/myapp/mylib.jar\"), job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/mytar.tar\", job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/mytgz.tgz\", job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/mytargz.tar.gz\", job);\n     \n     3. Use the cached files in the <A HREF=\"../../../../org/apache/hadoop/mapred/Mapper.html\" title=\"interface in org.apache.hadoop.mapred\"><CODE>Mapper</CODE></A>\n     or <A HREF=\"../../../../org/apache/hadoop/mapred/Reducer.html\" title=\"interface in org.apache.hadoop.mapred\"><CODE>Reducer</CODE></A>:\n     \n     public static class MapClass extends MapReduceBase  \n     implements Mapper&lt;K, V, K, V&gt; {\n     \n       private Path[] localArchives;\n       private Path[] localFiles;\n       \n       public void configure(JobConf job) {\n         // Get the cached archives/files\n         localArchives = DistributedCache.getLocalCacheArchives(job);\n         localFiles = DistributedCache.getLocalCacheFiles(job);\n       }\n       \n       public void map(K key, V value, \n                       OutputCollector&lt;K, V&gt; output, Reporter reporter) \n       throws IOException {\n         // Use data from the cached archives/files here\n         // ...\n         // ...\n         output.collect(k, v);\n       }\n     }\n     \n </pre></blockquote></p>\n It is also very common to use the DistributedCache by using\n <A HREF=\"../../../../org/apache/hadoop/util/GenericOptionsParser.html\" title=\"class in org.apache.hadoop.util\"><CODE>GenericOptionsParser</CODE></A>.\n\n This class includes methods that should be used by users\n (specifically those mentioned in the example above, as well\n as <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\"><CODE>addArchiveToClassPath(Path, Configuration)</CODE></A>),\n as well as methods intended for use by the MapReduce framework\n (e.g., <A HREF=\"../../../../org/apache/hadoop/mapred/JobClient.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobClient</CODE></A>).  For implementation\n details, see <A HREF=\"../../../../org/apache/hadoop/filecache/TrackerDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TrackerDistributedCacheManager</CODE></A> and\n <A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TaskDistributedCacheManager</CODE></A>.\n<P>\n\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../org/apache/hadoop/filecache/TrackerDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TrackerDistributedCacheManager</CODE></A>, \n<A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TaskDistributedCacheManager</CODE></A>, \n<A HREF=\"../../../../org/apache/hadoop/mapred/JobConf.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobConf</CODE></A>, \n<A HREF=\"../../../../org/apache/hadoop/mapred/JobClient.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobClient</CODE></A></DL>\n<HR>\n\n<P>\n<!-- =========== FIELD SUMMARY =========== -->\n\n<A NAME=\"field_summary\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"2\"><FONT SIZE=\"+2\">\n<B>Field Summary</B></FONT></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES\">CACHE_ARCHIVES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES\"><CODE>CACHE_ARCHIVES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_SIZES\">CACHE_ARCHIVES_SIZES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_SIZES\"><CODE>CACHE_ARCHIVES_SIZES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_TIMESTAMPS\">CACHE_ARCHIVES_TIMESTAMPS</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_TIMESTAMPS\"><CODE>CACHE_ARCHIVES_TIMESTAMPS</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES\">CACHE_FILES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES\"><CODE>CACHE_FILES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_SIZES\">CACHE_FILES_SIZES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_SIZES\"><CODE>CACHE_FILES_SIZES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_TIMESTAMPS\">CACHE_FILES_TIMESTAMPS</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_TIMESTAMPS\"><CODE>CACHE_FILES_TIMESTAMPS</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALARCHIVES\">CACHE_LOCALARCHIVES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALARCHIVES\"><CODE>CACHE_LOCALARCHIVES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALFILES\">CACHE_LOCALFILES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALFILES\"><CODE>CACHE_LOCALFILES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_SYMLINK\">CACHE_SYMLINK</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_SYMLINK\"><CODE>CACHE_SYMLINK</CODE></A> is not a *public* constant.</TD>\n</TR>\n</TABLE>\n&nbsp;\n<!-- ======== CONSTRUCTOR SUMMARY ======== -->\n\n<A NAME=\"constructor_summary\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"2\"><FONT SIZE=\"+2\">\n<B>Constructor Summary</B></FONT></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#DistributedCache()\">DistributedCache</A></B>()</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>\n</TR>\n</TABLE>\n&nbsp;\n<!-- ========== METHOD SUMMARY =========== -->\n\n<A NAME=\"method_summary\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"2\"><FONT SIZE=\"+2\">\n<B>Method Summary</B></FONT></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\">addArchiveToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                      <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addArchiveToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I></TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\">addArchiveToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                      <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                      <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add an archive path to the current set of classpath entries.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addCacheArchive(java.net.URI, org.apache.hadoop.conf.Configuration)\">addCacheArchive</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n                <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a archives to be localized to the conf.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addCacheFile(java.net.URI, org.apache.hadoop.conf.Configuration)\">addCacheFile</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n             <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a file to be localized to the conf.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\">addFileToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                   <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addFileToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I></TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\">addFileToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                   <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                   <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a file path to the current set of classpath entries.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\">addLocalArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a archive that has been localized to the conf.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\">addLocalFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n              <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a file that has been localized to the conf..</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;boolean</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#checkURIs(java.net.URI[], java.net.URI[])\">checkURIs</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriFiles,\n          <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriArchives)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method checks if there is a conflict in the fragment names \n of the uris.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#createAllSymlink(org.apache.hadoop.conf.Configuration, java.io.File, java.io.File)\">createAllSymlink</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;jobCacheDir,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;workDir)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>Internal to MapReduce framework.  Use DistributedCacheManager\n instead.</I></TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#createSymlink(org.apache.hadoop.conf.Configuration)\">createSymlink</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method allows you to create symlinks in the current working directory\n of the task to all the cache files/archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getArchiveClassPaths(org.apache.hadoop.conf.Configuration)\">getArchiveClassPaths</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the archive entries in classpath as an array of Path.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;long[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getArchiveTimestamps(org.apache.hadoop.conf.Configuration)\">getArchiveTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the timestamps of the archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getCacheArchives(org.apache.hadoop.conf.Configuration)\">getCacheArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get cache archives set in the Configuration.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getCacheFiles(org.apache.hadoop.conf.Configuration)\">getCacheFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get cache files set in the Configuration.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getFileClassPaths(org.apache.hadoop.conf.Configuration)\">getFileClassPaths</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the file entries in classpath as an array of Path.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\">FileStatus</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getFileStatus(org.apache.hadoop.conf.Configuration, java.net.URI)\">getFileStatus</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n              <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns <A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\"><CODE>FileStatus</CODE></A> of a given cache file on hdfs.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;long[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getFileTimestamps(org.apache.hadoop.conf.Configuration)\">getFileTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the timestamps of the files.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getLocalCacheArchives(org.apache.hadoop.conf.Configuration)\">getLocalCacheArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the path array of the localized caches.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getLocalCacheFiles(org.apache.hadoop.conf.Configuration)\">getLocalCacheFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the path array of the localized files.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;boolean</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getSymlink(org.apache.hadoop.conf.Configuration)\">getSymlink</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method checks to see if symlinks are to be create for the \n localized cache files in the current working directory \n Used by internal DistributedCache code.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;long</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getTimestamp(org.apache.hadoop.conf.Configuration, java.net.URI)\">getTimestamp</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n             <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns mtime of a given cache file on hdfs.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setArchiveTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\">setArchiveTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                     <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is to check the timestamp of the archives to be localized.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setCacheArchives(java.net.URI[], org.apache.hadoop.conf.Configuration)\">setCacheArchives</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;archives,\n                 <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the configuration with the given set of archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setCacheFiles(java.net.URI[], org.apache.hadoop.conf.Configuration)\">setCacheFiles</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;files,\n              <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the configuration with the given set of files.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setFileTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\">setFileTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                  <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is to check the timestamp of the files to be localized.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\">setLocalArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the conf to contain the location for localized archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\">setLocalFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n              <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the conf to contain the location for localized files.</TD>\n</TR>\n</TABLE>\n&nbsp;<A NAME=\"methods_inherited_from_class_java.lang.Object\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#EEEEFF\" CLASS=\"TableSubHeadingColor\">\n<TH ALIGN=\"left\"><B>Methods inherited from class java.lang.<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true\" title=\"class or interface in java.lang\">Object</A></B></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#clone()\" title=\"class or interface in java.lang\">clone</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#equals(java.lang.Object)\" title=\"class or interface in java.lang\">equals</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#finalize()\" title=\"class or interface in java.lang\">finalize</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#getClass()\" title=\"class or interface in java.lang\">getClass</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#hashCode()\" title=\"class or interface in java.lang\">hashCode</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#notify()\" title=\"class or interface in java.lang\">notify</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#notifyAll()\" title=\"class or interface in java.lang\">notifyAll</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#toString()\" title=\"class or interface in java.lang\">toString</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#wait()\" title=\"class or interface in java.lang\">wait</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#wait(long)\" title=\"class or interface in java.lang\">wait</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#wait(long, int)\" title=\"class or interface in java.lang\">wait</A></CODE></TD>\n</TR>\n</TABLE>\n&nbsp;\n<P>\n\n<!-- ============ FIELD DETAIL =========== -->\n\n<A NAME=\"field_detail\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"1\"><FONT SIZE=\"+2\">\n<B>Field Detail</B></FONT></TH>\n</TR>\n</TABLE>\n\n<A NAME=\"CACHE_FILES_SIZES\"><!-- --></A><H3>\nCACHE_FILES_SIZES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_FILES_SIZES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_SIZES\"><CODE>CACHE_FILES_SIZES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_FILES_SIZES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_ARCHIVES_SIZES\"><!-- --></A><H3>\nCACHE_ARCHIVES_SIZES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_ARCHIVES_SIZES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_SIZES\"><CODE>CACHE_ARCHIVES_SIZES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_ARCHIVES_SIZES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_ARCHIVES_TIMESTAMPS\"><!-- --></A><H3>\nCACHE_ARCHIVES_TIMESTAMPS</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_ARCHIVES_TIMESTAMPS</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_TIMESTAMPS\"><CODE>CACHE_ARCHIVES_TIMESTAMPS</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_ARCHIVES_TIMESTAMPS\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_FILES_TIMESTAMPS\"><!-- --></A><H3>\nCACHE_FILES_TIMESTAMPS</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_FILES_TIMESTAMPS</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_TIMESTAMPS\"><CODE>CACHE_FILES_TIMESTAMPS</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_FILES_TIMESTAMPS\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_ARCHIVES\"><!-- --></A><H3>\nCACHE_ARCHIVES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_ARCHIVES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES\"><CODE>CACHE_ARCHIVES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_ARCHIVES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_FILES\"><!-- --></A><H3>\nCACHE_FILES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_FILES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES\"><CODE>CACHE_FILES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_FILES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_LOCALARCHIVES\"><!-- --></A><H3>\nCACHE_LOCALARCHIVES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_LOCALARCHIVES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALARCHIVES\"><CODE>CACHE_LOCALARCHIVES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_LOCALARCHIVES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_LOCALFILES\"><!-- --></A><H3>\nCACHE_LOCALFILES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_LOCALFILES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALFILES\"><CODE>CACHE_LOCALFILES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_LOCALFILES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_SYMLINK\"><!-- --></A><H3>\nCACHE_SYMLINK</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_SYMLINK</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_SYMLINK\"><CODE>CACHE_SYMLINK</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_SYMLINK\">Constant Field Values</A></DL>\n</DL>\n\n<!-- ========= CONSTRUCTOR DETAIL ======== -->\n\n<A NAME=\"constructor_detail\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"1\"><FONT SIZE=\"+2\">\n<B>Constructor Detail</B></FONT></TH>\n</TR>\n</TABLE>\n\n<A NAME=\"DistributedCache()\"><!-- --></A><H3>\nDistributedCache</H3>\n<PRE>\npublic <B>DistributedCache</B>()</PRE>\n<DL>\n</DL>\n\n<!-- ============ METHOD DETAIL ========== -->\n\n<A NAME=\"method_detail\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"1\"><FONT SIZE=\"+2\">\n<B>Method Detail</B></FONT></TH>\n</TR>\n</TABLE>\n\n<A NAME=\"getFileStatus(org.apache.hadoop.conf.Configuration, java.net.URI)\"><!-- --></A><H3>\ngetFileStatus</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\">FileStatus</A> <B>getFileStatus</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                       <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)\n                                throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Returns <A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\"><CODE>FileStatus</CODE></A> of a given cache file on hdfs. Internal to \n MapReduce.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - configuration<DD><CODE>cache</CODE> - cache file\n<DT><B>Returns:</B><DD><code>FileStatus</code> of a given cache file on hdfs\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getTimestamp(org.apache.hadoop.conf.Configuration, java.net.URI)\"><!-- --></A><H3>\ngetTimestamp</H3>\n<PRE>\npublic static long <B>getTimestamp</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)\n                         throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Returns mtime of a given cache file on hdfs. Internal to MapReduce.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - configuration<DD><CODE>cache</CODE> - cache file\n<DT><B>Returns:</B><DD>mtime of a given cache file on hdfs\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"createAllSymlink(org.apache.hadoop.conf.Configuration, java.io.File, java.io.File)\"><!-- --></A><H3>\ncreateAllSymlink</H3>\n<PRE>\npublic static void <B>createAllSymlink</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;jobCacheDir,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;workDir)\n                             throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD><B>Deprecated.</B>&nbsp;<I>Internal to MapReduce framework.  Use DistributedCacheManager\n instead.</I>\n<P>\n<DD>This method create symlinks for all files in a given dir in another directory\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - the configuration<DD><CODE>jobCacheDir</CODE> - the target directory for creating symlinks<DD><CODE>workDir</CODE> - the directory in which the symlinks are created\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setCacheArchives(java.net.URI[], org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\nsetCacheArchives</H3>\n<PRE>\npublic static void <B>setCacheArchives</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;archives,\n                                    <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Set the configuration with the given set of archives. Intended\n to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>archives</CODE> - The list of archives that need to be localized<DD><CODE>conf</CODE> - Configuration which will be changed</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setCacheFiles(java.net.URI[], org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\nsetCacheFiles</H3>\n<PRE>\npublic static void <B>setCacheFiles</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;files,\n                                 <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Set the configuration with the given set of files.  Intended to be\n used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>files</CODE> - The list of files that need to be localized<DD><CODE>conf</CODE> - Configuration which will be changed</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getCacheArchives(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetCacheArchives</H3>\n<PRE>\npublic static <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[] <B>getCacheArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                              throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Get cache archives set in the Configuration.  Used by\n internal DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which contains the archives\n<DT><B>Returns:</B><DD>An array of the caches set in the Configuration\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getCacheFiles(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetCacheFiles</H3>\n<PRE>\npublic static <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[] <B>getCacheFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                           throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Get cache files set in the Configuration.  Used by internal\n DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which contains the files\n<DT><B>Returns:</B><DD>Am array of the files set in the Configuration\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getLocalCacheArchives(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetLocalCacheArchives</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getLocalCacheArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                                    throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Return the path array of the localized caches.  Intended to be used\n by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the localized archives\n<DT><B>Returns:</B><DD>A path array of localized caches\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getLocalCacheFiles(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetLocalCacheFiles</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getLocalCacheFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                                 throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Return the path array of the localized files.  Intended to be used\n by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the localized files\n<DT><B>Returns:</B><DD>A path array of localized files\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getArchiveTimestamps(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetArchiveTimestamps</H3>\n<PRE>\npublic static long[] <B>getArchiveTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the timestamps of the archives.  Used by internal\n DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which stored the timestamps\n<DT><B>Returns:</B><DD>a long array of timestamps\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getFileTimestamps(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetFileTimestamps</H3>\n<PRE>\npublic static long[] <B>getFileTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the timestamps of the files.  Used by internal\n DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which stored the timestamps\n<DT><B>Returns:</B><DD>a long array of timestamps\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setArchiveTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetArchiveTimestamps</H3>\n<PRE>\npublic static void <B>setArchiveTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                        <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</PRE>\n<DL>\n<DD>This is to check the timestamp of the archives to be localized.\n Used by internal MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration which stores the timestamp's<DD><CODE>timestamps</CODE> - comma separated list of timestamps of archives.\n The order should be the same as the order in which the archives are added.</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setFileTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetFileTimestamps</H3>\n<PRE>\npublic static void <B>setFileTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                     <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</PRE>\n<DL>\n<DD>This is to check the timestamp of the files to be localized.\n Used by internal MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration which stores the timestamp's<DD><CODE>timestamps</CODE> - comma separated list of timestamps of files.\n The order should be the same as the order in which the files are added.</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetLocalArchives</H3>\n<PRE>\npublic static void <B>setLocalArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Set the conf to contain the location for localized archives.  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local archives</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetLocalFiles</H3>\n<PRE>\npublic static void <B>setLocalFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Set the conf to contain the location for localized files.  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local files</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\naddLocalArchives</H3>\n<PRE>\npublic static void <B>addLocalArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Add a archive that has been localized to the conf.  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local archives</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\naddLocalFiles</H3>\n<PRE>\npublic static void <B>addLocalFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Add a file that has been localized to the conf..  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local files</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addCacheArchive(java.net.URI, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddCacheArchive</H3>\n<PRE>\npublic static void <B>addCacheArchive</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n                                   <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Add a archives to be localized to the conf.  Intended to\n be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>uri</CODE> - The uri of the cache to be localized<DD><CODE>conf</CODE> - Configuration to add the cache to</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addCacheFile(java.net.URI, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddCacheFile</H3>\n<PRE>\npublic static void <B>addCacheFile</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n                                <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Add a file to be localized to the conf.  Intended\n to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>uri</CODE> - The uri of the cache to be localized<DD><CODE>conf</CODE> - Configuration to add the cache to</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddFileToClassPath</H3>\n<PRE>\n<FONT SIZE=\"-1\"><A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Deprecated.html?is-external=true\" title=\"class or interface in java.lang\">@Deprecated</A>\n</FONT>public static void <B>addFileToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                                                 <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                               throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD><B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addFileToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I>\n<P>\n<DD>Add a file path to the current set of classpath entries. It adds the file\n to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>file</CODE> - Path of the file to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><!-- --></A><H3>\naddFileToClassPath</H3>\n<PRE>\npublic static void <B>addFileToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                                      <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                      <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)\n                               throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Add a file path to the current set of classpath entries. It adds the file\n to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>file</CODE> - Path of the file to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting<DD><CODE>fs</CODE> - FileSystem with respect to which <code>archivefile</code> should\n              be interpreted.\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getFileClassPaths(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetFileClassPaths</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getFileClassPaths</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the file entries in classpath as an array of Path.\n Used by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the classpath setting</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddArchiveToClassPath</H3>\n<PRE>\n<FONT SIZE=\"-1\"><A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Deprecated.html?is-external=true\" title=\"class or interface in java.lang\">@Deprecated</A>\n</FONT>public static void <B>addArchiveToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                                                    <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                                  throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD><B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addArchiveToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I>\n<P>\n<DD>Add an archive path to the current set of classpath entries. It adds the\n archive to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>archive</CODE> - Path of the archive to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><!-- --></A><H3>\naddArchiveToClassPath</H3>\n<PRE>\npublic static void <B>addArchiveToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                                         <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                         <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)\n                                  throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Add an archive path to the current set of classpath entries. It adds the\n archive to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>archive</CODE> - Path of the archive to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting<DD><CODE>fs</CODE> - FileSystem with respect to which <code>archive</code> should be interpreted.\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getArchiveClassPaths(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetArchiveClassPaths</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getArchiveClassPaths</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the archive entries in classpath as an array of Path.\n Used by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the classpath setting</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"createSymlink(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ncreateSymlink</H3>\n<PRE>\npublic static void <B>createSymlink</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>This method allows you to create symlinks in the current working directory\n of the task to all the cache files/archives.\n Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - the jobconf</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getSymlink(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetSymlink</H3>\n<PRE>\npublic static boolean <B>getSymlink</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>This method checks to see if symlinks are to be create for the \n localized cache files in the current working directory \n Used by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - the jobconf\n<DT><B>Returns:</B><DD>true if symlinks are to be created- else return false</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"checkURIs(java.net.URI[], java.net.URI[])\"><!-- --></A><H3>\ncheckURIs</H3>\n<PRE>\npublic static boolean <B>checkURIs</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriFiles,\n                                <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriArchives)</PRE>\n<DL>\n<DD>This method checks if there is a conflict in the fragment names \n of the uris. Also makes sure that each uri has a fragment. It \n is only to be called if you want to create symlinks for \n the various archives and files.  May be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>uriFiles</CODE> - The uri array of urifiles<DD><CODE>uriArchives</CODE> - the uri array of uri archives</DL>\n</DD>\n</DL>\n<!-- ========= END OF CLASS DATA ========= -->\n<HR>\n\n\n<!-- ======= START OF BOTTOM NAVBAR ====== -->\n<A NAME=\"navbar_bottom\"><!-- --></A>\n<A HREF=\"#skip-navbar_bottom\" title=\"Skip navigation links\"></A>\n<TABLE BORDER=\"0\" WIDTH=\"100%\" CELLPADDING=\"1\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR>\n<TD COLSPAN=2 BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">\n<A NAME=\"navbar_bottom_firstrow\"><!-- --></A>\n<TABLE BORDER=\"0\" CELLPADDING=\"0\" CELLSPACING=\"3\" SUMMARY=\"\">\n  <TR ALIGN=\"center\" VALIGN=\"top\">\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../overview-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Overview</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Package</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#FFFFFF\" CLASS=\"NavBarCell1Rev\"> &nbsp;<FONT CLASS=\"NavBarFont1Rev\"><B>Class</B></FONT>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"class-use/DistributedCache.html\"><FONT CLASS=\"NavBarFont1\"><B>Use</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-tree.html\"><FONT CLASS=\"NavBarFont1\"><B>Tree</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../deprecated-list.html\"><FONT CLASS=\"NavBarFont1\"><B>Deprecated</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../index-all.html\"><FONT CLASS=\"NavBarFont1\"><B>Index</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../help-doc.html\"><FONT CLASS=\"NavBarFont1\"><B>Help</B></FONT></A>&nbsp;</TD>\n  </TR>\n</TABLE>\n</TD>\n<TD ALIGN=\"right\" VALIGN=\"top\" ROWSPAN=3><EM>\n</EM>\n</TD>\n</TR>\n\n<TR>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n&nbsp;PREV CLASS&nbsp;\n&nbsp;<A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><B>NEXT CLASS</B></A></FONT></TD>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n  <A HREF=\"../../../../index.html?org/apache/hadoop/filecache/DistributedCache.html\" target=\"_top\"><B>FRAMES</B></A>  &nbsp;\n&nbsp;<A HREF=\"DistributedCache.html\" target=\"_top\"><B>NO FRAMES</B></A>  &nbsp;\n&nbsp;<SCRIPT type=\"text/javascript\">\n  <!--\n  if(window==top) {\n    document.writeln('<A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>');\n  }\n  //-->\n</SCRIPT>\n<NOSCRIPT>\n  <A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>\n</NOSCRIPT>\n\n\n</FONT></TD>\n</TR>\n<TR>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\n  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;<A HREF=\"#field_summary\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_summary\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_summary\">METHOD</A></FONT></TD>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\nDETAIL:&nbsp;<A HREF=\"#field_detail\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_detail\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_detail\">METHOD</A></FONT></TD>\n</TR>\n</TABLE>\n<A NAME=\"skip-navbar_bottom\"></A>\n<!-- ======== END OF BOTTOM NAVBAR ======= -->\n\n<HR>\nCopyright &copy; 2009 The Apache Software Foundation\n</BODY>\n</HTML>\n","source":"images/2012/02/DistributedCache.html","raw":"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<!--NewPage-->\n<HTML>\n<HEAD>\n<!-- Generated by javadoc (build 1.6.0_21) on Wed May 04 07:58:32 PDT 2011 -->\n<TITLE>\nDistributedCache (Hadoop 0.20.203.0 API)\n</TITLE>\n\n<META NAME=\"date\" CONTENT=\"2011-05-04\">\n\n<LINK REL =\"stylesheet\" TYPE=\"text/css\" HREF=\"../../../../stylesheet.css\" TITLE=\"Style\">\n\n<SCRIPT type=\"text/javascript\">\nfunction windowTitle()\n{\n    if (location.href.indexOf('is-external=true') == -1) {\n        parent.document.title=\"DistributedCache (Hadoop 0.20.203.0 API)\";\n    }\n}\n</SCRIPT>\n<NOSCRIPT>\n</NOSCRIPT>\n\n</HEAD>\n\n<BODY BGCOLOR=\"white\" onload=\"windowTitle();\">\n<HR>\n\n\n<!-- ========= START OF TOP NAVBAR ======= -->\n<A NAME=\"navbar_top\"><!-- --></A>\n<A HREF=\"#skip-navbar_top\" title=\"Skip navigation links\"></A>\n<TABLE BORDER=\"0\" WIDTH=\"100%\" CELLPADDING=\"1\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR>\n<TD COLSPAN=2 BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">\n<A NAME=\"navbar_top_firstrow\"><!-- --></A>\n<TABLE BORDER=\"0\" CELLPADDING=\"0\" CELLSPACING=\"3\" SUMMARY=\"\">\n  <TR ALIGN=\"center\" VALIGN=\"top\">\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../overview-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Overview</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Package</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#FFFFFF\" CLASS=\"NavBarCell1Rev\"> &nbsp;<FONT CLASS=\"NavBarFont1Rev\"><B>Class</B></FONT>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"class-use/DistributedCache.html\"><FONT CLASS=\"NavBarFont1\"><B>Use</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-tree.html\"><FONT CLASS=\"NavBarFont1\"><B>Tree</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../deprecated-list.html\"><FONT CLASS=\"NavBarFont1\"><B>Deprecated</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../index-all.html\"><FONT CLASS=\"NavBarFont1\"><B>Index</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../help-doc.html\"><FONT CLASS=\"NavBarFont1\"><B>Help</B></FONT></A>&nbsp;</TD>\n  </TR>\n</TABLE>\n</TD>\n<TD ALIGN=\"right\" VALIGN=\"top\" ROWSPAN=3><EM>\n</EM>\n</TD>\n</TR>\n\n<TR>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n&nbsp;PREV CLASS&nbsp;\n&nbsp;<A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><B>NEXT CLASS</B></A></FONT></TD>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n  <A HREF=\"../../../../index.html?org/apache/hadoop/filecache/DistributedCache.html\" target=\"_top\"><B>FRAMES</B></A>  &nbsp;\n&nbsp;<A HREF=\"DistributedCache.html\" target=\"_top\"><B>NO FRAMES</B></A>  &nbsp;\n&nbsp;<SCRIPT type=\"text/javascript\">\n  <!--\n  if(window==top) {\n    document.writeln('<A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>');\n  }\n  //-->\n</SCRIPT>\n<NOSCRIPT>\n  <A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>\n</NOSCRIPT>\n\n\n</FONT></TD>\n</TR>\n<TR>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\n  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;<A HREF=\"#field_summary\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_summary\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_summary\">METHOD</A></FONT></TD>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\nDETAIL:&nbsp;<A HREF=\"#field_detail\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_detail\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_detail\">METHOD</A></FONT></TD>\n</TR>\n</TABLE>\n<A NAME=\"skip-navbar_top\"></A>\n<!-- ========= END OF TOP NAVBAR ========= -->\n\n<HR>\n<!-- ======== START OF CLASS DATA ======== -->\n<H2>\n<FONT SIZE=\"-1\">\norg.apache.hadoop.filecache</FONT>\n<BR>\nClass DistributedCache</H2>\n<PRE>\n<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true\" title=\"class or interface in java.lang\">java.lang.Object</A>\n  <IMG SRC=\"../../../../resources/inherit.gif\" ALT=\"extended by \"><B>org.apache.hadoop.filecache.DistributedCache</B>\n</PRE>\n<HR>\n<DL>\n<DT><PRE>public class <B>DistributedCache</B><DT>extends <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true\" title=\"class or interface in java.lang\">Object</A></DL>\n</PRE>\n\n<P>\nDistribute application-specific large, read-only files efficiently.\n \n <p><code>DistributedCache</code> is a facility provided by the Map-Reduce\n framework to cache files (text, archives, jars etc.) needed by applications.\n </p>\n \n <p>Applications specify the files, via urls (hdfs:// or http://) to be cached \n via the <A HREF=\"../../../../org/apache/hadoop/mapred/JobConf.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobConf</CODE></A>.\n The <code>DistributedCache</code> assumes that the\n files specified via hdfs:// urls are already present on the \n <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\"><CODE>FileSystem</CODE></A> at the path specified by the url.</p>\n \n <p>The framework will copy the necessary files on to the slave node before \n any tasks for the job are executed on that node. Its efficiency stems from \n the fact that the files are only copied once per job and the ability to \n cache archives which are un-archived on the slaves.</p> \n\n <p><code>DistributedCache</code> can be used to distribute simple, read-only\n data/text files and/or more complex types such as archives, jars etc. \n Archives (zip, tar and tgz/tar.gz files) are un-archived at the slave nodes. \n Jars may be optionally added to the classpath of the tasks, a rudimentary \n software distribution mechanism.  Files have execution permissions.\n Optionally users can also direct it to symlink the distributed cache file(s)\n into the working directory of the task.</p>\n \n <p><code>DistributedCache</code> tracks modification timestamps of the cache \n files. Clearly the cache files should not be modified by the application \n or externally while the job is executing.</p>\n \n <p>Here is an illustrative example on how to use the \n <code>DistributedCache</code>:</p>\n <p><blockquote><pre>\n     // Setting up the cache for the application\n     \n     1. Copy the requisite files to the <code>FileSystem</code>:\n     \n     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat  \n     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip  \n     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar\n     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar\n     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz\n     $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz\n     \n     2. Setup the application's <code>JobConf</code>:\n     \n     JobConf job = new JobConf();\n     DistributedCache.addCacheFile(new URI(\"/myapp/lookup.dat#lookup.dat\"), \n                                   job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/map.zip\", job);\n     DistributedCache.addFileToClassPath(new Path(\"/myapp/mylib.jar\"), job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/mytar.tar\", job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/mytgz.tgz\", job);\n     DistributedCache.addCacheArchive(new URI(\"/myapp/mytargz.tar.gz\", job);\n     \n     3. Use the cached files in the <A HREF=\"../../../../org/apache/hadoop/mapred/Mapper.html\" title=\"interface in org.apache.hadoop.mapred\"><CODE>Mapper</CODE></A>\n     or <A HREF=\"../../../../org/apache/hadoop/mapred/Reducer.html\" title=\"interface in org.apache.hadoop.mapred\"><CODE>Reducer</CODE></A>:\n     \n     public static class MapClass extends MapReduceBase  \n     implements Mapper&lt;K, V, K, V&gt; {\n     \n       private Path[] localArchives;\n       private Path[] localFiles;\n       \n       public void configure(JobConf job) {\n         // Get the cached archives/files\n         localArchives = DistributedCache.getLocalCacheArchives(job);\n         localFiles = DistributedCache.getLocalCacheFiles(job);\n       }\n       \n       public void map(K key, V value, \n                       OutputCollector&lt;K, V&gt; output, Reporter reporter) \n       throws IOException {\n         // Use data from the cached archives/files here\n         // ...\n         // ...\n         output.collect(k, v);\n       }\n     }\n     \n </pre></blockquote></p>\n It is also very common to use the DistributedCache by using\n <A HREF=\"../../../../org/apache/hadoop/util/GenericOptionsParser.html\" title=\"class in org.apache.hadoop.util\"><CODE>GenericOptionsParser</CODE></A>.\n\n This class includes methods that should be used by users\n (specifically those mentioned in the example above, as well\n as <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\"><CODE>addArchiveToClassPath(Path, Configuration)</CODE></A>),\n as well as methods intended for use by the MapReduce framework\n (e.g., <A HREF=\"../../../../org/apache/hadoop/mapred/JobClient.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobClient</CODE></A>).  For implementation\n details, see <A HREF=\"../../../../org/apache/hadoop/filecache/TrackerDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TrackerDistributedCacheManager</CODE></A> and\n <A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TaskDistributedCacheManager</CODE></A>.\n<P>\n\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../org/apache/hadoop/filecache/TrackerDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TrackerDistributedCacheManager</CODE></A>, \n<A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><CODE>TaskDistributedCacheManager</CODE></A>, \n<A HREF=\"../../../../org/apache/hadoop/mapred/JobConf.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobConf</CODE></A>, \n<A HREF=\"../../../../org/apache/hadoop/mapred/JobClient.html\" title=\"class in org.apache.hadoop.mapred\"><CODE>JobClient</CODE></A></DL>\n<HR>\n\n<P>\n<!-- =========== FIELD SUMMARY =========== -->\n\n<A NAME=\"field_summary\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"2\"><FONT SIZE=\"+2\">\n<B>Field Summary</B></FONT></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES\">CACHE_ARCHIVES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES\"><CODE>CACHE_ARCHIVES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_SIZES\">CACHE_ARCHIVES_SIZES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_SIZES\"><CODE>CACHE_ARCHIVES_SIZES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_TIMESTAMPS\">CACHE_ARCHIVES_TIMESTAMPS</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_TIMESTAMPS\"><CODE>CACHE_ARCHIVES_TIMESTAMPS</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES\">CACHE_FILES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES\"><CODE>CACHE_FILES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_SIZES\">CACHE_FILES_SIZES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_SIZES\"><CODE>CACHE_FILES_SIZES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_TIMESTAMPS\">CACHE_FILES_TIMESTAMPS</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_TIMESTAMPS\"><CODE>CACHE_FILES_TIMESTAMPS</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALARCHIVES\">CACHE_LOCALARCHIVES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALARCHIVES\"><CODE>CACHE_LOCALARCHIVES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALFILES\">CACHE_LOCALFILES</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALFILES\"><CODE>CACHE_LOCALFILES</CODE></A> is not a *public* constant.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_SYMLINK\">CACHE_SYMLINK</A></B></CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_SYMLINK\"><CODE>CACHE_SYMLINK</CODE></A> is not a *public* constant.</TD>\n</TR>\n</TABLE>\n&nbsp;\n<!-- ======== CONSTRUCTOR SUMMARY ======== -->\n\n<A NAME=\"constructor_summary\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"2\"><FONT SIZE=\"+2\">\n<B>Constructor Summary</B></FONT></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#DistributedCache()\">DistributedCache</A></B>()</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>\n</TR>\n</TABLE>\n&nbsp;\n<!-- ========== METHOD SUMMARY =========== -->\n\n<A NAME=\"method_summary\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"2\"><FONT SIZE=\"+2\">\n<B>Method Summary</B></FONT></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\">addArchiveToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                      <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addArchiveToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I></TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\">addArchiveToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                      <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                      <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add an archive path to the current set of classpath entries.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addCacheArchive(java.net.URI, org.apache.hadoop.conf.Configuration)\">addCacheArchive</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n                <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a archives to be localized to the conf.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addCacheFile(java.net.URI, org.apache.hadoop.conf.Configuration)\">addCacheFile</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n             <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a file to be localized to the conf.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\">addFileToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                   <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addFileToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I></TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\">addFileToClassPath</A></B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                   <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                   <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a file path to the current set of classpath entries.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\">addLocalArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a archive that has been localized to the conf.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\">addLocalFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n              <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a file that has been localized to the conf..</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;boolean</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#checkURIs(java.net.URI[], java.net.URI[])\">checkURIs</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriFiles,\n          <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriArchives)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method checks if there is a conflict in the fragment names \n of the uris.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#createAllSymlink(org.apache.hadoop.conf.Configuration, java.io.File, java.io.File)\">createAllSymlink</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;jobCacheDir,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;workDir)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>Internal to MapReduce framework.  Use DistributedCacheManager\n instead.</I></TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#createSymlink(org.apache.hadoop.conf.Configuration)\">createSymlink</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method allows you to create symlinks in the current working directory\n of the task to all the cache files/archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getArchiveClassPaths(org.apache.hadoop.conf.Configuration)\">getArchiveClassPaths</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the archive entries in classpath as an array of Path.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;long[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getArchiveTimestamps(org.apache.hadoop.conf.Configuration)\">getArchiveTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the timestamps of the archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getCacheArchives(org.apache.hadoop.conf.Configuration)\">getCacheArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get cache archives set in the Configuration.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getCacheFiles(org.apache.hadoop.conf.Configuration)\">getCacheFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get cache files set in the Configuration.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getFileClassPaths(org.apache.hadoop.conf.Configuration)\">getFileClassPaths</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the file entries in classpath as an array of Path.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\">FileStatus</A></CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getFileStatus(org.apache.hadoop.conf.Configuration, java.net.URI)\">getFileStatus</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n              <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns <A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\"><CODE>FileStatus</CODE></A> of a given cache file on hdfs.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;long[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getFileTimestamps(org.apache.hadoop.conf.Configuration)\">getFileTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the timestamps of the files.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getLocalCacheArchives(org.apache.hadoop.conf.Configuration)\">getLocalCacheArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the path array of the localized caches.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[]</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getLocalCacheFiles(org.apache.hadoop.conf.Configuration)\">getLocalCacheFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the path array of the localized files.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;boolean</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getSymlink(org.apache.hadoop.conf.Configuration)\">getSymlink</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method checks to see if symlinks are to be create for the \n localized cache files in the current working directory \n Used by internal DistributedCache code.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;long</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#getTimestamp(org.apache.hadoop.conf.Configuration, java.net.URI)\">getTimestamp</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n             <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns mtime of a given cache file on hdfs.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setArchiveTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\">setArchiveTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                     <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is to check the timestamp of the archives to be localized.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setCacheArchives(java.net.URI[], org.apache.hadoop.conf.Configuration)\">setCacheArchives</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;archives,\n                 <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the configuration with the given set of archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setCacheFiles(java.net.URI[], org.apache.hadoop.conf.Configuration)\">setCacheFiles</A></B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;files,\n              <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the configuration with the given set of files.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setFileTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\">setFileTimestamps</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                  <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is to check the timestamp of the files to be localized.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\">setLocalArchives</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the conf to contain the location for localized archives.</TD>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD ALIGN=\"right\" VALIGN=\"top\" WIDTH=\"1%\"><FONT SIZE=\"-1\">\n<CODE>static&nbsp;void</CODE></FONT></TD>\n<TD><CODE><B><A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#setLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\">setLocalFiles</A></B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n              <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</CODE>\n\n<BR>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set the conf to contain the location for localized files.</TD>\n</TR>\n</TABLE>\n&nbsp;<A NAME=\"methods_inherited_from_class_java.lang.Object\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#EEEEFF\" CLASS=\"TableSubHeadingColor\">\n<TH ALIGN=\"left\"><B>Methods inherited from class java.lang.<A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true\" title=\"class or interface in java.lang\">Object</A></B></TH>\n</TR>\n<TR BGCOLOR=\"white\" CLASS=\"TableRowColor\">\n<TD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#clone()\" title=\"class or interface in java.lang\">clone</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#equals(java.lang.Object)\" title=\"class or interface in java.lang\">equals</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#finalize()\" title=\"class or interface in java.lang\">finalize</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#getClass()\" title=\"class or interface in java.lang\">getClass</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#hashCode()\" title=\"class or interface in java.lang\">hashCode</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#notify()\" title=\"class or interface in java.lang\">notify</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#notifyAll()\" title=\"class or interface in java.lang\">notifyAll</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#toString()\" title=\"class or interface in java.lang\">toString</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#wait()\" title=\"class or interface in java.lang\">wait</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#wait(long)\" title=\"class or interface in java.lang\">wait</A>, <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Object.html?is-external=true#wait(long, int)\" title=\"class or interface in java.lang\">wait</A></CODE></TD>\n</TR>\n</TABLE>\n&nbsp;\n<P>\n\n<!-- ============ FIELD DETAIL =========== -->\n\n<A NAME=\"field_detail\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"1\"><FONT SIZE=\"+2\">\n<B>Field Detail</B></FONT></TH>\n</TR>\n</TABLE>\n\n<A NAME=\"CACHE_FILES_SIZES\"><!-- --></A><H3>\nCACHE_FILES_SIZES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_FILES_SIZES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_SIZES\"><CODE>CACHE_FILES_SIZES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_FILES_SIZES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_ARCHIVES_SIZES\"><!-- --></A><H3>\nCACHE_ARCHIVES_SIZES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_ARCHIVES_SIZES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_SIZES\"><CODE>CACHE_ARCHIVES_SIZES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_ARCHIVES_SIZES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_ARCHIVES_TIMESTAMPS\"><!-- --></A><H3>\nCACHE_ARCHIVES_TIMESTAMPS</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_ARCHIVES_TIMESTAMPS</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES_TIMESTAMPS\"><CODE>CACHE_ARCHIVES_TIMESTAMPS</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_ARCHIVES_TIMESTAMPS\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_FILES_TIMESTAMPS\"><!-- --></A><H3>\nCACHE_FILES_TIMESTAMPS</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_FILES_TIMESTAMPS</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES_TIMESTAMPS\"><CODE>CACHE_FILES_TIMESTAMPS</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_FILES_TIMESTAMPS\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_ARCHIVES\"><!-- --></A><H3>\nCACHE_ARCHIVES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_ARCHIVES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_ARCHIVES\"><CODE>CACHE_ARCHIVES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_ARCHIVES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_FILES\"><!-- --></A><H3>\nCACHE_FILES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_FILES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_FILES\"><CODE>CACHE_FILES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_FILES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_LOCALARCHIVES\"><!-- --></A><H3>\nCACHE_LOCALARCHIVES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_LOCALARCHIVES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALARCHIVES\"><CODE>CACHE_LOCALARCHIVES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_LOCALARCHIVES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_LOCALFILES\"><!-- --></A><H3>\nCACHE_LOCALFILES</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_LOCALFILES</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_LOCALFILES\"><CODE>CACHE_LOCALFILES</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_LOCALFILES\">Constant Field Values</A></DL>\n</DL>\n<HR>\n\n<A NAME=\"CACHE_SYMLINK\"><!-- --></A><H3>\nCACHE_SYMLINK</H3>\n<PRE>\npublic static final <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A> <B>CACHE_SYMLINK</B></PRE>\n<DL>\n<DD>Warning: <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#CACHE_SYMLINK\"><CODE>CACHE_SYMLINK</CODE></A> is not a *public* constant.\n<P>\n<DL>\n<DT><B>See Also:</B><DD><A HREF=\"../../../../constant-values.html#org.apache.hadoop.filecache.DistributedCache.CACHE_SYMLINK\">Constant Field Values</A></DL>\n</DL>\n\n<!-- ========= CONSTRUCTOR DETAIL ======== -->\n\n<A NAME=\"constructor_detail\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"1\"><FONT SIZE=\"+2\">\n<B>Constructor Detail</B></FONT></TH>\n</TR>\n</TABLE>\n\n<A NAME=\"DistributedCache()\"><!-- --></A><H3>\nDistributedCache</H3>\n<PRE>\npublic <B>DistributedCache</B>()</PRE>\n<DL>\n</DL>\n\n<!-- ============ METHOD DETAIL ========== -->\n\n<A NAME=\"method_detail\"><!-- --></A>\n<TABLE BORDER=\"1\" WIDTH=\"100%\" CELLPADDING=\"3\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR BGCOLOR=\"#CCCCFF\" CLASS=\"TableHeadingColor\">\n<TH ALIGN=\"left\" COLSPAN=\"1\"><FONT SIZE=\"+2\">\n<B>Method Detail</B></FONT></TH>\n</TR>\n</TABLE>\n\n<A NAME=\"getFileStatus(org.apache.hadoop.conf.Configuration, java.net.URI)\"><!-- --></A><H3>\ngetFileStatus</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\">FileStatus</A> <B>getFileStatus</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                       <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)\n                                throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Returns <A HREF=\"../../../../org/apache/hadoop/fs/FileStatus.html\" title=\"class in org.apache.hadoop.fs\"><CODE>FileStatus</CODE></A> of a given cache file on hdfs. Internal to \n MapReduce.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - configuration<DD><CODE>cache</CODE> - cache file\n<DT><B>Returns:</B><DD><code>FileStatus</code> of a given cache file on hdfs\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getTimestamp(org.apache.hadoop.conf.Configuration, java.net.URI)\"><!-- --></A><H3>\ngetTimestamp</H3>\n<PRE>\npublic static long <B>getTimestamp</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;cache)\n                         throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Returns mtime of a given cache file on hdfs. Internal to MapReduce.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - configuration<DD><CODE>cache</CODE> - cache file\n<DT><B>Returns:</B><DD>mtime of a given cache file on hdfs\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"createAllSymlink(org.apache.hadoop.conf.Configuration, java.io.File, java.io.File)\"><!-- --></A><H3>\ncreateAllSymlink</H3>\n<PRE>\npublic static void <B>createAllSymlink</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;jobCacheDir,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/File.html?is-external=true\" title=\"class or interface in java.io\">File</A>&nbsp;workDir)\n                             throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD><B>Deprecated.</B>&nbsp;<I>Internal to MapReduce framework.  Use DistributedCacheManager\n instead.</I>\n<P>\n<DD>This method create symlinks for all files in a given dir in another directory\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - the configuration<DD><CODE>jobCacheDir</CODE> - the target directory for creating symlinks<DD><CODE>workDir</CODE> - the directory in which the symlinks are created\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setCacheArchives(java.net.URI[], org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\nsetCacheArchives</H3>\n<PRE>\npublic static void <B>setCacheArchives</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;archives,\n                                    <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Set the configuration with the given set of archives. Intended\n to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>archives</CODE> - The list of archives that need to be localized<DD><CODE>conf</CODE> - Configuration which will be changed</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setCacheFiles(java.net.URI[], org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\nsetCacheFiles</H3>\n<PRE>\npublic static void <B>setCacheFiles</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;files,\n                                 <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Set the configuration with the given set of files.  Intended to be\n used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>files</CODE> - The list of files that need to be localized<DD><CODE>conf</CODE> - Configuration which will be changed</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getCacheArchives(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetCacheArchives</H3>\n<PRE>\npublic static <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[] <B>getCacheArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                              throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Get cache archives set in the Configuration.  Used by\n internal DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which contains the archives\n<DT><B>Returns:</B><DD>An array of the caches set in the Configuration\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getCacheFiles(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetCacheFiles</H3>\n<PRE>\npublic static <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[] <B>getCacheFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                           throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Get cache files set in the Configuration.  Used by internal\n DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which contains the files\n<DT><B>Returns:</B><DD>Am array of the files set in the Configuration\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getLocalCacheArchives(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetLocalCacheArchives</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getLocalCacheArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                                    throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Return the path array of the localized caches.  Intended to be used\n by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the localized archives\n<DT><B>Returns:</B><DD>A path array of localized caches\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getLocalCacheFiles(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetLocalCacheFiles</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getLocalCacheFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                                 throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Return the path array of the localized files.  Intended to be used\n by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the localized files\n<DT><B>Returns:</B><DD>A path array of localized files\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getArchiveTimestamps(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetArchiveTimestamps</H3>\n<PRE>\npublic static long[] <B>getArchiveTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the timestamps of the archives.  Used by internal\n DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which stored the timestamps\n<DT><B>Returns:</B><DD>a long array of timestamps\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getFileTimestamps(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetFileTimestamps</H3>\n<PRE>\npublic static long[] <B>getFileTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the timestamps of the files.  Used by internal\n DistributedCache and MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The configuration which stored the timestamps\n<DT><B>Returns:</B><DD>a long array of timestamps\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setArchiveTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetArchiveTimestamps</H3>\n<PRE>\npublic static void <B>setArchiveTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                        <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</PRE>\n<DL>\n<DD>This is to check the timestamp of the archives to be localized.\n Used by internal MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration which stores the timestamp's<DD><CODE>timestamps</CODE> - comma separated list of timestamps of archives.\n The order should be the same as the order in which the archives are added.</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setFileTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetFileTimestamps</H3>\n<PRE>\npublic static void <B>setFileTimestamps</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                     <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;timestamps)</PRE>\n<DL>\n<DD>This is to check the timestamp of the files to be localized.\n Used by internal MapReduce code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration which stores the timestamp's<DD><CODE>timestamps</CODE> - comma separated list of timestamps of files.\n The order should be the same as the order in which the files are added.</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetLocalArchives</H3>\n<PRE>\npublic static void <B>setLocalArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Set the conf to contain the location for localized archives.  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local archives</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"setLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\nsetLocalFiles</H3>\n<PRE>\npublic static void <B>setLocalFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Set the conf to contain the location for localized files.  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local files</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\naddLocalArchives</H3>\n<PRE>\npublic static void <B>addLocalArchives</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                    <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Add a archive that has been localized to the conf.  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local archives</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)\"><!-- --></A><H3>\naddLocalFiles</H3>\n<PRE>\npublic static void <B>addLocalFiles</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                 <A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/String.html?is-external=true\" title=\"class or interface in java.lang\">String</A>&nbsp;str)</PRE>\n<DL>\n<DD>Add a file that has been localized to the conf..  Used\n by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - The conf to modify to contain the localized caches<DD><CODE>str</CODE> - a comma separated list of local files</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addCacheArchive(java.net.URI, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddCacheArchive</H3>\n<PRE>\npublic static void <B>addCacheArchive</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n                                   <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Add a archives to be localized to the conf.  Intended to\n be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>uri</CODE> - The uri of the cache to be localized<DD><CODE>conf</CODE> - Configuration to add the cache to</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addCacheFile(java.net.URI, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddCacheFile</H3>\n<PRE>\npublic static void <B>addCacheFile</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>&nbsp;uri,\n                                <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Add a file to be localized to the conf.  Intended\n to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>uri</CODE> - The uri of the cache to be localized<DD><CODE>conf</CODE> - Configuration to add the cache to</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddFileToClassPath</H3>\n<PRE>\n<FONT SIZE=\"-1\"><A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Deprecated.html?is-external=true\" title=\"class or interface in java.lang\">@Deprecated</A>\n</FONT>public static void <B>addFileToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                                                 <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                               throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD><B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addFileToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I>\n<P>\n<DD>Add a file path to the current set of classpath entries. It adds the file\n to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>file</CODE> - Path of the file to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addFileToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><!-- --></A><H3>\naddFileToClassPath</H3>\n<PRE>\npublic static void <B>addFileToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;file,\n                                      <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                      <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)\n                               throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Add a file path to the current set of classpath entries. It adds the file\n to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>file</CODE> - Path of the file to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting<DD><CODE>fs</CODE> - FileSystem with respect to which <code>archivefile</code> should\n              be interpreted.\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getFileClassPaths(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetFileClassPaths</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getFileClassPaths</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the file entries in classpath as an array of Path.\n Used by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the classpath setting</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\naddArchiveToClassPath</H3>\n<PRE>\n<FONT SIZE=\"-1\"><A HREF=\"http://java.sun.com/javase/6/docs/api/java/lang/Deprecated.html?is-external=true\" title=\"class or interface in java.lang\">@Deprecated</A>\n</FONT>public static void <B>addArchiveToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                                                    <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)\n                                  throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD><B>Deprecated.</B>&nbsp;<I>Please use <A HREF=\"../../../../org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><CODE>addArchiveToClassPath(Path, Configuration, FileSystem)</CODE></A> \n instead.  The <code>FileSystem</code> should be obtained within an\n appropriate <code>doAs</code>.</I>\n<P>\n<DD>Add an archive path to the current set of classpath entries. It adds the\n archive to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>archive</CODE> - Path of the archive to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem)\"><!-- --></A><H3>\naddArchiveToClassPath</H3>\n<PRE>\npublic static void <B>addArchiveToClassPath</B>(<A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>&nbsp;archive,\n                                         <A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf,\n                                         <A HREF=\"../../../../org/apache/hadoop/fs/FileSystem.html\" title=\"class in org.apache.hadoop.fs\">FileSystem</A>&nbsp;fs)\n                                  throws <A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></PRE>\n<DL>\n<DD>Add an archive path to the current set of classpath entries. It adds the\n archive to cache as well.  Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>archive</CODE> - Path of the archive to be added<DD><CODE>conf</CODE> - Configuration that contains the classpath setting<DD><CODE>fs</CODE> - FileSystem with respect to which <code>archive</code> should be interpreted.\n<DT><B>Throws:</B>\n<DD><CODE><A HREF=\"http://java.sun.com/javase/6/docs/api/java/io/IOException.html?is-external=true\" title=\"class or interface in java.io\">IOException</A></CODE></DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getArchiveClassPaths(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetArchiveClassPaths</H3>\n<PRE>\npublic static <A HREF=\"../../../../org/apache/hadoop/fs/Path.html\" title=\"class in org.apache.hadoop.fs\">Path</A>[] <B>getArchiveClassPaths</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>Get the archive entries in classpath as an array of Path.\n Used by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - Configuration that contains the classpath setting</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"createSymlink(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ncreateSymlink</H3>\n<PRE>\npublic static void <B>createSymlink</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>This method allows you to create symlinks in the current working directory\n of the task to all the cache files/archives.\n Intended to be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - the jobconf</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"getSymlink(org.apache.hadoop.conf.Configuration)\"><!-- --></A><H3>\ngetSymlink</H3>\n<PRE>\npublic static boolean <B>getSymlink</B>(<A HREF=\"../../../../org/apache/hadoop/conf/Configuration.html\" title=\"class in org.apache.hadoop.conf\">Configuration</A>&nbsp;conf)</PRE>\n<DL>\n<DD>This method checks to see if symlinks are to be create for the \n localized cache files in the current working directory \n Used by internal DistributedCache code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>conf</CODE> - the jobconf\n<DT><B>Returns:</B><DD>true if symlinks are to be created- else return false</DL>\n</DD>\n</DL>\n<HR>\n\n<A NAME=\"checkURIs(java.net.URI[], java.net.URI[])\"><!-- --></A><H3>\ncheckURIs</H3>\n<PRE>\npublic static boolean <B>checkURIs</B>(<A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriFiles,\n                                <A HREF=\"http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true\" title=\"class or interface in java.net\">URI</A>[]&nbsp;uriArchives)</PRE>\n<DL>\n<DD>This method checks if there is a conflict in the fragment names \n of the uris. Also makes sure that each uri has a fragment. It \n is only to be called if you want to create symlinks for \n the various archives and files.  May be used by user code.\n<P>\n<DD><DL>\n<DT><B>Parameters:</B><DD><CODE>uriFiles</CODE> - The uri array of urifiles<DD><CODE>uriArchives</CODE> - the uri array of uri archives</DL>\n</DD>\n</DL>\n<!-- ========= END OF CLASS DATA ========= -->\n<HR>\n\n\n<!-- ======= START OF BOTTOM NAVBAR ====== -->\n<A NAME=\"navbar_bottom\"><!-- --></A>\n<A HREF=\"#skip-navbar_bottom\" title=\"Skip navigation links\"></A>\n<TABLE BORDER=\"0\" WIDTH=\"100%\" CELLPADDING=\"1\" CELLSPACING=\"0\" SUMMARY=\"\">\n<TR>\n<TD COLSPAN=2 BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">\n<A NAME=\"navbar_bottom_firstrow\"><!-- --></A>\n<TABLE BORDER=\"0\" CELLPADDING=\"0\" CELLSPACING=\"3\" SUMMARY=\"\">\n  <TR ALIGN=\"center\" VALIGN=\"top\">\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../overview-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Overview</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-summary.html\"><FONT CLASS=\"NavBarFont1\"><B>Package</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#FFFFFF\" CLASS=\"NavBarCell1Rev\"> &nbsp;<FONT CLASS=\"NavBarFont1Rev\"><B>Class</B></FONT>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"class-use/DistributedCache.html\"><FONT CLASS=\"NavBarFont1\"><B>Use</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"package-tree.html\"><FONT CLASS=\"NavBarFont1\"><B>Tree</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../deprecated-list.html\"><FONT CLASS=\"NavBarFont1\"><B>Deprecated</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../index-all.html\"><FONT CLASS=\"NavBarFont1\"><B>Index</B></FONT></A>&nbsp;</TD>\n  <TD BGCOLOR=\"#EEEEFF\" CLASS=\"NavBarCell1\">    <A HREF=\"../../../../help-doc.html\"><FONT CLASS=\"NavBarFont1\"><B>Help</B></FONT></A>&nbsp;</TD>\n  </TR>\n</TABLE>\n</TD>\n<TD ALIGN=\"right\" VALIGN=\"top\" ROWSPAN=3><EM>\n</EM>\n</TD>\n</TR>\n\n<TR>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n&nbsp;PREV CLASS&nbsp;\n&nbsp;<A HREF=\"../../../../org/apache/hadoop/filecache/TaskDistributedCacheManager.html\" title=\"class in org.apache.hadoop.filecache\"><B>NEXT CLASS</B></A></FONT></TD>\n<TD BGCOLOR=\"white\" CLASS=\"NavBarCell2\"><FONT SIZE=\"-2\">\n  <A HREF=\"../../../../index.html?org/apache/hadoop/filecache/DistributedCache.html\" target=\"_top\"><B>FRAMES</B></A>  &nbsp;\n&nbsp;<A HREF=\"DistributedCache.html\" target=\"_top\"><B>NO FRAMES</B></A>  &nbsp;\n&nbsp;<SCRIPT type=\"text/javascript\">\n  <!--\n  if(window==top) {\n    document.writeln('<A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>');\n  }\n  //-->\n</SCRIPT>\n<NOSCRIPT>\n  <A HREF=\"../../../../allclasses-noframe.html\"><B>All Classes</B></A>\n</NOSCRIPT>\n\n\n</FONT></TD>\n</TR>\n<TR>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\n  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;<A HREF=\"#field_summary\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_summary\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_summary\">METHOD</A></FONT></TD>\n<TD VALIGN=\"top\" CLASS=\"NavBarCell3\"><FONT SIZE=\"-2\">\nDETAIL:&nbsp;<A HREF=\"#field_detail\">FIELD</A>&nbsp;|&nbsp;<A HREF=\"#constructor_detail\">CONSTR</A>&nbsp;|&nbsp;<A HREF=\"#method_detail\">METHOD</A></FONT></TD>\n</TR>\n</TABLE>\n<A NAME=\"skip-navbar_bottom\"></A>\n<!-- ======== END OF BOTTOM NAVBAR ======= -->\n\n<HR>\nCopyright &copy; 2009 The Apache Software Foundation\n</BODY>\n</HTML>\n","date":"2015-12-29T14:28:01.000Z","updated":"2015-12-29T14:28:01.000Z","path":"images/2012/02/DistributedCache.html","title":"","comments":1,"layout":"page","_id":"ciixba91000013x8fdtf3n21q"}],"Post":[{"title":"SSH 资料","date":"2015-12-30T12:19:00.000Z","_content":"\n## 1. 原理\n\n### 1.1 SSH 原理\n\n1. 远程主机收到用户的登录请求，把自己的公钥发给用户\n2. 用户使用这个公钥，将登录密码加密后，发送回来\n3. 远程主机用自己的私钥，解密登录密码。如果密码正确，就同意用户登录\n\n<!--more-->\n\n### 1.2 免密原理\n\n1. 将用户的公钥存在远程主机\n2. 登录时，远程主机通过用户的公钥加密一段随机的字符串发送回去\n3. 用户用自己的私钥解密字符串后返回。如果字符串正确，就同意用户登录\n\n### 1.3 为什么有 known_hosts\n\n1. 恶意拦截登录请求，冒充远程主机，将伪造的公钥发送给用户\n2. 用户拿到公钥后，加密密码发送过来。恶意拦截者就可以拿到密码\n\n为了避免上面的情况，加上 known_hosts，可以防止以后使用的时候被恶意拦截，但是无法防止第一次被恶意拦截（第一次一般会把公钥指纹打印出来问一下，个人觉得作用不大，没人会看的）\n\n## 2. 操作\n\n### 2.1 免询问\n\n[ssh: automatically accept keys](http://askubuntu.com/questions/123072/ssh-automatically-accept-keys)\n\n* ssh -oStrictHostKeyChecking=no user@host\n\n> 注：这个在 mac 上不生效\n\n或者在 ~/.ssh/config 中加\n\n```\nHost *\n  StrictHostKeyChecking no\n```\n\n> 注：如果服务器变更了，这个 known_hosts 需要删掉重新生成。你也可以将 known_hosts 这个文件指向 /dev/null。使用 `-oUserKnownHostsFile=/dev/null`\n\n### 2.2 免密\n\n在需要免密登录其它机器的机器上执行：\n\n* ssh-keygen -t rsa\n* ssh user@host 'mkdir -p .ssh && cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub\n\n> `'cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub` 的作用是，将本地的公钥文件 `~/.ssh/id_rsa.pub`，重定向追加到远程文件 authorized_keys 的末尾\n","source":"_posts/2015/12/SSH.md","raw":"title: SSH 资料\ndate: 2015-12-30 20:19:00\ncategories: 自学资料\ntags: [SSH]\n---\n\n## 1. 原理\n\n### 1.1 SSH 原理\n\n1. 远程主机收到用户的登录请求，把自己的公钥发给用户\n2. 用户使用这个公钥，将登录密码加密后，发送回来\n3. 远程主机用自己的私钥，解密登录密码。如果密码正确，就同意用户登录\n\n<!--more-->\n\n### 1.2 免密原理\n\n1. 将用户的公钥存在远程主机\n2. 登录时，远程主机通过用户的公钥加密一段随机的字符串发送回去\n3. 用户用自己的私钥解密字符串后返回。如果字符串正确，就同意用户登录\n\n### 1.3 为什么有 known_hosts\n\n1. 恶意拦截登录请求，冒充远程主机，将伪造的公钥发送给用户\n2. 用户拿到公钥后，加密密码发送过来。恶意拦截者就可以拿到密码\n\n为了避免上面的情况，加上 known_hosts，可以防止以后使用的时候被恶意拦截，但是无法防止第一次被恶意拦截（第一次一般会把公钥指纹打印出来问一下，个人觉得作用不大，没人会看的）\n\n## 2. 操作\n\n### 2.1 免询问\n\n[ssh: automatically accept keys](http://askubuntu.com/questions/123072/ssh-automatically-accept-keys)\n\n* ssh -oStrictHostKeyChecking=no user@host\n\n> 注：这个在 mac 上不生效\n\n或者在 ~/.ssh/config 中加\n\n```\nHost *\n  StrictHostKeyChecking no\n```\n\n> 注：如果服务器变更了，这个 known_hosts 需要删掉重新生成。你也可以将 known_hosts 这个文件指向 /dev/null。使用 `-oUserKnownHostsFile=/dev/null`\n\n### 2.2 免密\n\n在需要免密登录其它机器的机器上执行：\n\n* ssh-keygen -t rsa\n* ssh user@host 'mkdir -p .ssh && cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub\n\n> `'cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub` 的作用是，将本地的公钥文件 `~/.ssh/id_rsa.pub`，重定向追加到远程文件 authorized_keys 的末尾\n","slug":"2015/12/SSH","published":1,"updated":"2015-12-30T12:22:10.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91400023x8fjhea3hiw"},{"title":"Docker 插件 - Volume plugins","date":"2015-10-14T14:05:00.000Z","_content":"\n## Docker 插件是什么\n\ndocker 插件是 docker 提供出来的扩展机制，目前 docker 支持 volume 和 network 两种插件，由于 network 插件比较复杂而且没有好的开源项目，这里主要介绍 volume 插件。\n\n插件是一个独立的进程和 docker daemon 运行在同一台 host 上，通过 Plugin Discovery 的机制进行插件发现，插件有几个要求：\n\n* 插件名要求是小写\n* 插件可以运行在容器内也可以运行在容器外，不过现阶段建议运行在容器外\n\n<!-- more -->\n\n## 插件发现\n\n插件发现机制需要插件将自己的地址文件放在固定目录，方便 docker 发现插件进程，有三种文件可以设置：\n\n* `.sock` 文件是 UNIX domain sockets\n* `.spec` 文本文件内包含了一个 URL，比如：`unix:///other.sock`\n* `.json` 文本文件包含了插件的完整 JSON 描述\n\nUNIX domain socket 文件必须放在 `/run/docker/plugins` 目录，但是 `.spec`，`.json` 文件则可以放在 `/etc/docker/plugins` 或者 `/usr/lib/docker/plugins` 中。\n\n无后缀的文件名决定了插件的名字，比如 `/run/docker/plugins/myplugin.sock` 的插件名就是 `myplugin`。你可以在子目录中放置地址文件，比如 `/run/docker/plugins/myplugin/myplugin.sock`。\n\ndocker 优先搜索 `/run/docker/plugins` 目录，如果没有 unix socket 的话才会去搜索 `/etc/docker/plugins` 和 `/usr/lib/docker/plugins`，如果根据指定插件名搜到了插件就会立马停止搜索。\n\n### `.json`\n\nJSON 格式文件示例：\n\n```\n{\n  \"Name\": \"plugin-example\",\n  \"Addr\": \"https://example.com/docker/plugin\",\n  \"TLSConfig\": {\n    \"InsecureSkipVerify\": false,\n    \"CAFile\": \"/usr/shared/docker/certs/example-ca.pem\",\n    \"CertFile\": \"/usr/shared/docker/certs/example-cert.pem\",\n    \"KeyFile\": \"/usr/shared/docker/certs/example-key.pem\",\n  }\n}\n```\n\n## 插件生命周期\n\n* 启动插件\n* 启动 docker\n* 停止 docker\n* 停止插件\n\n\n## 插件激活\n\n运行命令 `docker run --volume-driver=foo` 即可以激活名为 `foo` 的 volume 插件，需要注意的是，插件是按需加载机制，只有被使用到了才会被激活。\n\n## volume 插件使用\n\n示例：\n\n```\n$ docker run -ti -v volumename:/data --volume-driver=flocker busybox sh\n```\n\n上面表示的意思是，使用 flocker 插件将 voluemname 挂载到容器的 /data 目录。\n\n注意：volumename 一定不能以 `/` 开头。（文档说的，没看 docker 源码，我实现一个以 `/` 开头好像也没问题，应该是规范吧）\n\n## 插件 API 设计\n\n插件是 API 是基于 HTTP 的 JSON POST 请求，所以插件需要实现一个 HTTP 服务器并且将其 bind 到一个 UNIX socket 上。API 的版本设置在了 HTTP\n头里面，现在这个头的固定值为：`application/vnd.docker.plugins.v1+json`\n\n不过 docker 的开发人员已经提供了一个比较好的 docker volume 的扩展 API 代码，可以参考：[docker-volume-extension-api](https://github.com/calavera/dkvolume)\n\n### `/Plugin.Activate`\n\n请求：空\n\n响应：\n\n```json\n{\n  \"Implements:\" [\"VolumeDriver\"]\n}\n```\n\n返回插件实现，表示是 volume 插件\n\n### `/VolumeDriver.Create`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n告诉插件用户想要创建一个 volume，并将用户输入的 volume 名传给插件。插件在这个时候可以不用理会这个请求，会有真正挂载的请求。\n\n响应：\n\n```\n{\n    \"Err\": null\n}\n```\n\n如果出错返回错误字符串。\n\n### `/VolumeDriver.Remove`\n\n与 Create 相对应。\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n响应：\n\n```\n{\n    \"Err\": null\n}\n```\n\n\n### `/VolumeDriver.Mount`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n用户请求挂载某个文件，这个请求仅会在容器启动时发送一次。\n\n响应：\n\n```\n{\n   \"Mountpoint\": \"/path/to/directory/on/host\",\n   \"Err\": null\n}\n```\n\n将 volume_name 挂载的真正挂载点返回给 docker，如果出错则返回错误字符串。\n\n\n### `/VolumeDriver.Path`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n响应：\n\n```\n{\n   \"Mountpoint\": \"/path/to/directory/on/host\",\n   \"Err\": null\n}\n```\n\n插件需要管理 volume_name 的真正挂载地址，这个请求需要将 volume_name 挂载的真正挂载点返回给 docker，如果出错则返回错误字符串。\n\n### `/VolumeDriver.Unmount`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n表示 docker 已经不需要这个 volume 了，插件需要安全的将这个挂载从挂载点卸载。\n\n响应：\n\n```\n{\n    \"Err\": null\n}\n```\n\n\n> 参考地址：[Understand Docker plugins](https://docs.docker.com/extend/plugins/)\n","source":"_posts/2015/10/docker-volume-plugin.md","raw":"title: Docker 插件 - Volume plugins\ndate: 2015-10-14 22:05:00\ncategories: 自学资料\ntags: [Docker, Docker plugins, Docker volume]\n---\n\n## Docker 插件是什么\n\ndocker 插件是 docker 提供出来的扩展机制，目前 docker 支持 volume 和 network 两种插件，由于 network 插件比较复杂而且没有好的开源项目，这里主要介绍 volume 插件。\n\n插件是一个独立的进程和 docker daemon 运行在同一台 host 上，通过 Plugin Discovery 的机制进行插件发现，插件有几个要求：\n\n* 插件名要求是小写\n* 插件可以运行在容器内也可以运行在容器外，不过现阶段建议运行在容器外\n\n<!-- more -->\n\n## 插件发现\n\n插件发现机制需要插件将自己的地址文件放在固定目录，方便 docker 发现插件进程，有三种文件可以设置：\n\n* `.sock` 文件是 UNIX domain sockets\n* `.spec` 文本文件内包含了一个 URL，比如：`unix:///other.sock`\n* `.json` 文本文件包含了插件的完整 JSON 描述\n\nUNIX domain socket 文件必须放在 `/run/docker/plugins` 目录，但是 `.spec`，`.json` 文件则可以放在 `/etc/docker/plugins` 或者 `/usr/lib/docker/plugins` 中。\n\n无后缀的文件名决定了插件的名字，比如 `/run/docker/plugins/myplugin.sock` 的插件名就是 `myplugin`。你可以在子目录中放置地址文件，比如 `/run/docker/plugins/myplugin/myplugin.sock`。\n\ndocker 优先搜索 `/run/docker/plugins` 目录，如果没有 unix socket 的话才会去搜索 `/etc/docker/plugins` 和 `/usr/lib/docker/plugins`，如果根据指定插件名搜到了插件就会立马停止搜索。\n\n### `.json`\n\nJSON 格式文件示例：\n\n```\n{\n  \"Name\": \"plugin-example\",\n  \"Addr\": \"https://example.com/docker/plugin\",\n  \"TLSConfig\": {\n    \"InsecureSkipVerify\": false,\n    \"CAFile\": \"/usr/shared/docker/certs/example-ca.pem\",\n    \"CertFile\": \"/usr/shared/docker/certs/example-cert.pem\",\n    \"KeyFile\": \"/usr/shared/docker/certs/example-key.pem\",\n  }\n}\n```\n\n## 插件生命周期\n\n* 启动插件\n* 启动 docker\n* 停止 docker\n* 停止插件\n\n\n## 插件激活\n\n运行命令 `docker run --volume-driver=foo` 即可以激活名为 `foo` 的 volume 插件，需要注意的是，插件是按需加载机制，只有被使用到了才会被激活。\n\n## volume 插件使用\n\n示例：\n\n```\n$ docker run -ti -v volumename:/data --volume-driver=flocker busybox sh\n```\n\n上面表示的意思是，使用 flocker 插件将 voluemname 挂载到容器的 /data 目录。\n\n注意：volumename 一定不能以 `/` 开头。（文档说的，没看 docker 源码，我实现一个以 `/` 开头好像也没问题，应该是规范吧）\n\n## 插件 API 设计\n\n插件是 API 是基于 HTTP 的 JSON POST 请求，所以插件需要实现一个 HTTP 服务器并且将其 bind 到一个 UNIX socket 上。API 的版本设置在了 HTTP\n头里面，现在这个头的固定值为：`application/vnd.docker.plugins.v1+json`\n\n不过 docker 的开发人员已经提供了一个比较好的 docker volume 的扩展 API 代码，可以参考：[docker-volume-extension-api](https://github.com/calavera/dkvolume)\n\n### `/Plugin.Activate`\n\n请求：空\n\n响应：\n\n```json\n{\n  \"Implements:\" [\"VolumeDriver\"]\n}\n```\n\n返回插件实现，表示是 volume 插件\n\n### `/VolumeDriver.Create`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n告诉插件用户想要创建一个 volume，并将用户输入的 volume 名传给插件。插件在这个时候可以不用理会这个请求，会有真正挂载的请求。\n\n响应：\n\n```\n{\n    \"Err\": null\n}\n```\n\n如果出错返回错误字符串。\n\n### `/VolumeDriver.Remove`\n\n与 Create 相对应。\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n响应：\n\n```\n{\n    \"Err\": null\n}\n```\n\n\n### `/VolumeDriver.Mount`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n用户请求挂载某个文件，这个请求仅会在容器启动时发送一次。\n\n响应：\n\n```\n{\n   \"Mountpoint\": \"/path/to/directory/on/host\",\n   \"Err\": null\n}\n```\n\n将 volume_name 挂载的真正挂载点返回给 docker，如果出错则返回错误字符串。\n\n\n### `/VolumeDriver.Path`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n响应：\n\n```\n{\n   \"Mountpoint\": \"/path/to/directory/on/host\",\n   \"Err\": null\n}\n```\n\n插件需要管理 volume_name 的真正挂载地址，这个请求需要将 volume_name 挂载的真正挂载点返回给 docker，如果出错则返回错误字符串。\n\n### `/VolumeDriver.Unmount`\n\n请求：\n\n```\n{\n    \"Name\": \"volume_name\"\n}\n```\n\n表示 docker 已经不需要这个 volume 了，插件需要安全的将这个挂载从挂载点卸载。\n\n响应：\n\n```\n{\n    \"Err\": null\n}\n```\n\n\n> 参考地址：[Understand Docker plugins](https://docs.docker.com/extend/plugins/)\n","slug":"2015/10/docker-volume-plugin","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91900073x8fg46h5uvz"},{"title":"Docker Compose 最佳实践","date":"2015-10-08T12:00:00.000Z","_content":"\n## docker-compose 小功能\n\n* daemon 模式\n\n`docker-compose up -d`\n\n* 设置 container 的名字\n\n显式设置 container 的名字：[Issue 讨论](https://github.com/docker/compose/pull/1711), [yml.md: container_name](https://github.com/docker/compose/blob/master/docs/yml.md#container_name)\n\n    * 设置了这个的话，scale 能力就无法使用了\n    * container 名字默认格式：${PROJECT}-${NAME}-${sacle_num}\n\n## docker-compose 的问题\n\n* 无法给 yml 传递参数 [Issue 讨论](https://github.com/docker/compose/issues/1377)，好像有[解决方案](https://github.com/docker/compose/blob/master/docs/yml.md#variable-substitution)，不过不好用\n* 无法给 container 之间加依赖 [Issue](https://github.com/docker/compose/issues/374)\n* 仅能控制多个 container 的启动和关系，如果有初始化任务（如DB 初始化），还需要额外写脚本文件\n* docker-compose 和 docker-swarm 集成还在开发中：https://github.com/docker/compose/blob/master/SWARM.md\n","source":"_posts/2015/10/docker-compose-pratice.md","raw":"title: Docker Compose 最佳实践\ndate: 2015-10-08 20:00:00\ncategories: 最佳实践\ntags: [Docker, Docker Compose]\n---\n\n## docker-compose 小功能\n\n* daemon 模式\n\n`docker-compose up -d`\n\n* 设置 container 的名字\n\n显式设置 container 的名字：[Issue 讨论](https://github.com/docker/compose/pull/1711), [yml.md: container_name](https://github.com/docker/compose/blob/master/docs/yml.md#container_name)\n\n    * 设置了这个的话，scale 能力就无法使用了\n    * container 名字默认格式：${PROJECT}-${NAME}-${sacle_num}\n\n## docker-compose 的问题\n\n* 无法给 yml 传递参数 [Issue 讨论](https://github.com/docker/compose/issues/1377)，好像有[解决方案](https://github.com/docker/compose/blob/master/docs/yml.md#variable-substitution)，不过不好用\n* 无法给 container 之间加依赖 [Issue](https://github.com/docker/compose/issues/374)\n* 仅能控制多个 container 的启动和关系，如果有初始化任务（如DB 初始化），还需要额外写脚本文件\n* docker-compose 和 docker-swarm 集成还在开发中：https://github.com/docker/compose/blob/master/SWARM.md\n","slug":"2015/10/docker-compose-pratice","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91b000f3x8f5at6t47y"},{"title":"Java 网络编程最佳实践","date":"2015-09-09T16:00:00.000Z","_content":"\n\n## 1. 通信层\n\n* 直接使用最成熟的网络框架，如 Netty\n* 单连接 & 连接复用 & 长连接\n  * 建议提前设计心跳机制\n  * 集群较小，长连接无需开启心跳\n  * 如果网络情况比较复杂，建议开启心跳。如有防火墙，会将连接清掉且不会向客户端发送 RST 信令，导致长连接变成一个脏连接\n\n<!-- more -->\n\n## 2. 线程模型\n\n如果采用了 Netty 这样的框架，线程模型基本已经决定了，但是 Netty 只需负责 IO 处理，需要提供额外的业务线程池负责处理业务请求。\n\n* 序列化过程在业务线程中处理\n* 请求/响应包多个批量从 IO 线程交给业务线程处理\n* 服务端线程池需要有保护策略\n  * 框架层面的 RejectExcetpion\n  * 业务层面的限流策略\n* 需要定时打印线程池大小，方便性能分析\n\n## 3. 序列化\n\n* 全站都是 Java 系\n\n为了以后能让其它语言更好的交互，协议设计越扁平越好，切忌将整个协议类对象序列化，仅整个序列化方法参数对象列表及返回值对象。\n\n* 全站各种语言百花齐放\n\n可以考虑直接使用 Protobuf 或者 msgpack 这样的跨语言的序列化协议，但我个人没使用经验。\n\n\n## 4. 容灾\n\n* 必须要有超时时间，分布式环境下无超时机制对整体环境影响非常大\n* 需要有连接隔离机制（根据请求量、错误率等）\n  * 可参考 [Netflix 的 ribbon](https://github.com/Netflix/ribbon)\n\n\n## 5. 故障定位\n\n* 客户端发送、服务端接收均需要打印日志，日志必须要有的几个字段：\n  * 时间戳\n  * 唯一ID： 由于客户端和服务端的请求量较大，所以需要有唯一 ID 能够将客户端日志和服务端请求串起来\n  * IP 信息\n  * 如下：\n   * client:time,unique-id,server-ip\n   * server:time,unique-id,client-ip\n* 提供较好的方式打印网络层日志\n  * 经常会发生客户端有请求日志，但是服务端没有接收日志的情况\n  * 这个时候无法判断是客户端出错还是服务端出错，可以提供 debug 日志打印网络层的请求日志\n","source":"_posts/2015/09/remoting-practice.md","raw":"title: Java 网络编程最佳实践\ncategories: 最佳实践\ntags: [Java, Network]\ndate: 2015-09-10 00:00:00\n---\n\n\n## 1. 通信层\n\n* 直接使用最成熟的网络框架，如 Netty\n* 单连接 & 连接复用 & 长连接\n  * 建议提前设计心跳机制\n  * 集群较小，长连接无需开启心跳\n  * 如果网络情况比较复杂，建议开启心跳。如有防火墙，会将连接清掉且不会向客户端发送 RST 信令，导致长连接变成一个脏连接\n\n<!-- more -->\n\n## 2. 线程模型\n\n如果采用了 Netty 这样的框架，线程模型基本已经决定了，但是 Netty 只需负责 IO 处理，需要提供额外的业务线程池负责处理业务请求。\n\n* 序列化过程在业务线程中处理\n* 请求/响应包多个批量从 IO 线程交给业务线程处理\n* 服务端线程池需要有保护策略\n  * 框架层面的 RejectExcetpion\n  * 业务层面的限流策略\n* 需要定时打印线程池大小，方便性能分析\n\n## 3. 序列化\n\n* 全站都是 Java 系\n\n为了以后能让其它语言更好的交互，协议设计越扁平越好，切忌将整个协议类对象序列化，仅整个序列化方法参数对象列表及返回值对象。\n\n* 全站各种语言百花齐放\n\n可以考虑直接使用 Protobuf 或者 msgpack 这样的跨语言的序列化协议，但我个人没使用经验。\n\n\n## 4. 容灾\n\n* 必须要有超时时间，分布式环境下无超时机制对整体环境影响非常大\n* 需要有连接隔离机制（根据请求量、错误率等）\n  * 可参考 [Netflix 的 ribbon](https://github.com/Netflix/ribbon)\n\n\n## 5. 故障定位\n\n* 客户端发送、服务端接收均需要打印日志，日志必须要有的几个字段：\n  * 时间戳\n  * 唯一ID： 由于客户端和服务端的请求量较大，所以需要有唯一 ID 能够将客户端日志和服务端请求串起来\n  * IP 信息\n  * 如下：\n   * client:time,unique-id,server-ip\n   * server:time,unique-id,client-ip\n* 提供较好的方式打印网络层日志\n  * 经常会发生客户端有请求日志，但是服务端没有接收日志的情况\n  * 这个时候无法判断是客户端出错还是服务端出错，可以提供 debug 日志打印网络层的请求日志\n","slug":"2015/09/remoting-practice","published":1,"updated":"2015-12-30T12:23:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91c000l3x8frog3l52g"},{"title":"Java 源码阅读 最佳实践","date":"2015-09-01T15:40:00.000Z","_content":"\n## 1. 原则\n\n\n### 原则1：了解使用\n\n仔细查看使用文档和说明，写较为详细的 demo 程序。\n\n### 原则2：了解全局\n\n了解该产品解决了哪些问题，并了解其周边产品及优缺点。\n\n### 原则3：了解原理\n\n查看其内部架构文档，如果有了周边产品的了解，可以从周边产品推算出其实现基本原理。不过当对其它产品了解比较深刻的时候，这个原则很容易就可以达到了。\n\n<!-- more -->\n\n## 2. 方式\n\n### 单步 Debug\n\n单步 Debug 是比直接阅读源码来得更加直观快速的方式，通过单步 Debug 的主要目的是了解整个源码的大概逻辑和流程。\n\n* 针对应用程序（Tomcat、Hadoop）：找启动脚本 -> 找到 main 入口 -> 开始 Debug\n* 库、框架（FastJSON、Spring）：找到关键类 -> 打断点 -> 查看堆栈\n\n### 「无法 Debug？」\n\n很多时候我们拿到的都是「无法运行」的代码，比如说构建起来很麻烦的源码、只有一个 lib 包。在这个时候只能采取一些其它手段了。\n\n* 在我看来，最有效的方式还是 Debug。如果遇到了比较难构建的源码，那么暂时放弃构建它的想法，可以直接新建一个工程并添加其 Jar 包依赖，写测试代码的方式进行 Debug。至于测试代码，可以从源码的官方文档、或者它的测试用例中找到；\n\n* 如果以上也是比较麻烦的话，那就直接强行裸看代码吧，不过裸看代码也有一些简便的方法让你更好的裸看代码，比如接下来的。\n\n\n### 梳理整个工程的类依赖关系图\n\n以 Intellij IDEA 的为例：\n\n\n* 右键 Java 类\n* 点击 Diagrams\n* 点击 Show Diagrams\n\n\n### 查找方法调用关系\n\n以 Intellij IDEA 的 Mac OS X 的快捷键为例：\n\n* Alt + F7 (Find Usage)\n* Ctrl + Alt + H (Call Hierarchy)\n\n\n### 搜索关键字\n\n这个一般在无头绪的时候才会使用的。比如出现了框架内部的错误，里面包含了特殊关键字，如错误日志或者没有堆栈的方法。\n\n这个在 Intellij IDEA 中比较好用，可以包含依赖的三方库进行全文搜索。快捷键：\n\n* Ctrl + Shift + F (Find in path)\n\n> 搜索三方库需要选择 Custom -> Project and Libraries\n\n如果是 Eclipse 的话，似乎只能是大概知道是哪个 Jar 包之后，解压 source 包再全文搜索吧 = =\n\n## 3. 其他\n\n我好像就上面几个方法，看任何源码（Hadoop、Spring、Netty、公司内部的）都能够很快上手。如果想到啥或者有啥比较典型的实例的话，再补充吧。\n","source":"_posts/2015/09/java-source-code-practice.md","raw":"title: Java 源码阅读 最佳实践\ndate: 2015-09-01 23:40:00\ncategories: 最佳实践\ntags: [Java, 源码]\n---\n\n## 1. 原则\n\n\n### 原则1：了解使用\n\n仔细查看使用文档和说明，写较为详细的 demo 程序。\n\n### 原则2：了解全局\n\n了解该产品解决了哪些问题，并了解其周边产品及优缺点。\n\n### 原则3：了解原理\n\n查看其内部架构文档，如果有了周边产品的了解，可以从周边产品推算出其实现基本原理。不过当对其它产品了解比较深刻的时候，这个原则很容易就可以达到了。\n\n<!-- more -->\n\n## 2. 方式\n\n### 单步 Debug\n\n单步 Debug 是比直接阅读源码来得更加直观快速的方式，通过单步 Debug 的主要目的是了解整个源码的大概逻辑和流程。\n\n* 针对应用程序（Tomcat、Hadoop）：找启动脚本 -> 找到 main 入口 -> 开始 Debug\n* 库、框架（FastJSON、Spring）：找到关键类 -> 打断点 -> 查看堆栈\n\n### 「无法 Debug？」\n\n很多时候我们拿到的都是「无法运行」的代码，比如说构建起来很麻烦的源码、只有一个 lib 包。在这个时候只能采取一些其它手段了。\n\n* 在我看来，最有效的方式还是 Debug。如果遇到了比较难构建的源码，那么暂时放弃构建它的想法，可以直接新建一个工程并添加其 Jar 包依赖，写测试代码的方式进行 Debug。至于测试代码，可以从源码的官方文档、或者它的测试用例中找到；\n\n* 如果以上也是比较麻烦的话，那就直接强行裸看代码吧，不过裸看代码也有一些简便的方法让你更好的裸看代码，比如接下来的。\n\n\n### 梳理整个工程的类依赖关系图\n\n以 Intellij IDEA 的为例：\n\n\n* 右键 Java 类\n* 点击 Diagrams\n* 点击 Show Diagrams\n\n\n### 查找方法调用关系\n\n以 Intellij IDEA 的 Mac OS X 的快捷键为例：\n\n* Alt + F7 (Find Usage)\n* Ctrl + Alt + H (Call Hierarchy)\n\n\n### 搜索关键字\n\n这个一般在无头绪的时候才会使用的。比如出现了框架内部的错误，里面包含了特殊关键字，如错误日志或者没有堆栈的方法。\n\n这个在 Intellij IDEA 中比较好用，可以包含依赖的三方库进行全文搜索。快捷键：\n\n* Ctrl + Shift + F (Find in path)\n\n> 搜索三方库需要选择 Custom -> Project and Libraries\n\n如果是 Eclipse 的话，似乎只能是大概知道是哪个 Jar 包之后，解压 source 包再全文搜索吧 = =\n\n## 3. 其他\n\n我好像就上面几个方法，看任何源码（Hadoop、Spring、Netty、公司内部的）都能够很快上手。如果想到啥或者有啥比较典型的实例的话，再补充吧。\n","slug":"2015/09/java-source-code-practice","published":1,"updated":"2015-12-30T14:13:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91e000r3x8fg6cm4qqq"},{"title":"Java 最佳实践","date":"2015-08-28T14:32:00.000Z","_content":"\n* [毕玄的演讲](/upload/java-at-alibaba.pptx)\n","source":"_posts/2015/08/java-practice.md","raw":"title: Java 最佳实践\ndate: 2015-08-28 22:32:00\ncategories: 最佳实践\ntags: [Java]\n---\n\n* [毕玄的演讲](/upload/java-at-alibaba.pptx)\n","slug":"2015/08/java-practice","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91g000w3x8fizaap2wx"},{"title":"GIT 最佳实践","date":"2015-08-28T14:32:00.000Z","_content":"\n以下是我整理的自己使用及见过比较好的 GIT 实践，草草总结如下，有空会详细描述各项内容：\n\n<!--more-->\n\n## 1. Labels\n\n* BUG\n* P0\n* P1\n* P2\n* TIPS\n* TODO\n* 功能Label\n\t* 交互\n\t* 视觉\n\t* ...\n* 子功能Label\n    * 业务1\n    * 业务2\n\t* ...\n\n\n## 2. Branch\n\n* 方式1: [Git版本控制与工作流](http://www.jianshu.com/p/67afe711c731)\n\t* master\n\t* release\n\t* develop\n\t* feature/*\n\t* hotfix\n* 方式2\n\t* master\n\t* name-dev\n\n## 3. Issue\n\n* 命名\n\t* XX1-XX2-Description\n\t* XX1: 功能\n\t* XX2: 子功能\n* Push\n\t* comment 中添加 #{Issue Id}\n\n## 4. Push\n\n* 最小功能提交原则\n\n\n## 5. Milestone\n\n### 常规\n\n* 命名\n\t* v1.0.0\n\t* v1.0.1\n\t* ...\n* 内容\n\t* 基本描述\n\t* 版本负责人\n\n## 6. 其它\n\n* Application Talk\n\t* 针对 Application 的讨论与展望\n\t* 同时也可以发设计文档\n\n## 7. 其它实践\n\n* 可以建立 project-management 分支\n\t* 更多的关注项目管理的内容\n\t\t* 文档\n\t\t* 项目管理 issue\n\t* 如果有些 issue 同时涉及多个不同子工程，可以将这些写在这里面\n","source":"_posts/2015/08/git-practice.md","raw":"title: GIT 最佳实践\ndate: 2015-08-28 22:32:00\ncategories: 最佳实践\ntags: GIT\n---\n\n以下是我整理的自己使用及见过比较好的 GIT 实践，草草总结如下，有空会详细描述各项内容：\n\n<!--more-->\n\n## 1. Labels\n\n* BUG\n* P0\n* P1\n* P2\n* TIPS\n* TODO\n* 功能Label\n\t* 交互\n\t* 视觉\n\t* ...\n* 子功能Label\n    * 业务1\n    * 业务2\n\t* ...\n\n\n## 2. Branch\n\n* 方式1: [Git版本控制与工作流](http://www.jianshu.com/p/67afe711c731)\n\t* master\n\t* release\n\t* develop\n\t* feature/*\n\t* hotfix\n* 方式2\n\t* master\n\t* name-dev\n\n## 3. Issue\n\n* 命名\n\t* XX1-XX2-Description\n\t* XX1: 功能\n\t* XX2: 子功能\n* Push\n\t* comment 中添加 #{Issue Id}\n\n## 4. Push\n\n* 最小功能提交原则\n\n\n## 5. Milestone\n\n### 常规\n\n* 命名\n\t* v1.0.0\n\t* v1.0.1\n\t* ...\n* 内容\n\t* 基本描述\n\t* 版本负责人\n\n## 6. 其它\n\n* Application Talk\n\t* 针对 Application 的讨论与展望\n\t* 同时也可以发设计文档\n\n## 7. 其它实践\n\n* 可以建立 project-management 分支\n\t* 更多的关注项目管理的内容\n\t\t* 文档\n\t\t* 项目管理 issue\n\t* 如果有些 issue 同时涉及多个不同子工程，可以将这些写在这里面\n","slug":"2015/08/git-practice","published":1,"updated":"2015-12-30T12:25:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91i000z3x8fxh74snht"},{"title":"CSS 最佳实践","date":"2015-08-29T16:03:00.000Z","_content":"\n## 1. 善用 伪元素(Pseudo-elements)\n\n### 常用伪元素\n\n* `:before`\n* `:after`\n* `:first-child`\n* `:last-child`\n\n### 实例\n\n* 插入类似列表符号：[JsFiddle](https://jsfiddle.net/pg4kpc3k/)\n\n* 插入 clearfix：[JsFiddle](https://jsfiddle.net/hy4av6eu/)\n\n* first-child & last-child 常用在 ul li 中，用以针对头尾设置不同的样式\n\n\n## 2. 善用属性选择器\n\n有了属性选择器可以设计很多自己的组件，比如按钮分组、进度条之类的。\n\n* [进度条实现 - JsFiddle](https://jsfiddle.net/j94nvngo/)\n\n\n## 3. 待续...\n","source":"_posts/2015/08/css-practice.md","raw":"title: CSS 最佳实践\ndate: 2015-08-30 00:03:00\ncategories: 最佳实践\ntags: [CSS, Frontend]\n---\n\n## 1. 善用 伪元素(Pseudo-elements)\n\n### 常用伪元素\n\n* `:before`\n* `:after`\n* `:first-child`\n* `:last-child`\n\n### 实例\n\n* 插入类似列表符号：[JsFiddle](https://jsfiddle.net/pg4kpc3k/)\n\n* 插入 clearfix：[JsFiddle](https://jsfiddle.net/hy4av6eu/)\n\n* first-child & last-child 常用在 ul li 中，用以针对头尾设置不同的样式\n\n\n## 2. 善用属性选择器\n\n有了属性选择器可以设计很多自己的组件，比如按钮分组、进度条之类的。\n\n* [进度条实现 - JsFiddle](https://jsfiddle.net/j94nvngo/)\n\n\n## 3. 待续...\n","slug":"2015/08/css-practice","published":1,"updated":"2015-12-30T12:24:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91k00133x8fiuoohec4"},{"title":"OOM Killer 的一次问题定位","date":"2015-03-16T14:01:01.000Z","_content":"\n这两天为了节省服务器资源，讲多个不同的 JVM 部署到了同一个 VM 上，想着应该没什么事，大不了处理速度慢一点而已，但是没想到确出现了意想不到的状况：各个 VM 上的 JVM 不约而同的挂了。挂了没事，解决 Bug 嘛，但是问题在于 JVM 是怎么挂掉了就没有搞清楚，也没有特殊日志打印，我花了半天时间定位了问题。\n<!--more-->\n\n### 1. JVM 为什么挂掉了？\n\n正常来说，JVM 挂了要么会生成内存 dump ，要么直接生成 core 文件，我的机器什么都没有产生。于是乎只能借助系统工具了，如下命令能够捕获进程信号：\n\n```\nstrace -e trace=signal -o /home/admin/strace.log -p [PID];\n```\n\n等了几个小时后，有 JVM 挂了，日志输出如下：\n\n```\n+++ killed by SIGKILL +++\n```\n\n[signal - overview of signals](http://man7.org/linux/man-pages/man7/signal.7.html) 这里可以看到，SIGKILL 就是 kill -9，进程不会做任何处理直接退出。看到这个我以为咱们的 OS 部署了监控进程，会用来 kill 掉耗资源的进程，咨询运维人员后，没有这样的程序部署，排除他方的因素，自身程序有问题可能性较大。\n\n从信号上看不出什么端倪，就只能从系统日志上面来找了，通过以下命令发现 JVM 挂掉的原因：\n\n```\ndmesg | egrep -i -B100 'killed process'\n[5673702.665338] [20189]   522 20189     1017       22   2       0             0 sleep\n[5673702.665338] [20308]     0 20308    47967    20414   3       0             0 puppet\n[5673702.665338] [20536]     0 20536    47969    20419   1       0             0 puppet\n[5673702.665338] Out of memory: Kill process 29953 (java) score 431 or sacrifice child\n[5673702.665338] Killed process 29953, UID 500, (java) total-vm:9805316kB, anon-rss:2344496kB, file-rss:128kB\n```\n\n是由于 Out of memory 导致 JVM 被直接 kill 掉，这也是较为常见的 OOM Killer 了，关于 OOM Killer 网上有篇不错的解析文章，摘抄见后文。\n\n> 定位 OOM 具体问题，除了 dump 内存分析之外，还有一些较为简单快捷的方式对整个内存进行一次摸底。\n  pmap -x [PID]: 能查看进程的内存映射;\n  jmap -heap [PID]: 快速查看 JVM 各内存区域的使用情况。</blockquote>\n\n### 2. 理解和配置 Linux 下的 OOM Killer\n\n最近有位 VPS 客户抱怨 MySQL 无缘无故挂掉，还有位客户抱怨 VPS 经常死机，登陆到终端看了一下，都是常见的 Out of memory 问题。这通常是因为某时刻应用程序大量请求内存导致系统内存不足造成的，这通常会触发 Linux 内核里的 Out of Memory (OOM) killer，OOM killer 会杀掉某个进程以腾出内存留给系统用，不致于让系统立刻崩溃。如果检查相关的日志文件（`/var/log/messages`）就会看到下面类似的 `Out of memory: Kill process` 信息：\n\n```\n...\nOut of memory: Kill process 9682 (mysqld) score 9 or sacrifice child\nKilled process 9682, UID 27, (mysqld) total-vm:47388kB, anon-rss:3744kB, file-rss:80kB\nhttpd invoked oom-killer: gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0\nhttpd cpuset=/ mems_allowed=0\nPid: 8911, comm: httpd Not tainted 2.6.32-279.1.1.el6.i686 #1\n...\n21556 total pagecache pages\n21049 pages in swap cache\nSwap cache stats: add 12819103, delete 12798054, find 3188096/4634617\nFree swap  = 0kB\nTotal swap = 524280kB\n131071 pages RAM\n0 pages HighMem\n3673 pages reserved\n67960 pages shared\n124940 pages non-shared\n```\n\nLinux 内核根据应用程序的要求分配内存，通常来说应用程序分配了内存但是并没有实际全部使用，为了提高性能，这部分没用的内存可以留作它用，这部分内存是属于每个进程的，内核直接回收利用的话比较麻烦，所以内核采用一种过度分配内存（over-commit memory）的办法来间接利用这部分 “空闲” 的内存，提高整体内存的使用效率。一般来说这样做没有问题，但当大多数应用程序都消耗完自己的内存的时候麻烦就来了，因为这些应用程序的内存需求加起来超出了物理内存（包括 swap）的容量，内核（OOM killer）必须杀掉一些进程才能腾出空间保障系统正常运行。用银行的例子来讲可能更容易懂一些，部分人取钱的时候银行不怕，银行有足够的存款应付，当全国人民（或者绝大多数）都取钱而且每个人都想把自己钱取完的时候银行的麻烦就来了，银行实际上是没有这么多钱给大家取的。\n\n内核检测到系统内存不足、挑选并杀掉某个进程的过程可以参考内核源代码 [linux/mm/oom_kill.c] (https://github.com/torvalds/linux/blob/master/mm/oom_kill.c)，当系统内存不足的时候，`out_of_memory()` 被触发，然后调用 `select_bad_process()` 选择一个 “bad” 进程杀掉，如何判断和选择一个 “bad” 进程呢，总不能随机选吧？挑选的过程由 `oom_badness()` 决定，挑选的算法和想法都很简单很朴实：最 bad 的那个进程就是那个最占用内存的进程。\n\n``` c\n/**\n * oom_badness - heuristic function to determine which candidate task to kill\n * @p: task struct of which task we should calculate\n * @totalpages: total present RAM allowed for page allocation\n *\n * The heuristic for determining which task to kill is made to be as simple and\n * predictable as possible.  The goal is to return the highest value for the\n * task consuming the most memory to avoid subsequent oom failures.\n */\nunsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,\n\t\t\t  const nodemask_t *nodemask, unsigned long totalpages)\n{\n\tlong points;\n\tlong adj;\n\n\tif (oom_unkillable_task(p, memcg, nodemask))\n\t\treturn 0;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 0;\n\n\tadj = (long)p-&gt;signal-&gt;oom_score_adj;\n\tif (adj == OOM_SCORE_ADJ_MIN) {\n\t\ttask_unlock(p);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * The baseline for the badness score is the proportion of RAM that each\n\t * task's rss, pagetable and swap space use.\n\t */\n\tpoints = get_mm_rss(p-&gt;mm) + p-&gt;mm-&gt;nr_ptes +\n\t\t get_mm_counter(p-&gt;mm, MM_SWAPENTS);\n\ttask_unlock(p);\n\n\t/*\n\t * Root processes get 3% bonus, just like the __vm_enough_memory()\n\t * implementation used by LSMs.\n\t */\n\tif (has_capability_noaudit(p, CAP_SYS_ADMIN))\n\t\tadj -= 30;\n\n\t/* Normalize to oom_score_adj units */\n\tadj *= totalpages / 1000;\n\tpoints += adj;\n\n\t/*\n\t * Never return 0 for an eligible task regardless of the root bonus and\n\t * oom_score_adj (oom_score_adj can't be OOM_SCORE_ADJ_MIN here).\n\t */\n\treturn points &gt; 0 ? points : 1;\n}\n```\n\n上面代码里的注释写的很明白，理解了这个算法我们就理解了为啥 MySQL 躺着也能中枪了，因为它的体积总是最大（一般来说它在系统上占用内存最多），所以如果 Out of Memeory (OOM) 的话总是不幸第一个被 kill 掉。解决这个问题最简单的办法就是增加内存，或者[想办法优化 MySQL 使其占用更少的内存](http://www.vpsee.com/2009/06/64mb-vps-optimize-mysql/)，除了优化 MySQL 外还可以优化系统（[优化 Debian 5](http://www.vpsee.com/2009/06/64mb-vps-optimize-debian5/)，[优化 CentOS 5.x](http://www.vpsee.com/2009/06/128mb-vps-optimize-centos5/)），让系统尽可能使用少的内存以便应用程序（如 MySQL) 能使用更多的内存，还有一个临时的办法就是调整内核参数，让 MySQL 进程不容易被 OOM killer 发现。\n\n#### 2.1 配置 OOM killer\n\n我们可以通过一些内核参数来调整 OOM killer 的行为，避免系统在那里不停的杀进程。比如我们可以在触发 OOM 后立刻触发 kernel panic，kernel panic 10秒后自动重启系统。\n\n```\n# sysctl -w vm.panic_on_oom=1\nvm.panic_on_oom = 1\n\n# sysctl -w kernel.panic=10\nkernel.panic = 10\n\n# echo \"vm.panic_on_oom=1\" &gt;&gt; /etc/sysctl.conf\n# echo \"kernel.panic=10\" &gt;&gt; /etc/sysctl.conf\n```\n\n从上面的 oom_kill.c 代码里可以看到 `oom_badness()` 给每个进程打分，根据 points 的高低来决定杀哪个进程，这个 points 可以根据 adj 调节，root 权限的进程通常被认为很重要，不应该被轻易杀掉，所以打分的时候可以得到 3% 的优惠（adj -= 30; 分数越低越不容易被杀掉）。我们可以在用户空间通过操作每个进程的 oom_adj 内核参数来决定哪些进程不这么容易被 OOM killer 选中杀掉。比如，如果不想 MySQL 进程被轻易杀掉的话可以找到 MySQL 运行的进程号后，调整 oom_score_adj 为 -15（注意 points 越小越不容易被杀）：\n\n```\n# ps aux | grep mysqld\nmysql    2196  1.6  2.1 623800 44876 ?        Ssl  09:42   0:00 /usr/sbin/mysqld\n\n# cat /proc/2196/oom_score_adj\n0\n# echo -15 &gt; /proc/2196/oom_score_adj\n```\n\n当然，如果需要的话可以完全关闭 OOM killer（不推荐用在生产环境）：\n\n```\n# sysctl -w vm.overcommit_memory=2\n\n# echo \"vm.overcommit_memory=2\" &gt;&gt; /etc/sysctl.conf\n```\n\n#### 2.2 找出最有可能被 OOM Killer 杀掉的进程\n\n我们知道了在用户空间可以通过操作每个进程的 oom_adj 内核参数来调整进程的分数，这个分数也可以通过 oom_score 这个内核参数看到，比如查看进程号为981的 omm_score，这个分数被上面提到的 omm_score_adj 参数调整后（－15），就变成了3：\n\n```\n# cat /proc/981/oom_score\n18\n\n# echo -15 &gt; /proc/981/oom_score_adj\n# cat /proc/981/oom_score\n3\n```\n\n下面这个 bash 脚本可用来打印当前系统上 oom_score 分数最高（最容易被 OOM Killer 杀掉）的进程：\n\n```\n# vi oomscore.sh\n#!/bin/bash\nfor proc in $(find /proc -maxdepth 1 -regex '/proc/[0-9]+'); do\n    printf \"%2d %5d %s\\n\" \\\n        \"$(cat $proc/oom_score)\" \\\n        \"$(basename $proc)\" \\\n        \"$(cat $proc/cmdline | tr '\\0' ' ' | head -c 50)\"\ndone 2&gt;/dev/null | sort -nr | head -n 10\n\n# chmod +x oomscore.sh\n# ./oomscore.sh\n18   981 /usr/sbin/mysqld\n 4 31359 -bash\n 4 31056 -bash\n 1 31358 sshd: root@pts/6\n 1 31244 sshd: vpsee [priv]\n 1 31159 -bash\n 1 31158 sudo -i\n 1 31055 sshd: root@pts/3\n 1 30912 sshd: vpsee [priv]\n 1 29547 /usr/sbin/sshd -D\n```\n\n> 原文链接：http://www.vpsee.com/2013/10/how-to-configure-the-linux-oom-killer/\n","source":"_posts/2015/03/oom-killer-1.md","raw":"title: OOM Killer 的一次问题定位\ndate: 2015-03-16 22:01:01\ncategories: 最佳实践\ntags: [Java, JVM, OOM Killer]\n---\n\n这两天为了节省服务器资源，讲多个不同的 JVM 部署到了同一个 VM 上，想着应该没什么事，大不了处理速度慢一点而已，但是没想到确出现了意想不到的状况：各个 VM 上的 JVM 不约而同的挂了。挂了没事，解决 Bug 嘛，但是问题在于 JVM 是怎么挂掉了就没有搞清楚，也没有特殊日志打印，我花了半天时间定位了问题。\n<!--more-->\n\n### 1. JVM 为什么挂掉了？\n\n正常来说，JVM 挂了要么会生成内存 dump ，要么直接生成 core 文件，我的机器什么都没有产生。于是乎只能借助系统工具了，如下命令能够捕获进程信号：\n\n```\nstrace -e trace=signal -o /home/admin/strace.log -p [PID];\n```\n\n等了几个小时后，有 JVM 挂了，日志输出如下：\n\n```\n+++ killed by SIGKILL +++\n```\n\n[signal - overview of signals](http://man7.org/linux/man-pages/man7/signal.7.html) 这里可以看到，SIGKILL 就是 kill -9，进程不会做任何处理直接退出。看到这个我以为咱们的 OS 部署了监控进程，会用来 kill 掉耗资源的进程，咨询运维人员后，没有这样的程序部署，排除他方的因素，自身程序有问题可能性较大。\n\n从信号上看不出什么端倪，就只能从系统日志上面来找了，通过以下命令发现 JVM 挂掉的原因：\n\n```\ndmesg | egrep -i -B100 'killed process'\n[5673702.665338] [20189]   522 20189     1017       22   2       0             0 sleep\n[5673702.665338] [20308]     0 20308    47967    20414   3       0             0 puppet\n[5673702.665338] [20536]     0 20536    47969    20419   1       0             0 puppet\n[5673702.665338] Out of memory: Kill process 29953 (java) score 431 or sacrifice child\n[5673702.665338] Killed process 29953, UID 500, (java) total-vm:9805316kB, anon-rss:2344496kB, file-rss:128kB\n```\n\n是由于 Out of memory 导致 JVM 被直接 kill 掉，这也是较为常见的 OOM Killer 了，关于 OOM Killer 网上有篇不错的解析文章，摘抄见后文。\n\n> 定位 OOM 具体问题，除了 dump 内存分析之外，还有一些较为简单快捷的方式对整个内存进行一次摸底。\n  pmap -x [PID]: 能查看进程的内存映射;\n  jmap -heap [PID]: 快速查看 JVM 各内存区域的使用情况。</blockquote>\n\n### 2. 理解和配置 Linux 下的 OOM Killer\n\n最近有位 VPS 客户抱怨 MySQL 无缘无故挂掉，还有位客户抱怨 VPS 经常死机，登陆到终端看了一下，都是常见的 Out of memory 问题。这通常是因为某时刻应用程序大量请求内存导致系统内存不足造成的，这通常会触发 Linux 内核里的 Out of Memory (OOM) killer，OOM killer 会杀掉某个进程以腾出内存留给系统用，不致于让系统立刻崩溃。如果检查相关的日志文件（`/var/log/messages`）就会看到下面类似的 `Out of memory: Kill process` 信息：\n\n```\n...\nOut of memory: Kill process 9682 (mysqld) score 9 or sacrifice child\nKilled process 9682, UID 27, (mysqld) total-vm:47388kB, anon-rss:3744kB, file-rss:80kB\nhttpd invoked oom-killer: gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0\nhttpd cpuset=/ mems_allowed=0\nPid: 8911, comm: httpd Not tainted 2.6.32-279.1.1.el6.i686 #1\n...\n21556 total pagecache pages\n21049 pages in swap cache\nSwap cache stats: add 12819103, delete 12798054, find 3188096/4634617\nFree swap  = 0kB\nTotal swap = 524280kB\n131071 pages RAM\n0 pages HighMem\n3673 pages reserved\n67960 pages shared\n124940 pages non-shared\n```\n\nLinux 内核根据应用程序的要求分配内存，通常来说应用程序分配了内存但是并没有实际全部使用，为了提高性能，这部分没用的内存可以留作它用，这部分内存是属于每个进程的，内核直接回收利用的话比较麻烦，所以内核采用一种过度分配内存（over-commit memory）的办法来间接利用这部分 “空闲” 的内存，提高整体内存的使用效率。一般来说这样做没有问题，但当大多数应用程序都消耗完自己的内存的时候麻烦就来了，因为这些应用程序的内存需求加起来超出了物理内存（包括 swap）的容量，内核（OOM killer）必须杀掉一些进程才能腾出空间保障系统正常运行。用银行的例子来讲可能更容易懂一些，部分人取钱的时候银行不怕，银行有足够的存款应付，当全国人民（或者绝大多数）都取钱而且每个人都想把自己钱取完的时候银行的麻烦就来了，银行实际上是没有这么多钱给大家取的。\n\n内核检测到系统内存不足、挑选并杀掉某个进程的过程可以参考内核源代码 [linux/mm/oom_kill.c] (https://github.com/torvalds/linux/blob/master/mm/oom_kill.c)，当系统内存不足的时候，`out_of_memory()` 被触发，然后调用 `select_bad_process()` 选择一个 “bad” 进程杀掉，如何判断和选择一个 “bad” 进程呢，总不能随机选吧？挑选的过程由 `oom_badness()` 决定，挑选的算法和想法都很简单很朴实：最 bad 的那个进程就是那个最占用内存的进程。\n\n``` c\n/**\n * oom_badness - heuristic function to determine which candidate task to kill\n * @p: task struct of which task we should calculate\n * @totalpages: total present RAM allowed for page allocation\n *\n * The heuristic for determining which task to kill is made to be as simple and\n * predictable as possible.  The goal is to return the highest value for the\n * task consuming the most memory to avoid subsequent oom failures.\n */\nunsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,\n\t\t\t  const nodemask_t *nodemask, unsigned long totalpages)\n{\n\tlong points;\n\tlong adj;\n\n\tif (oom_unkillable_task(p, memcg, nodemask))\n\t\treturn 0;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 0;\n\n\tadj = (long)p-&gt;signal-&gt;oom_score_adj;\n\tif (adj == OOM_SCORE_ADJ_MIN) {\n\t\ttask_unlock(p);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * The baseline for the badness score is the proportion of RAM that each\n\t * task's rss, pagetable and swap space use.\n\t */\n\tpoints = get_mm_rss(p-&gt;mm) + p-&gt;mm-&gt;nr_ptes +\n\t\t get_mm_counter(p-&gt;mm, MM_SWAPENTS);\n\ttask_unlock(p);\n\n\t/*\n\t * Root processes get 3% bonus, just like the __vm_enough_memory()\n\t * implementation used by LSMs.\n\t */\n\tif (has_capability_noaudit(p, CAP_SYS_ADMIN))\n\t\tadj -= 30;\n\n\t/* Normalize to oom_score_adj units */\n\tadj *= totalpages / 1000;\n\tpoints += adj;\n\n\t/*\n\t * Never return 0 for an eligible task regardless of the root bonus and\n\t * oom_score_adj (oom_score_adj can't be OOM_SCORE_ADJ_MIN here).\n\t */\n\treturn points &gt; 0 ? points : 1;\n}\n```\n\n上面代码里的注释写的很明白，理解了这个算法我们就理解了为啥 MySQL 躺着也能中枪了，因为它的体积总是最大（一般来说它在系统上占用内存最多），所以如果 Out of Memeory (OOM) 的话总是不幸第一个被 kill 掉。解决这个问题最简单的办法就是增加内存，或者[想办法优化 MySQL 使其占用更少的内存](http://www.vpsee.com/2009/06/64mb-vps-optimize-mysql/)，除了优化 MySQL 外还可以优化系统（[优化 Debian 5](http://www.vpsee.com/2009/06/64mb-vps-optimize-debian5/)，[优化 CentOS 5.x](http://www.vpsee.com/2009/06/128mb-vps-optimize-centos5/)），让系统尽可能使用少的内存以便应用程序（如 MySQL) 能使用更多的内存，还有一个临时的办法就是调整内核参数，让 MySQL 进程不容易被 OOM killer 发现。\n\n#### 2.1 配置 OOM killer\n\n我们可以通过一些内核参数来调整 OOM killer 的行为，避免系统在那里不停的杀进程。比如我们可以在触发 OOM 后立刻触发 kernel panic，kernel panic 10秒后自动重启系统。\n\n```\n# sysctl -w vm.panic_on_oom=1\nvm.panic_on_oom = 1\n\n# sysctl -w kernel.panic=10\nkernel.panic = 10\n\n# echo \"vm.panic_on_oom=1\" &gt;&gt; /etc/sysctl.conf\n# echo \"kernel.panic=10\" &gt;&gt; /etc/sysctl.conf\n```\n\n从上面的 oom_kill.c 代码里可以看到 `oom_badness()` 给每个进程打分，根据 points 的高低来决定杀哪个进程，这个 points 可以根据 adj 调节，root 权限的进程通常被认为很重要，不应该被轻易杀掉，所以打分的时候可以得到 3% 的优惠（adj -= 30; 分数越低越不容易被杀掉）。我们可以在用户空间通过操作每个进程的 oom_adj 内核参数来决定哪些进程不这么容易被 OOM killer 选中杀掉。比如，如果不想 MySQL 进程被轻易杀掉的话可以找到 MySQL 运行的进程号后，调整 oom_score_adj 为 -15（注意 points 越小越不容易被杀）：\n\n```\n# ps aux | grep mysqld\nmysql    2196  1.6  2.1 623800 44876 ?        Ssl  09:42   0:00 /usr/sbin/mysqld\n\n# cat /proc/2196/oom_score_adj\n0\n# echo -15 &gt; /proc/2196/oom_score_adj\n```\n\n当然，如果需要的话可以完全关闭 OOM killer（不推荐用在生产环境）：\n\n```\n# sysctl -w vm.overcommit_memory=2\n\n# echo \"vm.overcommit_memory=2\" &gt;&gt; /etc/sysctl.conf\n```\n\n#### 2.2 找出最有可能被 OOM Killer 杀掉的进程\n\n我们知道了在用户空间可以通过操作每个进程的 oom_adj 内核参数来调整进程的分数，这个分数也可以通过 oom_score 这个内核参数看到，比如查看进程号为981的 omm_score，这个分数被上面提到的 omm_score_adj 参数调整后（－15），就变成了3：\n\n```\n# cat /proc/981/oom_score\n18\n\n# echo -15 &gt; /proc/981/oom_score_adj\n# cat /proc/981/oom_score\n3\n```\n\n下面这个 bash 脚本可用来打印当前系统上 oom_score 分数最高（最容易被 OOM Killer 杀掉）的进程：\n\n```\n# vi oomscore.sh\n#!/bin/bash\nfor proc in $(find /proc -maxdepth 1 -regex '/proc/[0-9]+'); do\n    printf \"%2d %5d %s\\n\" \\\n        \"$(cat $proc/oom_score)\" \\\n        \"$(basename $proc)\" \\\n        \"$(cat $proc/cmdline | tr '\\0' ' ' | head -c 50)\"\ndone 2&gt;/dev/null | sort -nr | head -n 10\n\n# chmod +x oomscore.sh\n# ./oomscore.sh\n18   981 /usr/sbin/mysqld\n 4 31359 -bash\n 4 31056 -bash\n 1 31358 sshd: root@pts/6\n 1 31244 sshd: vpsee [priv]\n 1 31159 -bash\n 1 31158 sudo -i\n 1 31055 sshd: root@pts/3\n 1 30912 sshd: vpsee [priv]\n 1 29547 /usr/sbin/sshd -D\n```\n\n> 原文链接：http://www.vpsee.com/2013/10/how-to-configure-the-linux-oom-killer/\n","slug":"2015/03/oom-killer-1","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91m00193x8f7xbhre62"},{"title":"我现在就要答案 《REMOTE》","date":"2014-10-21T15:23:00.000Z","_content":"\n人人都在办公室坐着的时候，你很容易就会养成一种坏习惯：不管大事小事，不管什么时间，也不管是否会打断对方的工作，只要想起来，就会打扰对方。为何有那么多人在传统的办公室里工作效率低下，这就是关键原因。习惯了这种工作模式之后，你很难设想一个无法立即得到反馈的世界是什么模样——无论事情有多小。可是，这样的世界是存在的，而且适宜人类居住。<!--more-->\n\n首先，你需要认识到，并不是每个问题都需要立即得到解答。没有什么能比拿着一个无须立即得到答案的问题去打扰别人更傲慢的行为了。这意味着你要明白，并不是所有的事情都同样重要。\n\n一旦明白了这个道理，你就踏上醒悟和高效之路了。可以过几个小时再得到回答的问题，可以发邮件解决。几分钟内需要知道答案的问题，可以用即时消息。至于那些如天塌下来一般、不能等的急事儿，你可以使用一种老式的发明：电话。\n\n想清楚这些，你就会很快意识到，你的问题中有80%都是不着急的，而且发邮件往往比走到某人办公室前更合适。更妙的是，你得到的回答是写下来的，还能留着日后备查。\n\n接下来的50%的问题可以用即时通信工具来解决，绝大多数人懒得打那么多字，所以大家基本上都直奔主题。本来有可能耗时15分钟的工作中断，如今变成了3分钟的速战速决。\n\n最后余下5%的问题可以打电话。的确，打电话的时候看不见对方的身体语言，可以，除非你是要炒掉谁，或是主持一个棘手的面试，否则身体语言的作用没你想的那么大。\n\n想要戒掉你和其他人的“立即回复”上瘾症，肯定会遇到点阻碍。最初几天，你得头脑处在适应阶段，还在判断什么问题改用什么媒介，这时候你会有点泄气。你还要抗拒的一个诱惑是，对你选择的沟通媒介有不切实际的要求。要是别人10分钟之内没回复你的邮件，你就会冒火，那么你没法用电子邮件来处理80%的问题。\n\n然而，一旦从“立即回复”上瘾症中解脱出来，你就会对之前的工作方式感到惊讶无比：在连续不断的干扰下，你是怎么干活的啊。放开手，别抓狂，等到对方准备好协助你的时候，回答自然会朝你走来——这里面几乎蕴含着一种禅意。运用这种镇定气度，更加高效的工作吧。\n\n> 摘抄自：《REMOTE》\n","source":"_posts/2014/10/i-need-an-answer-now-from-remote.md","raw":"title: 我现在就要答案 《REMOTE》\ndate: 2014-10-21 23:23:00\ncategories: 生活点滴\ntags: [Remote, 书摘, 生活]\n---\n\n人人都在办公室坐着的时候，你很容易就会养成一种坏习惯：不管大事小事，不管什么时间，也不管是否会打断对方的工作，只要想起来，就会打扰对方。为何有那么多人在传统的办公室里工作效率低下，这就是关键原因。习惯了这种工作模式之后，你很难设想一个无法立即得到反馈的世界是什么模样——无论事情有多小。可是，这样的世界是存在的，而且适宜人类居住。<!--more-->\n\n首先，你需要认识到，并不是每个问题都需要立即得到解答。没有什么能比拿着一个无须立即得到答案的问题去打扰别人更傲慢的行为了。这意味着你要明白，并不是所有的事情都同样重要。\n\n一旦明白了这个道理，你就踏上醒悟和高效之路了。可以过几个小时再得到回答的问题，可以发邮件解决。几分钟内需要知道答案的问题，可以用即时消息。至于那些如天塌下来一般、不能等的急事儿，你可以使用一种老式的发明：电话。\n\n想清楚这些，你就会很快意识到，你的问题中有80%都是不着急的，而且发邮件往往比走到某人办公室前更合适。更妙的是，你得到的回答是写下来的，还能留着日后备查。\n\n接下来的50%的问题可以用即时通信工具来解决，绝大多数人懒得打那么多字，所以大家基本上都直奔主题。本来有可能耗时15分钟的工作中断，如今变成了3分钟的速战速决。\n\n最后余下5%的问题可以打电话。的确，打电话的时候看不见对方的身体语言，可以，除非你是要炒掉谁，或是主持一个棘手的面试，否则身体语言的作用没你想的那么大。\n\n想要戒掉你和其他人的“立即回复”上瘾症，肯定会遇到点阻碍。最初几天，你得头脑处在适应阶段，还在判断什么问题改用什么媒介，这时候你会有点泄气。你还要抗拒的一个诱惑是，对你选择的沟通媒介有不切实际的要求。要是别人10分钟之内没回复你的邮件，你就会冒火，那么你没法用电子邮件来处理80%的问题。\n\n然而，一旦从“立即回复”上瘾症中解脱出来，你就会对之前的工作方式感到惊讶无比：在连续不断的干扰下，你是怎么干活的啊。放开手，别抓狂，等到对方准备好协助你的时候，回答自然会朝你走来——这里面几乎蕴含着一种禅意。运用这种镇定气度，更加高效的工作吧。\n\n> 摘抄自：《REMOTE》\n","slug":"2014/10/i-need-an-answer-now-from-remote","published":1,"updated":"2015-12-30T14:34:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91n001g3x8fc6uue4jh"},{"title":"netty-mina深入学习与对比（二）","date":"2014-05-17T14:03:00.000Z","_content":"\n上文讲了对netty-mina的线程模型以及任务调度粒度的理解，这篇则主要是讲nio编程中的注意事项，netty-mina的对这些注意事项的实现方式的差异，以及业务层会如何处理这些注意事项。\n\n<!--more-->\n\n### 1. 数据是如何write出去的\njava nio如果是non-blocking的话，在每次write(bytes[N])的时候，并不会将N字节全部write出去，每次write仅一部分（具体大小和`tcp_write_buffer`有关）。那么，mina和netty是怎么处理这种情况的呢？\n\n#### 1.1 代码\n\n* `mina-1.1.7`: SocketIoProcessor.doFlush\n* `mina-2.0.4`: AbstractPollingIoProcessor.flushNow\n* `mina-3.0.0.M3-SNAPSHOT`: AbstractNioSession.processWrite\n* `netty-3.5.8.Final`: AbstractNioWorker.write0\n* `netty-4.0.6.Final`: AbstractNioByteChannel.doWrite\n\n#### 1.2 分析\n\nmina1、2，netty3的方式基本一致。 在发送端每个session均有一个writeBufferQueue，有这样一个队列，可以保证写入与写出均有序。在真正write时，大致逻辑均是一一将队列中的writeBuffer取出，写入socket。但有一些不同的是，mina1是每次peek一次，当该buffer全部写出之后再poll（mina3也是这种机制）；而mina2、netty3则是直接poll第一个，将其存为currentWriteRequest，直到currentWriteRequest全部写出之后，才会再poll下一个。这样的做法是为了省几次peek的时间么？\n\n同时mina、netty在write时，有一种spin write的机制，即循环write多次。mina1的spin write count为256，写死在代码里了，表示256有点大；mina2这个机制废除但代码保留；netty3则可以配置，默认为16。netty在这里略胜一筹！\n\nnetty4与netty3的机制差不多，但是netty4为这个事情特意写了一个ChannelOutboundBuffer类，输出队列写在了该类的flushed:Object[]成员中，但表示ChannelOutboundBuffer这个类的代码有点长，就暂不深究了。\n\n### 2. 数据是如何read进来的\n如第三段内容，每次write只是输出了一部分数据，read同理，也有可能只会读入部分数据，这样就是导致读入的数据是残缺的。而mina和netty默认不会理会这种由于nio导致的数据分片，需要由业务层自己额外做配置或者处理。\n\n#### 2.1 代码\n\n* `nfs-rpc`: ProtocolUtils.decode\n* `mina-1.1.7`: SocketIoProcessor.read, CumulativeProtocolDecoder.decode\n* `mina-2.0.4`: AbstractPollingIoProcessor.read, CumulativeProtocolDecoder.decode\n* `mina-3.0.0.M3-SNAPSHOT`: NioSelectorLoop.readBuffer\n* `netty-3.5.8.Final`: NioWorker.read, FrameDecoder\n* `netty-4.0.6.Fianl`: AbstractNioByteChannel$NioByteUnsafe.read\n\n#### 2.2 业务层处理\n\nnfs-rpc在协议反序列化的过程中，就会考虑这个的问题，依次读入每个字节，当发现当前字节或者剩余字节数不够时，会将buf的readerIndex设置为初始状态。具体的实现，有兴趣的同学可以学习`nfs-rpc：ProtocolUtils.decode`\n\nnfs-rpc在decode时，出现错误就会将buf的readerIndex设为0，把readerIndex设置为0就必须要有个前提假设：每次decode时buf是同一个，即该buf是复用的。那么，具体情况是怎样呢？\n\n#### 2.3 框架层处理\n\n我看读mina与netty这块的代码，发现主要演进与不同的点在两个地方：读buffer的创建与数据分片的处理方式。\n\n**mina:**\n\nmina1、2的读buffer创建方式比较土，在每次read之前，会重新allocate一个新的buf对象，该buf对象的大小是根据读入数据大小动态调整。当本次读入数据等于该buf大小，下一次allocate的buf对象大小会翻倍；当本次读入数据不足该buf大小的二分之一，下一次allocate的buf对象同样会缩小至一半。需要注意的是，\\*2与/2的代码都可以用位运算，但是mina1竟没用位运算，有意思。\n\nmina1、2处理数据分片可以继承CumulativeProtocolDecoder，该decoder会在session中存入(BUFFER, cumulativeBuffer)。decode过程为：1）先将message追加至cumulativeBuffer；2）调用具体的decode逻辑；3）判断cumulativeBuffer.hasRemaining()，为true则压缩cumulativeBuffer，为false则直接删除(BUFFER, cumulativeBuffer)。实现业务的decode逻辑可以参考nfs-rpc中MinaProtocolDecoder的代码。\n\nmina3在处理读buffer的创建与数据分片比较巧妙，它所有的读buffer共用一个buffer对象（默认64kb），每次均会将读入的数据追加至该buffer中，这样即省去了buffer的创建与销毁事件，也省去了cumulativeDecoder的处理逻辑，让代码很清爽啊！\n\n**netty:**\n\nnetty3在读buffer创建部分的代码还是挺有意思的，首先，它创建了一个SocketReceiveBufferAllocator的allocate对象，名字为recvBufferPool，但是里面代码完全和pool扯不上关系；其次，它每次创建buffer也会动态修改初始大小的机制，它设计了232个大小档位，最大值为Integer.MAX_VALUE，没有具体考究，这种实现方式似乎比每次大小翻倍优雅一点，具体代码可以参考：`AdaptiveReceiveBufferSizePredictor`。\n\n对应mina的CumulativeProtocolDecoder类，在netty中则是FrameDecoder和ReplayingDecoder，没深入只是大致扫了下代码，原理基本一致。BTW，ReplayingDecoder似乎挺强大的，有兴趣的可以看看这两篇：\n\n> [High speed custom codecs with ReplayingDecoder](http://biasedbit.com/netty-tutorial-replaying-decoder)\n> [An enhanced version of ReplayingDecoder for Netty](http://biasedbit.com/an-enhanced-version-of-replayingdecoder-for-netty/)\n\nnetty4在读buffer创建部分机制与netty3大同小异，不过由于netty有了ByteBufAllocator的概念，要想每次不重新创建销毁buffer的话，可以采用PooledByteBufAllocator。\n\n在处理分片上，netty4抽象出了Message这样的概念，我的理解就是，一个Message就是业务可读的数据，转换Message的抽象类：ByteToMessageDecoder，当然也有netty3中的ReplayingDecoder，继承自ByteToMessageDecoder，具体可以研究代码。\n\n### 3. ByteBuffer设计的差异\n\n#### 3.1 自建buffer的原因\n\n**mina:**\n\n需要说明的是，只有mina1、2才有自己的buffer类，mina3内部只用nio的原生ByteBuffer类（提供了一个组合buffer的代理类-IoBuffer）。mina1、2自建buffer的原因如下：\n\n* It doesn’t provide useful getters and putters such as fill,get/putString, and get/putAsciiInt()enough.\n* It is difficult to write variable-length data due to its fixed capacity\n\n第一条比较好理解，即提供了更为方便的方法用以操作buffer。第二条则是觉得nio的ByteBuffer是定长的，无法自动扩容或者缩容，所以提供了自动扩/缩容的方法：IoBuffer.setAutoExpand, IoBuffer.setAutoShrink。但是扩/缩容的实现，也是基于nio的ByteBuffer，重新ByteBuffer.allocate(capacity)，再把原有的数据拷贝过去。\n\n**netty:**\n\n在我前面的博文（[Netty 4.x学习笔记 – ByteBuf](http://hongweiyi.com/2014/01/netty-4-x-bytebuf/)）我已经提到这些原因：\n\n* 需要的话，可以自定义buffer类型\n* 通过组合buffer类型，可实现透明的zero-copy\n* 提供动态的buffer类型，如StringBuffer一样(扩容方式也是每次double)，容量是按需扩展\n* 无需调用flip()方法\n* 常常「often」比ByteBuffer快\n\n以上理由来自netty3的API文档：[Package org.jboss.netty.buffer](http://docs.jboss.org/netty/3.2/api/org/jboss/netty/buffer/package-summary.html)，netty4没见到官方的说法，但是我觉得还得加上一个更为重要也是最为重要的理由，就是可以实现buffer池化管理。\n\n#### 3.2 实现的差异\n\n**mina:**\n\nmina的实现较为基础，仅仅只是在ByteBuffer上的一些简单封装。\n\n**netty:**\n\nnetty3与netty4的实现大致相同（ChannlBuffer -&gt; ByteBuf），具体可以参见：[Netty 4.x学习笔记 – ByteBuf](http://hongweiyi.com/2014/01/netty-4-x-bytebuf/)，netty4实现了PooledByteBufAllocator，传闻是可以大大减少GC的压力，但是官方不保证没有内存泄露，我自己压测中也出现了内存泄露的警告，建议生产中谨慎使用该功能。\n\n> netty5.x有一个更为高级的buffer泄露跟踪机制，PooledByteBufAllocator也已经默认开启，有机会可以尝试使用一下。\n","source":"_posts/2014/05/netty-mina-in-depth-2.md","raw":"title: netty-mina深入学习与对比（二）\ndate: 2014-05-17 22:03:00\ncategories: 技术分享\ntags: [Java, Mina, Netty]\n\n---\n\n上文讲了对netty-mina的线程模型以及任务调度粒度的理解，这篇则主要是讲nio编程中的注意事项，netty-mina的对这些注意事项的实现方式的差异，以及业务层会如何处理这些注意事项。\n\n<!--more-->\n\n### 1. 数据是如何write出去的\njava nio如果是non-blocking的话，在每次write(bytes[N])的时候，并不会将N字节全部write出去，每次write仅一部分（具体大小和`tcp_write_buffer`有关）。那么，mina和netty是怎么处理这种情况的呢？\n\n#### 1.1 代码\n\n* `mina-1.1.7`: SocketIoProcessor.doFlush\n* `mina-2.0.4`: AbstractPollingIoProcessor.flushNow\n* `mina-3.0.0.M3-SNAPSHOT`: AbstractNioSession.processWrite\n* `netty-3.5.8.Final`: AbstractNioWorker.write0\n* `netty-4.0.6.Final`: AbstractNioByteChannel.doWrite\n\n#### 1.2 分析\n\nmina1、2，netty3的方式基本一致。 在发送端每个session均有一个writeBufferQueue，有这样一个队列，可以保证写入与写出均有序。在真正write时，大致逻辑均是一一将队列中的writeBuffer取出，写入socket。但有一些不同的是，mina1是每次peek一次，当该buffer全部写出之后再poll（mina3也是这种机制）；而mina2、netty3则是直接poll第一个，将其存为currentWriteRequest，直到currentWriteRequest全部写出之后，才会再poll下一个。这样的做法是为了省几次peek的时间么？\n\n同时mina、netty在write时，有一种spin write的机制，即循环write多次。mina1的spin write count为256，写死在代码里了，表示256有点大；mina2这个机制废除但代码保留；netty3则可以配置，默认为16。netty在这里略胜一筹！\n\nnetty4与netty3的机制差不多，但是netty4为这个事情特意写了一个ChannelOutboundBuffer类，输出队列写在了该类的flushed:Object[]成员中，但表示ChannelOutboundBuffer这个类的代码有点长，就暂不深究了。\n\n### 2. 数据是如何read进来的\n如第三段内容，每次write只是输出了一部分数据，read同理，也有可能只会读入部分数据，这样就是导致读入的数据是残缺的。而mina和netty默认不会理会这种由于nio导致的数据分片，需要由业务层自己额外做配置或者处理。\n\n#### 2.1 代码\n\n* `nfs-rpc`: ProtocolUtils.decode\n* `mina-1.1.7`: SocketIoProcessor.read, CumulativeProtocolDecoder.decode\n* `mina-2.0.4`: AbstractPollingIoProcessor.read, CumulativeProtocolDecoder.decode\n* `mina-3.0.0.M3-SNAPSHOT`: NioSelectorLoop.readBuffer\n* `netty-3.5.8.Final`: NioWorker.read, FrameDecoder\n* `netty-4.0.6.Fianl`: AbstractNioByteChannel$NioByteUnsafe.read\n\n#### 2.2 业务层处理\n\nnfs-rpc在协议反序列化的过程中，就会考虑这个的问题，依次读入每个字节，当发现当前字节或者剩余字节数不够时，会将buf的readerIndex设置为初始状态。具体的实现，有兴趣的同学可以学习`nfs-rpc：ProtocolUtils.decode`\n\nnfs-rpc在decode时，出现错误就会将buf的readerIndex设为0，把readerIndex设置为0就必须要有个前提假设：每次decode时buf是同一个，即该buf是复用的。那么，具体情况是怎样呢？\n\n#### 2.3 框架层处理\n\n我看读mina与netty这块的代码，发现主要演进与不同的点在两个地方：读buffer的创建与数据分片的处理方式。\n\n**mina:**\n\nmina1、2的读buffer创建方式比较土，在每次read之前，会重新allocate一个新的buf对象，该buf对象的大小是根据读入数据大小动态调整。当本次读入数据等于该buf大小，下一次allocate的buf对象大小会翻倍；当本次读入数据不足该buf大小的二分之一，下一次allocate的buf对象同样会缩小至一半。需要注意的是，\\*2与/2的代码都可以用位运算，但是mina1竟没用位运算，有意思。\n\nmina1、2处理数据分片可以继承CumulativeProtocolDecoder，该decoder会在session中存入(BUFFER, cumulativeBuffer)。decode过程为：1）先将message追加至cumulativeBuffer；2）调用具体的decode逻辑；3）判断cumulativeBuffer.hasRemaining()，为true则压缩cumulativeBuffer，为false则直接删除(BUFFER, cumulativeBuffer)。实现业务的decode逻辑可以参考nfs-rpc中MinaProtocolDecoder的代码。\n\nmina3在处理读buffer的创建与数据分片比较巧妙，它所有的读buffer共用一个buffer对象（默认64kb），每次均会将读入的数据追加至该buffer中，这样即省去了buffer的创建与销毁事件，也省去了cumulativeDecoder的处理逻辑，让代码很清爽啊！\n\n**netty:**\n\nnetty3在读buffer创建部分的代码还是挺有意思的，首先，它创建了一个SocketReceiveBufferAllocator的allocate对象，名字为recvBufferPool，但是里面代码完全和pool扯不上关系；其次，它每次创建buffer也会动态修改初始大小的机制，它设计了232个大小档位，最大值为Integer.MAX_VALUE，没有具体考究，这种实现方式似乎比每次大小翻倍优雅一点，具体代码可以参考：`AdaptiveReceiveBufferSizePredictor`。\n\n对应mina的CumulativeProtocolDecoder类，在netty中则是FrameDecoder和ReplayingDecoder，没深入只是大致扫了下代码，原理基本一致。BTW，ReplayingDecoder似乎挺强大的，有兴趣的可以看看这两篇：\n\n> [High speed custom codecs with ReplayingDecoder](http://biasedbit.com/netty-tutorial-replaying-decoder)\n> [An enhanced version of ReplayingDecoder for Netty](http://biasedbit.com/an-enhanced-version-of-replayingdecoder-for-netty/)\n\nnetty4在读buffer创建部分机制与netty3大同小异，不过由于netty有了ByteBufAllocator的概念，要想每次不重新创建销毁buffer的话，可以采用PooledByteBufAllocator。\n\n在处理分片上，netty4抽象出了Message这样的概念，我的理解就是，一个Message就是业务可读的数据，转换Message的抽象类：ByteToMessageDecoder，当然也有netty3中的ReplayingDecoder，继承自ByteToMessageDecoder，具体可以研究代码。\n\n### 3. ByteBuffer设计的差异\n\n#### 3.1 自建buffer的原因\n\n**mina:**\n\n需要说明的是，只有mina1、2才有自己的buffer类，mina3内部只用nio的原生ByteBuffer类（提供了一个组合buffer的代理类-IoBuffer）。mina1、2自建buffer的原因如下：\n\n* It doesn’t provide useful getters and putters such as fill,get/putString, and get/putAsciiInt()enough.\n* It is difficult to write variable-length data due to its fixed capacity\n\n第一条比较好理解，即提供了更为方便的方法用以操作buffer。第二条则是觉得nio的ByteBuffer是定长的，无法自动扩容或者缩容，所以提供了自动扩/缩容的方法：IoBuffer.setAutoExpand, IoBuffer.setAutoShrink。但是扩/缩容的实现，也是基于nio的ByteBuffer，重新ByteBuffer.allocate(capacity)，再把原有的数据拷贝过去。\n\n**netty:**\n\n在我前面的博文（[Netty 4.x学习笔记 – ByteBuf](http://hongweiyi.com/2014/01/netty-4-x-bytebuf/)）我已经提到这些原因：\n\n* 需要的话，可以自定义buffer类型\n* 通过组合buffer类型，可实现透明的zero-copy\n* 提供动态的buffer类型，如StringBuffer一样(扩容方式也是每次double)，容量是按需扩展\n* 无需调用flip()方法\n* 常常「often」比ByteBuffer快\n\n以上理由来自netty3的API文档：[Package org.jboss.netty.buffer](http://docs.jboss.org/netty/3.2/api/org/jboss/netty/buffer/package-summary.html)，netty4没见到官方的说法，但是我觉得还得加上一个更为重要也是最为重要的理由，就是可以实现buffer池化管理。\n\n#### 3.2 实现的差异\n\n**mina:**\n\nmina的实现较为基础，仅仅只是在ByteBuffer上的一些简单封装。\n\n**netty:**\n\nnetty3与netty4的实现大致相同（ChannlBuffer -&gt; ByteBuf），具体可以参见：[Netty 4.x学习笔记 – ByteBuf](http://hongweiyi.com/2014/01/netty-4-x-bytebuf/)，netty4实现了PooledByteBufAllocator，传闻是可以大大减少GC的压力，但是官方不保证没有内存泄露，我自己压测中也出现了内存泄露的警告，建议生产中谨慎使用该功能。\n\n> netty5.x有一个更为高级的buffer泄露跟踪机制，PooledByteBufAllocator也已经默认开启，有机会可以尝试使用一下。\n","slug":"2014/05/netty-mina-in-depth-2","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91q001p3x8fqygs3v42"},{"title":"netty-mina深入学习与对比（一）","date":"2014-05-17T13:42:00.000Z","_content":"\n这博文的系列主要是为了更好的了解一个完整的nio框架的编程细节以及演进过程，我选了同父（Trustin Lee）的两个框架netty与mina做对比。版本涉及了netty3.x、netty4.x、mina1.x、mina2.x、mina3.x。这里并没有写netty5.x的细节，看了[netty5的修改文档](http://netty.io/wiki/new-and-noteworthy-in-5.x.html)，似乎有一些比较有意思的改动，准备单独写一篇netty4.x与netty5.x的不同。\n\n<!--more-->\n\nnetty从twitter发布的这篇[Netty 4 at Twitter: Reduced GC Overhead](https://blog.twitter.com/2013/netty-4-at-twitter-reduced-gc-overhead)文章让国内Java界为之一振，也小火了一把，同时netty的社区发展也不错，版本迭代非常快，半年不关注大、小版本就发了好几轮了。但是mina就有点淡了，github上面它最后大多数代码最后的修改日期均在2013年，不过我从个人情感上还是挺喜欢mina3的代码，没有太多的用不上的功能（支持各种协议啥的），跑自带的benchmark性能也比netty4好一些。但是如果是生产用的话，就偏向netty多一些了，毕竟社区活跃，版本迭代也快。\n\n### 1. mina、netty的线程模型\n\nmina与netty都是Trustin Lee的作品，所以在很多方面都十分相似，他们线程模型也是基本一致，采用了Reactors in threads模型，即Main Reactor + Sub Reactors的模式。由main reactor处理连接相关的任务：accept、connect等，当连接处理完毕并建立一个socket连接（称之为session）后，给每个session分配一个sub reactor，之后该session的所有IO、业务逻辑处理均交给了该sub reactor。每个reactor均是一个线程，sub reactor中只靠内核调度，没有任何通信且互不打扰。\n\n在前面的博文：[Netty 4.x学习笔记 – 线程模型](http://hongweiyi.com/2014/01/netty-4-x-thread-model/)，对netty的线程模型有一定的介绍。现在来讲讲我对线程模型演进的一些理解：\n\n* **`Thread per Connection`**: 在没有nio之前，这是传统的java网络编程方案所采用的线程模型。即有一个主循环，socket.accept阻塞等待，当建立连接后，创建新的线程/从线程池中取一个，把该socket连接交由新线程全权处理。这种方案优缺点都很明显，优点即实现简单，缺点则是方案的伸缩性受到线程数的限制。\n* **`Reactor in Single Thread`**: 有了nio后，可以采用IO多路复用机制了。我们抽取出一个单线程版的reactor模型，时序图见下文，该方案只有一个线程，所有的socket连接均注册在了该reactor上，由一个线程全权负责所有的任务。它实现简单，且不受线程数的限制。这种方案受限于使用场景，仅适合于IO密集的应用，不太适合CPU密集的应用，且适合于CPU资源紧张的应用上。\n\n<center><div style=\"width: 30%;\">![Reactor Single Thread](/images/reactor-single-thread.png)</div></center>\n\n* **`Reactor + Thread Pool`**: 方案2由于受限于使用场景，但为了可以更充分的使用CPU资源，抽取出一个逻辑处理线程池。reactor仅负责IO任务，线程池负责所有其它逻辑的处理。虽然该方案可以充分利用CPU资源，但是这个方案多了进出thread pool的两次上下文切换。\n\n<center><div style=\"width: 50%;\">![Reactor + Thread Pool](/images/reactor-thread-pool.png)</div></center>\n\n* **`Reactors in threads`**: 基于方案3缺点的考虑，将reactor分成两个部分。main reactor负责连接任务（accept、connect等），sub reactor负责IO、逻辑任务，即mina与netty的线程模型。该方案适应性十分强，可以调整sub reactor的数量适应CPU资源紧张的应用；同时CPU密集型任务时，又可以在业务处理逻辑中将任务交由线程池处理，如方案5。该方案有一个不太明显的缺点，即session没有分优先级，所有session平等对待均分到所有的线程中，这样可能会导致优先级低耗资源的session堵塞高优先级的session，但似乎netty与mina并没有针对这个做优化。\n\n<center><div style=\"width: 60%;\">![Reactors in threads](/images/reactors-in-threads.png)</div></center>\n\n* **`Reactors in threads + Threads pool`**: 这也是我所在公司应用框架采用的模型，可以更为灵活的适应所有的应用场景：调整reactor数量、调整thread pool大小等。\n\n<center><div style=\"width: 70%;\">![Reactors in threads + Thread pool](/images/reactors-in-threads-thread-pool.png)</div></center>\n\n> 以上图片及总结参考：《Linux多线程服务端编程》\n\n### 2. mina、netty的任务调度粒度\n\nmina、netty在线程模型上并没有太大的差异性，主要的差异还是在任务调度的粒度的不同。任务从逻辑上我给它分为成三种类型：连接相关的任务（bind、connect等）、写任务（write、flush）、调度任务（延迟、定时等），读任务则由selector加循环时间控制了。mina、netty任务调度的趋势是逐渐变小，从session级别的调度 -&gt; 类型级别任务的调度 -&gt; 任务的调度。\n\n#### 2.1 代码\n\n* `mina-1.1.7`: SocketIoProcessor$Worker.run\n* `mina-2.0.4`: AbstractPollingIoProcessor$Processor.run\n* `mina-3.0.0.M3-SNAPSHOT`: AbstractNioSession.processWrite\n* `netty-3.5.8.Final`: AbstractNioSelector.run\n* `netty-4.0.6.Final`: NioEventLoop.run\n\n#### 2.2 分析\n\nmina1、2的任务调度粒度为session。mina会将有IO任务的的session写入队列中，当循环执行任务时，则会轮询所有的session，并依次把session中的所有任务取出来运行。这样粗粒度的调度是不公平调度，会导致某些请求的延迟很高。\n\nmina3的模型改动比较大，代码相对就比较难看了，我仅是随便扫了一下，它仅提炼出了writeQueue。\n\n而netty3的调度粒度则是按照IO操作，分成了registerTaskQueue、writeTaskQueue、eventQueue三个队列，当有IO任务时，依次processRegisterTaskQueue、processEventQueue、processWriteTaskQueue、processSelectedKeys(selector.selectedKeys)。\n\nnetty4可能觉得netty3的粒度还是比较粗，将队列细分成了taskQueue和delayedTaskQueue，所有的任务均放在taskQueue中，delayedTaskQueue则是定时调度任务，且netty4可以灵活配置task与selectedKey处理的时间比例。\n\n> BTW: netty3.6.0之后，所有的队列均合并成了一个taskQueue\n\n有意思的是，netty4会优先处理selectedKeys，然后再处理任务，netty3则相反。mina1、2则是先处理新建的session，再处理selectedKeys，再处理任务。\n\n> 难道selectedKeys处理顺序有讲究么？\n","source":"_posts/2014/05/netty-mina-in-depth-1.md","raw":"title: netty-mina深入学习与对比（一）\ndate: 2014-05-17 21:42:00\ncategories: 技术分享\ntags: [Java, Mina, Netty, 线程模型]\n\n---\n\n这博文的系列主要是为了更好的了解一个完整的nio框架的编程细节以及演进过程，我选了同父（Trustin Lee）的两个框架netty与mina做对比。版本涉及了netty3.x、netty4.x、mina1.x、mina2.x、mina3.x。这里并没有写netty5.x的细节，看了[netty5的修改文档](http://netty.io/wiki/new-and-noteworthy-in-5.x.html)，似乎有一些比较有意思的改动，准备单独写一篇netty4.x与netty5.x的不同。\n\n<!--more-->\n\nnetty从twitter发布的这篇[Netty 4 at Twitter: Reduced GC Overhead](https://blog.twitter.com/2013/netty-4-at-twitter-reduced-gc-overhead)文章让国内Java界为之一振，也小火了一把，同时netty的社区发展也不错，版本迭代非常快，半年不关注大、小版本就发了好几轮了。但是mina就有点淡了，github上面它最后大多数代码最后的修改日期均在2013年，不过我从个人情感上还是挺喜欢mina3的代码，没有太多的用不上的功能（支持各种协议啥的），跑自带的benchmark性能也比netty4好一些。但是如果是生产用的话，就偏向netty多一些了，毕竟社区活跃，版本迭代也快。\n\n### 1. mina、netty的线程模型\n\nmina与netty都是Trustin Lee的作品，所以在很多方面都十分相似，他们线程模型也是基本一致，采用了Reactors in threads模型，即Main Reactor + Sub Reactors的模式。由main reactor处理连接相关的任务：accept、connect等，当连接处理完毕并建立一个socket连接（称之为session）后，给每个session分配一个sub reactor，之后该session的所有IO、业务逻辑处理均交给了该sub reactor。每个reactor均是一个线程，sub reactor中只靠内核调度，没有任何通信且互不打扰。\n\n在前面的博文：[Netty 4.x学习笔记 – 线程模型](http://hongweiyi.com/2014/01/netty-4-x-thread-model/)，对netty的线程模型有一定的介绍。现在来讲讲我对线程模型演进的一些理解：\n\n* **`Thread per Connection`**: 在没有nio之前，这是传统的java网络编程方案所采用的线程模型。即有一个主循环，socket.accept阻塞等待，当建立连接后，创建新的线程/从线程池中取一个，把该socket连接交由新线程全权处理。这种方案优缺点都很明显，优点即实现简单，缺点则是方案的伸缩性受到线程数的限制。\n* **`Reactor in Single Thread`**: 有了nio后，可以采用IO多路复用机制了。我们抽取出一个单线程版的reactor模型，时序图见下文，该方案只有一个线程，所有的socket连接均注册在了该reactor上，由一个线程全权负责所有的任务。它实现简单，且不受线程数的限制。这种方案受限于使用场景，仅适合于IO密集的应用，不太适合CPU密集的应用，且适合于CPU资源紧张的应用上。\n\n<center><div style=\"width: 30%;\">![Reactor Single Thread](/images/reactor-single-thread.png)</div></center>\n\n* **`Reactor + Thread Pool`**: 方案2由于受限于使用场景，但为了可以更充分的使用CPU资源，抽取出一个逻辑处理线程池。reactor仅负责IO任务，线程池负责所有其它逻辑的处理。虽然该方案可以充分利用CPU资源，但是这个方案多了进出thread pool的两次上下文切换。\n\n<center><div style=\"width: 50%;\">![Reactor + Thread Pool](/images/reactor-thread-pool.png)</div></center>\n\n* **`Reactors in threads`**: 基于方案3缺点的考虑，将reactor分成两个部分。main reactor负责连接任务（accept、connect等），sub reactor负责IO、逻辑任务，即mina与netty的线程模型。该方案适应性十分强，可以调整sub reactor的数量适应CPU资源紧张的应用；同时CPU密集型任务时，又可以在业务处理逻辑中将任务交由线程池处理，如方案5。该方案有一个不太明显的缺点，即session没有分优先级，所有session平等对待均分到所有的线程中，这样可能会导致优先级低耗资源的session堵塞高优先级的session，但似乎netty与mina并没有针对这个做优化。\n\n<center><div style=\"width: 60%;\">![Reactors in threads](/images/reactors-in-threads.png)</div></center>\n\n* **`Reactors in threads + Threads pool`**: 这也是我所在公司应用框架采用的模型，可以更为灵活的适应所有的应用场景：调整reactor数量、调整thread pool大小等。\n\n<center><div style=\"width: 70%;\">![Reactors in threads + Thread pool](/images/reactors-in-threads-thread-pool.png)</div></center>\n\n> 以上图片及总结参考：《Linux多线程服务端编程》\n\n### 2. mina、netty的任务调度粒度\n\nmina、netty在线程模型上并没有太大的差异性，主要的差异还是在任务调度的粒度的不同。任务从逻辑上我给它分为成三种类型：连接相关的任务（bind、connect等）、写任务（write、flush）、调度任务（延迟、定时等），读任务则由selector加循环时间控制了。mina、netty任务调度的趋势是逐渐变小，从session级别的调度 -&gt; 类型级别任务的调度 -&gt; 任务的调度。\n\n#### 2.1 代码\n\n* `mina-1.1.7`: SocketIoProcessor$Worker.run\n* `mina-2.0.4`: AbstractPollingIoProcessor$Processor.run\n* `mina-3.0.0.M3-SNAPSHOT`: AbstractNioSession.processWrite\n* `netty-3.5.8.Final`: AbstractNioSelector.run\n* `netty-4.0.6.Final`: NioEventLoop.run\n\n#### 2.2 分析\n\nmina1、2的任务调度粒度为session。mina会将有IO任务的的session写入队列中，当循环执行任务时，则会轮询所有的session，并依次把session中的所有任务取出来运行。这样粗粒度的调度是不公平调度，会导致某些请求的延迟很高。\n\nmina3的模型改动比较大，代码相对就比较难看了，我仅是随便扫了一下，它仅提炼出了writeQueue。\n\n而netty3的调度粒度则是按照IO操作，分成了registerTaskQueue、writeTaskQueue、eventQueue三个队列，当有IO任务时，依次processRegisterTaskQueue、processEventQueue、processWriteTaskQueue、processSelectedKeys(selector.selectedKeys)。\n\nnetty4可能觉得netty3的粒度还是比较粗，将队列细分成了taskQueue和delayedTaskQueue，所有的任务均放在taskQueue中，delayedTaskQueue则是定时调度任务，且netty4可以灵活配置task与selectedKey处理的时间比例。\n\n> BTW: netty3.6.0之后，所有的队列均合并成了一个taskQueue\n\n有意思的是，netty4会优先处理selectedKeys，然后再处理任务，netty3则相反。mina1、2则是先处理新建的session，再处理selectedKeys，再处理任务。\n\n> 难道selectedKeys处理顺序有讲究么？\n","slug":"2014/05/netty-mina-in-depth-1","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91s001x3x8fzd532v71"},{"title":"REWORK - 灵感稍纵即逝","date":"2014-04-14T13:40:00.000Z","_content":"\n昨天在hn上看到37signals，顺带看到《rework》这本书，多看试读了一下，立马亚马逊下单了。怎么说吧，里面的大多数文字都让我产生深深的共鸣，两小时读完，有种恨不得立马去他公司去体验一把的感觉，这感觉是如此奇妙。\n\n苦于自己不善于文字，就只能在这里把书里最后的总结摘抄上来，与大家分享分享：\n\n<!--more-->\n\n> 我们都有想法，想法是不朽的，一直都会存在。\n\n> 最不可能长存的就是灵感。灵感就像新鲜水果或牛奶：有一定的保质期。\n\n> 如果你想去做一件事，就得马上下手。不能把这事搁置起来过两个月再考虑。不要对自己说『以后再说吧』。以后，你压根儿不会再提起这件事了。\n\n> 如果你的灵感是在周五驾临，那就放弃周末，直奔主题。当你为了这个灵感而亢奋时，就能够在24小时内做完两个星期的工作。从这一点来讲，灵感就是时光机。\n\n> 灵感是个奇妙的东西，是效率放大器，是推进器。但是它不会停下来等你。灵感转瞬即逝，当它来找你时，要立即把它捕捉住，将其投入工作中去。\n","source":"_posts/2014/04/rework-digest.md","raw":"title: REWORK - 灵感稍纵即逝\ndate: 2014-04-14 21:40:00\ncategories: 生活点滴\ntags: [REWORK, 37signals, 书摘, 生活]\n---\n\n昨天在hn上看到37signals，顺带看到《rework》这本书，多看试读了一下，立马亚马逊下单了。怎么说吧，里面的大多数文字都让我产生深深的共鸣，两小时读完，有种恨不得立马去他公司去体验一把的感觉，这感觉是如此奇妙。\n\n苦于自己不善于文字，就只能在这里把书里最后的总结摘抄上来，与大家分享分享：\n\n<!--more-->\n\n> 我们都有想法，想法是不朽的，一直都会存在。\n\n> 最不可能长存的就是灵感。灵感就像新鲜水果或牛奶：有一定的保质期。\n\n> 如果你想去做一件事，就得马上下手。不能把这事搁置起来过两个月再考虑。不要对自己说『以后再说吧』。以后，你压根儿不会再提起这件事了。\n\n> 如果你的灵感是在周五驾临，那就放弃周末，直奔主题。当你为了这个灵感而亢奋时，就能够在24小时内做完两个星期的工作。从这一点来讲，灵感就是时光机。\n\n> 灵感是个奇妙的东西，是效率放大器，是推进器。但是它不会停下来等你。灵感转瞬即逝，当它来找你时，要立即把它捕捉住，将其投入工作中去。\n","slug":"2014/04/rework-digest","published":1,"updated":"2015-12-30T14:14:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91u00243x8ffljdbbw3"},{"title":"有求皆苦、无欲则刚","date":"2014-04-13T15:48:00.000Z","_content":"\n\n最近会听到不少周围朋友负能量的东西，无外乎是升职、加薪、倒挂之类与经济利益挂钩的事情。当然，也会听到一些情感的负能量，不过这个应该算是占少数吧，毕竟我对这个不太感冒。正如前女友所说：「天底下像你一样的程序员屌丝男真的不多。」「像我哪样了」「不寂寞，不要女朋友的」，忧桑。\n\n<!--more-->\n\n最近几周和老同学聚的比较多，可能是同学的缘故，大家也不会藏着掖着，有啥话都会直接说。\n\n> 「擦，我那老大太变态了，需求一天一个变」\n> 「哎，这特么升P又卡住了，还得面试」\n> 「你知道那谁不，不到一年老大特批给升了」\n\n嗯，顺带也听了不少坊间八卦。与他们沟通具体情况时，发现公司其他部门情况（主要是业务部门）与我想像中的大相径庭，各种勾心斗角尔虞我诈。但是奇怪的是，我在这里也待了一年了，并没有发现类似的情况出现，一切都是那么的和谐，是我想简单了么？\n\n不过，在众多负能量中也有一些正能量的出现。有趣的是，在这个正能量的主人身上很少听到负能量的东西：\n\n> 「老大让我好好准备下5月份的晋升」\n\n所以，我又在想：是他们主动产生了正/负能量，还是客观的正/负能量影响了他们，抑或是相辅相成的，搞不明白了。我一直都会说，计算机艺术相较于人的艺术是那么的简单。看样子不仅仅是简单那么简单了，应该改成：「计算机艺术相较于人的艺术是那么的小儿科」\n\n对于升职加薪之类的，我确实也挺想的，谁不想呢。但是我会尽可能保持一种「有求皆苦、无欲则刚」的态度，这八个字在去年这个时候我还不太理解，在入职培训的时候，和爱好佛学的@湛然同学请教过这个，能稍微有点参悟。大致对话回忆如下：\n\n> 「菩萨不是说要救苦救难，普度众生啥的，这不算是欲么？」\n> 「就是这样，他才是菩萨。观音菩萨要普度众生，地藏菩萨要地狱没有鬼魂，这都是欲，当无欲了才能称之为佛。佛讲究无欲，并不是无所诉求，而是不会将诉求放在口中、心上。如你努力工作，不把欲望挂在心中，你就会得到你所要的东西」\n\n嗯，希望我也能继续保持这种态度吧，尽可能广/多/快的汲取周边的营养让自己得到快速的成长（这也算欲吧？我还是比较贪的，嗯）。\n","source":"_posts/2014/04/forget-your-lusts.md","raw":"title: 有求皆苦、无欲则刚\ndate: 2014-04-13 23:48:00\ncategories: 生活点滴\ntags: [价值观, 生活]\n---\n\n\n最近会听到不少周围朋友负能量的东西，无外乎是升职、加薪、倒挂之类与经济利益挂钩的事情。当然，也会听到一些情感的负能量，不过这个应该算是占少数吧，毕竟我对这个不太感冒。正如前女友所说：「天底下像你一样的程序员屌丝男真的不多。」「像我哪样了」「不寂寞，不要女朋友的」，忧桑。\n\n<!--more-->\n\n最近几周和老同学聚的比较多，可能是同学的缘故，大家也不会藏着掖着，有啥话都会直接说。\n\n> 「擦，我那老大太变态了，需求一天一个变」\n> 「哎，这特么升P又卡住了，还得面试」\n> 「你知道那谁不，不到一年老大特批给升了」\n\n嗯，顺带也听了不少坊间八卦。与他们沟通具体情况时，发现公司其他部门情况（主要是业务部门）与我想像中的大相径庭，各种勾心斗角尔虞我诈。但是奇怪的是，我在这里也待了一年了，并没有发现类似的情况出现，一切都是那么的和谐，是我想简单了么？\n\n不过，在众多负能量中也有一些正能量的出现。有趣的是，在这个正能量的主人身上很少听到负能量的东西：\n\n> 「老大让我好好准备下5月份的晋升」\n\n所以，我又在想：是他们主动产生了正/负能量，还是客观的正/负能量影响了他们，抑或是相辅相成的，搞不明白了。我一直都会说，计算机艺术相较于人的艺术是那么的简单。看样子不仅仅是简单那么简单了，应该改成：「计算机艺术相较于人的艺术是那么的小儿科」\n\n对于升职加薪之类的，我确实也挺想的，谁不想呢。但是我会尽可能保持一种「有求皆苦、无欲则刚」的态度，这八个字在去年这个时候我还不太理解，在入职培训的时候，和爱好佛学的@湛然同学请教过这个，能稍微有点参悟。大致对话回忆如下：\n\n> 「菩萨不是说要救苦救难，普度众生啥的，这不算是欲么？」\n> 「就是这样，他才是菩萨。观音菩萨要普度众生，地藏菩萨要地狱没有鬼魂，这都是欲，当无欲了才能称之为佛。佛讲究无欲，并不是无所诉求，而是不会将诉求放在口中、心上。如你努力工作，不把欲望挂在心中，你就会得到你所要的东西」\n\n嗯，希望我也能继续保持这种态度吧，尽可能广/多/快的汲取周边的营养让自己得到快速的成长（这也算欲吧？我还是比较贪的，嗯）。\n","slug":"2014/04/forget-your-lusts","published":1,"updated":"2015-12-30T16:15:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba91x002c3x8ffayl09am"},{"title":"Netty 4.x学习笔记 - 线程模型","date":"2014-01-14T15:25:00.000Z","_content":"\n### 1、前言\n\n前面两篇学习笔记已经说完了[ByteBuf](http://hongweiyi.com/2014/01/netty-4-x-bytebuf/)和[Channel和Pipeline](http://hongweiyi.com/2014/01/netty-4-x-channel-pipeline/)，这篇开始讲讲前面欠的债——线程模型（EventLoop和EventExecutor）。\n\n<!--more-->\n\n\n### 2、Netty线程模型\n\n将具体代码实现前，先来谈谈Netty的线程模型。正如许多博客所提到的，Netty采用了Reactor模式，但是许多博客也只是提到了而已，同时大家也不会忘记附上几张Doug Lee大神的图，但是并不会深入的解释。为了更好的学习和理解Netty的线程模型，我在这里稍微详细的说一下我对它的理解。\n\nReactor模式有多个变种，Netty基于Multiple Reactors模式（如下图）做了一定的修改，Mutilple Reactors模式有多个reactor：mainReactor和subReactor，其中mainReactor负责客户端的连接请求，并将请求转交给subReactor，后由subReactor负责相应通道的IO请求，非IO请求（具体逻辑处理）的任务则会直接写入队列，等待worker threads进行处理。\n\n<center><div style=\"width: 80%;\">![Multiple Reactors](/images/wpid-Multi-reactors3.png)</div></center>\n\nNetty的线程模型基于Multiple Reactors模式，借用了mainReactor和subReactor的结构，但是从代码里看来，它并没有Thread Pool这个东东。Netty的subReactor与worker thread是同一个线程，采用IO多路复用机制，可以使一个subReactor监听并处理多个channel的IO请求，我给称之为：「Single Thread with many Channel」。我根据代码整理出下面这种Netty线程模型图：\n\n<center><div style=\"width: 80%;\">![Netty线程模型](/images/wpid-Netty-thread-model3.png)</div></center>\n\n上图中的parentGroup和childGroup是Bootstrap构造方法中传入的两个对象，这两个group均是线程池，childGroup线程池会被各个subReactor充分利用，parentGroup线程池则只是在bind某个端口后，获得其中一个线程作为mainReactor。上图我将subReactor和worker thread合并成了一个个的loop，具体的请求操作均在loop中完成，下文会对loop有个稍微详细的解释。&nbsp;\n\n>\t以上均是Nio情况下。Oio采用的是Thread per Channel机制，即每个连接均创建一个线程负责该连接的所有事宜。\n>\tDoug Lee大神的Reactor介绍：[Scalable IO in Java](http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf)\n\n### 3、EventLoop和EventExecutor实现\n\nEventLoop和EventExecutor实现共有4个主要逻辑接口，EventLoop、EventLoopGroup、EventExecutor、EventExecutorGroup，内部实现、继承的逻辑表示无法直视，有种擦边球的感觉。具体的类图如下：\n\n<center><div style=\"width: 80%;\">![EventLoop和EventExecutor类图](/images/wpid-EventLoopAndEventExecutor3.jpg)</div></center>\n\n#### 3.1 EventLoopGroup:\n\n主要方法是newChild，我理解为EventLoop的工厂类。`**EventLoopGroup.newChild`创建`**EventLoop`对象。OioEventLoopGroup除外，它没有实现newChild方法，调用父类的并创建ThreadPerChannelEventLoop对象。\n\n#### 3.2 EventLoop:\n\n主要方法是run()，是整个Netty执行过程的逻辑代码实现，后面细说。\n\n#### 3.3 EventExecutorGroup:\n\n线程池实现，主要成员是children数组，主要方法是next()，获得线程池中的一个线程，由子类调用。由于Oio采用的是Thread per Channel机制，所以没有实现前面两个。\n\n#### 3.4 EventExecutor:\n\nTask的执行类，主要成员是taskQueue以及真正的运行线程对象executor，主要方法是taskQueue操作方法execute、takeTask、addTask等，以及doStartThread方法，后面细说。\n\n### 4、NioEventLoopGroup实现\n\n这里以常用的NioEventLoopGroup为例。NioEventLoopGroup在Bootstrap初始化时作为参数传入构造方法，由于NioEventLoopGroup涉及的代码较多，就不大篇幅的贴代码了，只写流程性的文字或相应类和方法：\n\n#### 4.1 mainReactor:\n\n`1. Bootstrap.bind(port)`\n`2. Bootstrap.initAndRegister()`\n`2.1 Boostrap.init()`\n\n> 初始化Channel，配置Channel参数，以及Pipeline。其中初始化Pipeline中，需要插入ServerBootstrapAcceptor对象用作acceptor接收客户端连接请求，acceptor也是一种ChannelInboundHandlerAdapter。\n\n``` java\np.addLast(new ChannelInitializer<Channel>() {\n  @Override\n  public void initChannel(Channel ch) throws Exception {\n    ch.pipeline().addLast(new ServerBootstrapAcceptor(currentChildHandler, currentChildOptions,\n       currentChildAttrs));\n  }\n});\n```\n\n> 调用channel的unsafe对象注册selector，具体实现类为AbstractChannel$AbstractUnsafe.register。如下：\n\n``` java\npublic final void register(final ChannelPromise promise) {\n  if (eventLoop.inEventLoop()) {  // 是否在Channel的loop中\n    register0(promise);\n  } else {  // 不在\n    try {\n      eventLoop.execute(new Runnable() {  // EventLoop执行一个任务\n        @Override\n        public void run() {\n          register0(promise);\n        }\n      });\n    } catch (Throwable t) {\n    // ...\n    }\n  }\n}\n```\n\n> eventLoop.execute(runnable);是比较重要的一个方法。在没有启动真正线程时，它会启动线程并将待执行任务放入执行队列里面。启动真正线程(startThread())会判断是否该线程已经启动，如果已经启动则会直接跳过，达到线程复用的目的。启动的线程，主要调用方法是NioEventLoop的run()方法，run()方法在下面有详细介绍：\n\n``` java\npublic void execute(Runnable task) {\n  if (task == null) {\n    throw new NullPointerException(&quot;task&quot;);\n  }\n\n  boolean inEventLoop = inEventLoop();\n  if (inEventLoop) {\n    addTask(task);\n  } else {\n    startThread();  // 启动线程\n    addTask(task);  // 添加任务队列\n\n    // ...\n\n  }\n\n  if (!addTaskWakesUp) {\n    wakeup(inEventLoop);\n  }\n}\n```\n\n   2.2 group().register(channel)\n\n> 将 channel 注册到下一个 EventLoop 中。\n\n`3. 接收连接请求`\n\n由NioEventLoop.run()接收到请求：\n\n`3.1 AbstractNioMessageChannel$NioMessageUnsafe.read()`\n\n`3.2 NioServerSocketChannel.doReadMessages()`\n\n> 获得childEventLoopGroup中的EventLoop，并依据该loop创建新的SocketChannel对象。\n\n`3.3 pipeline.fireChannelRead(readBuf.get(i));`\n\n> readBuf.get(i)就是3.2中创建的SocketChannel对象。在2.2初始化Bootstrap的时候，已经将acceptor处理器插入pipeline中，所以理所当然，这个SocketChannel对象由acceptor处理器处理。\n\n`3.4 ServerBootstrapAcceptor$ServerBootstrapAcceptor.channelRead();`\n\n> 该方法流程与2.2、2.3类似，初始化子channel，并注册到相应的selector。注册的时候，也会调用eventLoop.execute用以执行注册任务，execute时，启动子线程。即启动了subReactor。\n\n#### 4.2 subReactor:\n\nsubReactor的流程较为简单，主体完全依赖于loop，用以执行read、write还有自定义的NioTask操作，就不深入了，直接跳过解释loop过程。\n\n**loop:**\n\nloop是我自己提出来的组件，仅是代表subReactor的主要运行逻辑。例子可以参考NioEventLoop.run()。\n\nloop会不断循环一个过程：select -&gt; processSelectedKeys(IO操作) -&gt; runAllTasks(非IO操作)，如下代码：\n\n``` java\nprotected void run() {\n  for (;;) {\n    // ...\n    try {\n      if (hasTasks()) { // 如果队列中仍有任务\n        selectNow();\n      } else {\n        select();\n        // ...\n      }\n\n      // ...\n\n      final long ioStartTime = System.nanoTime();  // 用以控制IO任务与非IO任务的运行时间比\n      needsToSelectAgain = false;\n      // IO任务\n      if (selectedKeys != null) {\n        processSelectedKeysOptimized(selectedKeys.flip());\n      } else {\n        processSelectedKeysPlain(selector.selectedKeys());\n      }\n      final long ioTime = System.nanoTime() - ioStartTime;\n\n      final int ioRatio = this.ioRatio;\n      // 非IO任务\n      runAllTasks(ioTime * (100 - ioRatio) / ioRatio);\n\n      if (isShuttingDown()) {\n        closeAll();\n        if (confirmShutdown()) {\n          break;\n        }\n      }\n    } catch (Throwable t) {\n    // ...\n    }\n  }\n}\n```\n\n\n就目前而言，基本上IO任务都会走processSelectedKeysOptimized方法，该方法即代表使用了优化的SelectedKeys。除非采用了比较特殊的JDK实现，基本都会走该方法。\n\n> 1. selectedKeys在openSelector()方法中初始化，Netty通过反射修改了Selector的selectedKeys成员和publicSelectedKeys成员。替换成了自己的实现&mdash;&mdash;SelectedSelectionKeySet。\n> 2. 从OpenJDK 6/7的SelectorImpl中可以看到，selectedKeys和publicSeletedKeys均采用了HashSet实现。HashSet采用HashMap实现，插入需要计算Hash并解决Hash冲突并挂链，而SelectedSelectionKeySet实现使用了双数组，每次插入尾部，扩展策略为double，调用flip()则返回当前数组并切换到另外一个数据。\n> 3. ByteBuf中去掉了flip，在这里是否也可以呢？\n\nprocessSelectedKeysOptimized主要流程如下：\n\n``` java\nfinal Object a = k.attachment();\n\nif (a instanceof AbstractNioChannel) {\n  processSelectedKey(k, (AbstractNioChannel) a);\n} else {\n  @SuppressWarnings(&quot;unchecked&quot;)\n  NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n  processSelectedKey(k, task);\n}\n```\n\n在获得attachment后，判断是Channel呢还是其他，其他则是NioTask。找遍代码并没有发现Netty有注册NioTask的行为，同时也没发现NioTask的实现类。只有在NioEventLoop.register方法中有注册NioTask至selector的行为，便判断该行为是由用户调用，可以针对某个Channel注册自己的NioTask。这里就只讲第一个processSelectdKey(k, (AbstractNioChannel) a)，但代码就不贴了。\n\n和常规的NIO代码类似，processSelectdKey是判断SeletedKeys的readyOps，并做出相应的操作。操作均是unsafe做的。如read可以参考：AbstractNioByteChannel$NioByteUnsafe.read()。IO操作的流程大致都是：\n\n* 获得数据\n* 调用pipeline的方法，`fireChannel***`\n* 插入任务队列\n\n执行完所有IO操作后，开始执行非IO任务（runAllTasks）。Netty会控制IO和非IO任务的比例，ioTime * (100 - ioRatio) / ioRatio，默认ioRatio为50。runAllTasks乃是父类SingleThreadExecutor的方法。方法主体很简单，将任务从TaskQueue拎出来，直接调用任务的run方法即可。\n\n>\t代码调用的是task.run()，而不是task.start()。即是单线程执行所有任务\n\n``` java\nprotected boolean runAllTasks(long timeoutNanos) {\n  fetchFromDelayedQueue();\n  Runnable task = pollTask();\n  if (task == null) {\n    return false;\n  }\n\n  // 控制时间\n  final long deadline = ScheduledFutureTask.nanoTime() + timeoutNanos;\n  long runTasks = 0;\n  long lastExecutionTime;\n  for (;;) {\n    try {\n      task.run();\n    } catch (Throwable t) {\n      logger.warn(&quot;A task raised an exception.&quot;, t);\n    }\n\n    runTasks ++;\n\n    // Check timeout every 64 tasks because nanoTime() is relatively expensive.\n    // XXX: Hard-coded value - will make it configurable if it is really a problem.\n    if ((runTasks & 0x3F) == 0) {\n        lastExecutionTime = ScheduledFutureTask.nanoTime();\n      if (lastExecutionTime >= deadline) {\n        break;\n      }\n    }\n\n    task = pollTask();\n    if (task == null) {\n     lastExecutionTime = ScheduledFutureTask.nanoTime();\n     break;\n    }\n  }\n\n  this.lastExecutionTime = lastExecutionTime;\n  return true;\n}\n```\n\n### 5、总结\n\n以上内容从设计和代码层面总结Netty线程模型的大致内容，中间有很多我的不成熟的思考与理解，请朋友轻拍与指正。\n\n看源码过程中是比较折磨人的。首先得了解你学习东西的业务价值是哪里？即你学了这个之后能用在哪里，只是不考虑场景仅仅为了看代码而看代码比较难以深入理解其内涵；其次，看代码一定一定得从逻辑、结构层面看，从细节层面看只会越陷越深，有种一叶障目不见泰山的感觉；最后，最好是能够将代码逻辑、结构画出来，或者整理出思维导图啥的，可以用以理清思路。前面两篇文章思维道路较为清晰，线程模型的导图有一些但是比较混乱，就不贴出来了，用作自己参考，有兴趣的可以找我要噢。\n","source":"_posts/2014/01/netty-4-x-thread-model.md","raw":"title: Netty 4.x学习笔记 - 线程模型\ndate: 2014-01-14 23:25:00\ncategories: 技术分享\ntags: [Netty, 线程模型]\n---\n\n### 1、前言\n\n前面两篇学习笔记已经说完了[ByteBuf](http://hongweiyi.com/2014/01/netty-4-x-bytebuf/)和[Channel和Pipeline](http://hongweiyi.com/2014/01/netty-4-x-channel-pipeline/)，这篇开始讲讲前面欠的债——线程模型（EventLoop和EventExecutor）。\n\n<!--more-->\n\n\n### 2、Netty线程模型\n\n将具体代码实现前，先来谈谈Netty的线程模型。正如许多博客所提到的，Netty采用了Reactor模式，但是许多博客也只是提到了而已，同时大家也不会忘记附上几张Doug Lee大神的图，但是并不会深入的解释。为了更好的学习和理解Netty的线程模型，我在这里稍微详细的说一下我对它的理解。\n\nReactor模式有多个变种，Netty基于Multiple Reactors模式（如下图）做了一定的修改，Mutilple Reactors模式有多个reactor：mainReactor和subReactor，其中mainReactor负责客户端的连接请求，并将请求转交给subReactor，后由subReactor负责相应通道的IO请求，非IO请求（具体逻辑处理）的任务则会直接写入队列，等待worker threads进行处理。\n\n<center><div style=\"width: 80%;\">![Multiple Reactors](/images/wpid-Multi-reactors3.png)</div></center>\n\nNetty的线程模型基于Multiple Reactors模式，借用了mainReactor和subReactor的结构，但是从代码里看来，它并没有Thread Pool这个东东。Netty的subReactor与worker thread是同一个线程，采用IO多路复用机制，可以使一个subReactor监听并处理多个channel的IO请求，我给称之为：「Single Thread with many Channel」。我根据代码整理出下面这种Netty线程模型图：\n\n<center><div style=\"width: 80%;\">![Netty线程模型](/images/wpid-Netty-thread-model3.png)</div></center>\n\n上图中的parentGroup和childGroup是Bootstrap构造方法中传入的两个对象，这两个group均是线程池，childGroup线程池会被各个subReactor充分利用，parentGroup线程池则只是在bind某个端口后，获得其中一个线程作为mainReactor。上图我将subReactor和worker thread合并成了一个个的loop，具体的请求操作均在loop中完成，下文会对loop有个稍微详细的解释。&nbsp;\n\n>\t以上均是Nio情况下。Oio采用的是Thread per Channel机制，即每个连接均创建一个线程负责该连接的所有事宜。\n>\tDoug Lee大神的Reactor介绍：[Scalable IO in Java](http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf)\n\n### 3、EventLoop和EventExecutor实现\n\nEventLoop和EventExecutor实现共有4个主要逻辑接口，EventLoop、EventLoopGroup、EventExecutor、EventExecutorGroup，内部实现、继承的逻辑表示无法直视，有种擦边球的感觉。具体的类图如下：\n\n<center><div style=\"width: 80%;\">![EventLoop和EventExecutor类图](/images/wpid-EventLoopAndEventExecutor3.jpg)</div></center>\n\n#### 3.1 EventLoopGroup:\n\n主要方法是newChild，我理解为EventLoop的工厂类。`**EventLoopGroup.newChild`创建`**EventLoop`对象。OioEventLoopGroup除外，它没有实现newChild方法，调用父类的并创建ThreadPerChannelEventLoop对象。\n\n#### 3.2 EventLoop:\n\n主要方法是run()，是整个Netty执行过程的逻辑代码实现，后面细说。\n\n#### 3.3 EventExecutorGroup:\n\n线程池实现，主要成员是children数组，主要方法是next()，获得线程池中的一个线程，由子类调用。由于Oio采用的是Thread per Channel机制，所以没有实现前面两个。\n\n#### 3.4 EventExecutor:\n\nTask的执行类，主要成员是taskQueue以及真正的运行线程对象executor，主要方法是taskQueue操作方法execute、takeTask、addTask等，以及doStartThread方法，后面细说。\n\n### 4、NioEventLoopGroup实现\n\n这里以常用的NioEventLoopGroup为例。NioEventLoopGroup在Bootstrap初始化时作为参数传入构造方法，由于NioEventLoopGroup涉及的代码较多，就不大篇幅的贴代码了，只写流程性的文字或相应类和方法：\n\n#### 4.1 mainReactor:\n\n`1. Bootstrap.bind(port)`\n`2. Bootstrap.initAndRegister()`\n`2.1 Boostrap.init()`\n\n> 初始化Channel，配置Channel参数，以及Pipeline。其中初始化Pipeline中，需要插入ServerBootstrapAcceptor对象用作acceptor接收客户端连接请求，acceptor也是一种ChannelInboundHandlerAdapter。\n\n``` java\np.addLast(new ChannelInitializer<Channel>() {\n  @Override\n  public void initChannel(Channel ch) throws Exception {\n    ch.pipeline().addLast(new ServerBootstrapAcceptor(currentChildHandler, currentChildOptions,\n       currentChildAttrs));\n  }\n});\n```\n\n> 调用channel的unsafe对象注册selector，具体实现类为AbstractChannel$AbstractUnsafe.register。如下：\n\n``` java\npublic final void register(final ChannelPromise promise) {\n  if (eventLoop.inEventLoop()) {  // 是否在Channel的loop中\n    register0(promise);\n  } else {  // 不在\n    try {\n      eventLoop.execute(new Runnable() {  // EventLoop执行一个任务\n        @Override\n        public void run() {\n          register0(promise);\n        }\n      });\n    } catch (Throwable t) {\n    // ...\n    }\n  }\n}\n```\n\n> eventLoop.execute(runnable);是比较重要的一个方法。在没有启动真正线程时，它会启动线程并将待执行任务放入执行队列里面。启动真正线程(startThread())会判断是否该线程已经启动，如果已经启动则会直接跳过，达到线程复用的目的。启动的线程，主要调用方法是NioEventLoop的run()方法，run()方法在下面有详细介绍：\n\n``` java\npublic void execute(Runnable task) {\n  if (task == null) {\n    throw new NullPointerException(&quot;task&quot;);\n  }\n\n  boolean inEventLoop = inEventLoop();\n  if (inEventLoop) {\n    addTask(task);\n  } else {\n    startThread();  // 启动线程\n    addTask(task);  // 添加任务队列\n\n    // ...\n\n  }\n\n  if (!addTaskWakesUp) {\n    wakeup(inEventLoop);\n  }\n}\n```\n\n   2.2 group().register(channel)\n\n> 将 channel 注册到下一个 EventLoop 中。\n\n`3. 接收连接请求`\n\n由NioEventLoop.run()接收到请求：\n\n`3.1 AbstractNioMessageChannel$NioMessageUnsafe.read()`\n\n`3.2 NioServerSocketChannel.doReadMessages()`\n\n> 获得childEventLoopGroup中的EventLoop，并依据该loop创建新的SocketChannel对象。\n\n`3.3 pipeline.fireChannelRead(readBuf.get(i));`\n\n> readBuf.get(i)就是3.2中创建的SocketChannel对象。在2.2初始化Bootstrap的时候，已经将acceptor处理器插入pipeline中，所以理所当然，这个SocketChannel对象由acceptor处理器处理。\n\n`3.4 ServerBootstrapAcceptor$ServerBootstrapAcceptor.channelRead();`\n\n> 该方法流程与2.2、2.3类似，初始化子channel，并注册到相应的selector。注册的时候，也会调用eventLoop.execute用以执行注册任务，execute时，启动子线程。即启动了subReactor。\n\n#### 4.2 subReactor:\n\nsubReactor的流程较为简单，主体完全依赖于loop，用以执行read、write还有自定义的NioTask操作，就不深入了，直接跳过解释loop过程。\n\n**loop:**\n\nloop是我自己提出来的组件，仅是代表subReactor的主要运行逻辑。例子可以参考NioEventLoop.run()。\n\nloop会不断循环一个过程：select -&gt; processSelectedKeys(IO操作) -&gt; runAllTasks(非IO操作)，如下代码：\n\n``` java\nprotected void run() {\n  for (;;) {\n    // ...\n    try {\n      if (hasTasks()) { // 如果队列中仍有任务\n        selectNow();\n      } else {\n        select();\n        // ...\n      }\n\n      // ...\n\n      final long ioStartTime = System.nanoTime();  // 用以控制IO任务与非IO任务的运行时间比\n      needsToSelectAgain = false;\n      // IO任务\n      if (selectedKeys != null) {\n        processSelectedKeysOptimized(selectedKeys.flip());\n      } else {\n        processSelectedKeysPlain(selector.selectedKeys());\n      }\n      final long ioTime = System.nanoTime() - ioStartTime;\n\n      final int ioRatio = this.ioRatio;\n      // 非IO任务\n      runAllTasks(ioTime * (100 - ioRatio) / ioRatio);\n\n      if (isShuttingDown()) {\n        closeAll();\n        if (confirmShutdown()) {\n          break;\n        }\n      }\n    } catch (Throwable t) {\n    // ...\n    }\n  }\n}\n```\n\n\n就目前而言，基本上IO任务都会走processSelectedKeysOptimized方法，该方法即代表使用了优化的SelectedKeys。除非采用了比较特殊的JDK实现，基本都会走该方法。\n\n> 1. selectedKeys在openSelector()方法中初始化，Netty通过反射修改了Selector的selectedKeys成员和publicSelectedKeys成员。替换成了自己的实现&mdash;&mdash;SelectedSelectionKeySet。\n> 2. 从OpenJDK 6/7的SelectorImpl中可以看到，selectedKeys和publicSeletedKeys均采用了HashSet实现。HashSet采用HashMap实现，插入需要计算Hash并解决Hash冲突并挂链，而SelectedSelectionKeySet实现使用了双数组，每次插入尾部，扩展策略为double，调用flip()则返回当前数组并切换到另外一个数据。\n> 3. ByteBuf中去掉了flip，在这里是否也可以呢？\n\nprocessSelectedKeysOptimized主要流程如下：\n\n``` java\nfinal Object a = k.attachment();\n\nif (a instanceof AbstractNioChannel) {\n  processSelectedKey(k, (AbstractNioChannel) a);\n} else {\n  @SuppressWarnings(&quot;unchecked&quot;)\n  NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n  processSelectedKey(k, task);\n}\n```\n\n在获得attachment后，判断是Channel呢还是其他，其他则是NioTask。找遍代码并没有发现Netty有注册NioTask的行为，同时也没发现NioTask的实现类。只有在NioEventLoop.register方法中有注册NioTask至selector的行为，便判断该行为是由用户调用，可以针对某个Channel注册自己的NioTask。这里就只讲第一个processSelectdKey(k, (AbstractNioChannel) a)，但代码就不贴了。\n\n和常规的NIO代码类似，processSelectdKey是判断SeletedKeys的readyOps，并做出相应的操作。操作均是unsafe做的。如read可以参考：AbstractNioByteChannel$NioByteUnsafe.read()。IO操作的流程大致都是：\n\n* 获得数据\n* 调用pipeline的方法，`fireChannel***`\n* 插入任务队列\n\n执行完所有IO操作后，开始执行非IO任务（runAllTasks）。Netty会控制IO和非IO任务的比例，ioTime * (100 - ioRatio) / ioRatio，默认ioRatio为50。runAllTasks乃是父类SingleThreadExecutor的方法。方法主体很简单，将任务从TaskQueue拎出来，直接调用任务的run方法即可。\n\n>\t代码调用的是task.run()，而不是task.start()。即是单线程执行所有任务\n\n``` java\nprotected boolean runAllTasks(long timeoutNanos) {\n  fetchFromDelayedQueue();\n  Runnable task = pollTask();\n  if (task == null) {\n    return false;\n  }\n\n  // 控制时间\n  final long deadline = ScheduledFutureTask.nanoTime() + timeoutNanos;\n  long runTasks = 0;\n  long lastExecutionTime;\n  for (;;) {\n    try {\n      task.run();\n    } catch (Throwable t) {\n      logger.warn(&quot;A task raised an exception.&quot;, t);\n    }\n\n    runTasks ++;\n\n    // Check timeout every 64 tasks because nanoTime() is relatively expensive.\n    // XXX: Hard-coded value - will make it configurable if it is really a problem.\n    if ((runTasks & 0x3F) == 0) {\n        lastExecutionTime = ScheduledFutureTask.nanoTime();\n      if (lastExecutionTime >= deadline) {\n        break;\n      }\n    }\n\n    task = pollTask();\n    if (task == null) {\n     lastExecutionTime = ScheduledFutureTask.nanoTime();\n     break;\n    }\n  }\n\n  this.lastExecutionTime = lastExecutionTime;\n  return true;\n}\n```\n\n### 5、总结\n\n以上内容从设计和代码层面总结Netty线程模型的大致内容，中间有很多我的不成熟的思考与理解，请朋友轻拍与指正。\n\n看源码过程中是比较折磨人的。首先得了解你学习东西的业务价值是哪里？即你学了这个之后能用在哪里，只是不考虑场景仅仅为了看代码而看代码比较难以深入理解其内涵；其次，看代码一定一定得从逻辑、结构层面看，从细节层面看只会越陷越深，有种一叶障目不见泰山的感觉；最后，最好是能够将代码逻辑、结构画出来，或者整理出思维导图啥的，可以用以理清思路。前面两篇文章思维道路较为清晰，线程模型的导图有一些但是比较混乱，就不贴出来了，用作自己参考，有兴趣的可以找我要噢。\n","slug":"2014/01/netty-4-x-thread-model","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba920002h3x8f12pn122x"},{"title":"Netty 4.x学习笔记 - Channel和Pipeline","date":"2014-01-07T09:51:00.000Z","_content":"\n### 1、前言\n\nChannel概念与java.nio.channel概念一致，用以连接IO设备（socket、文件等）的纽带。Netty 4.x之后的Channel变化较大，官方的唬人的说法是无法通过简单的关键字替换进行迁移。用得较多应该是：ChannelHandler接口重新设计，换了个较为清晰的名字；write不会主动flush。由于笔者3.x、4.x都没用过，所以也无法深入理解版本的变化了。\n\n<!--more-->\n\n> 关于channel 4.x的新变化可以参考这里：new and noteworthy: Channel API changes\n\n### 2、Channel总览\n\n<center><div style=\"width: 80%;\">![Netty Channel整体结构思维导图](/images/wpid-Channel.png)</div></center>\n\nChannel的IO类型主要有两种：非阻塞IO（NIO）以及阻塞IO（OIO）；数据传输类型有两种：按事件消息传递（Message）以及按字节传递（Byte）；适用方类型也有两种：服务器（ServerSocket）以及客户端（Socket）。还有一些根据传输协议而制定的的Channel，如：UDT、SCTP等。\n\nNetty按照类型逐层设计相应的类。最底层的为抽象类AbstractChannel，再以此根据IO类型、数据传输类型、适用方类型实现。类图可以一目了然，如下图所示：\n\n<center><div style=\"width: 80%;\">![Netty Channel类图](/images/wpid-nio-oio.jpg)</div></center>\n\n### 3、ChannelPipeline实现分析\n\n从AbstractChannel分析，它提供了一些IO操作方法，read、write等，Channel仅仅做了一个封装，方法中将参数直接传递给了Channel的Pipeline成员的相应方法。\n\nPipeline则是Channel里面非常重要的概念。从数据结构的角度，它是一个双向链表，每个节点均是DefaultChannelHandlerContext对象；从逻辑的角度，它则是netty的逻辑处理链，每个节点均包含一个逻辑处理器（ChannelHandler），用以实现网络通信的编/解码、处理等功能。\n\nPipeline的链表上有两种handler，Inbound Handler和Outbound handler。从Netty内部IO线程接读到IO数据，依次经过N个Handler到达最内部的逻辑处理单元，这种称之为Inbound Handler；从Channel发出IO请求，依次经过M个Handler到达Netty内部IO线程，这种称之为Outbound Handler。内部代码实现流程则是：Head -> Tail (Inbound)，Tail -> Head (Outbound)。下图截取自ChannelPipeline的注释中，简单明了：\n\n<center><div style=\"width: 60%;\">![Netty Pipeline](/images/wpid-Netty-ChannelPipeline-.png)</div></center>\n\n### 4、逻辑处理器\n\nChannelPipeline仅仅只是逻辑处理的流程，真正逻辑处理器则是ChannelHandlerInvoker。在获得链表节点后，节点会调用自己的invoker成员执行(invoke)逻辑。\n\n``` java\npublic ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {\n    DefaultChannelHandlerContext next = findContextOutbound();\n    next.invoker.invokeWriteAndFlush(next, msg, promise);\n    return promise;\n}\n```\n\n在DefaultChannelHandlerInvoker中只有一个成员(executor)，执行逻辑的过程中，Invoker会先判断当前运行线程是否是executor，如果是则直接运行相应方法，不是则启动子线程运行相应方法。\n\n``` java\nprivate void invokeWrite(ChannelHandlerContext ctx, Object msg, boolean flush, ChannelPromise promise) {\n\n    if (executor.inEventLoop()) { // 判断是否是当前线程\n        invokeWriteNow(ctx, msg, promise);\n        if (flush) {\n            invokeFlushNow(ctx);\n        }\n    } else {\n        AbstractChannel channel = (AbstractChannel) ctx.channel();\n        int size = channel.estimatorHandle().size(msg);\n        if (size > 0) {\n            ChannelOutboundBuffer buffer = channel.unsafe().outboundBuffer();\n            // Check for null as it may be set to null if the channel is closed already\n            if (buffer != null) {\n                buffer.incrementPendingOutboundBytes(size);\n            }\n        }\n        // 创建一个新的WriteTask\n        // executor.execute(task);\n        safeExecuteOutbound(WriteTask.newInstance(ctx, msg, size, flush, promise), promise, msg);\n    }\n\n}\n```\n\nexecutor继承自[EventExecutor](http://netty.io/4.0/api/io/netty/util/concurrent/EventExecutor.html)，同时，该对象实现类一般而言也是实现[EventLoop](http://netty.io/4.0/api/io/netty/channel/EventLoop.html)接口。EventLoop的实现体现了Netty 4.x的IO线程模型，非常重要，后面再详细解析。\n\n#### 5、总结\n\n至此，上面简单总结了Channel以及Pipeline的处理流程。`Channel.write -> ChannelPipeline.write -> ChannelHandlerContext.write -> ChannelHandlerInvoker.write -> ChannelHandler.write`。在这个过程中，我也是捡简单的、流程性的代码总结，像EventLoop、EventExecutor这种核心部分并没有深入总结，压后再详细解说。\n","source":"_posts/2014/01/netty-4-x-channel-pipeline.md","raw":"title: Netty 4.x学习笔记 - Channel和Pipeline\ndate: 2014-01-07 17:51:00\ncategories: 技术分享\ntags: [Netty]\n---\n\n### 1、前言\n\nChannel概念与java.nio.channel概念一致，用以连接IO设备（socket、文件等）的纽带。Netty 4.x之后的Channel变化较大，官方的唬人的说法是无法通过简单的关键字替换进行迁移。用得较多应该是：ChannelHandler接口重新设计，换了个较为清晰的名字；write不会主动flush。由于笔者3.x、4.x都没用过，所以也无法深入理解版本的变化了。\n\n<!--more-->\n\n> 关于channel 4.x的新变化可以参考这里：new and noteworthy: Channel API changes\n\n### 2、Channel总览\n\n<center><div style=\"width: 80%;\">![Netty Channel整体结构思维导图](/images/wpid-Channel.png)</div></center>\n\nChannel的IO类型主要有两种：非阻塞IO（NIO）以及阻塞IO（OIO）；数据传输类型有两种：按事件消息传递（Message）以及按字节传递（Byte）；适用方类型也有两种：服务器（ServerSocket）以及客户端（Socket）。还有一些根据传输协议而制定的的Channel，如：UDT、SCTP等。\n\nNetty按照类型逐层设计相应的类。最底层的为抽象类AbstractChannel，再以此根据IO类型、数据传输类型、适用方类型实现。类图可以一目了然，如下图所示：\n\n<center><div style=\"width: 80%;\">![Netty Channel类图](/images/wpid-nio-oio.jpg)</div></center>\n\n### 3、ChannelPipeline实现分析\n\n从AbstractChannel分析，它提供了一些IO操作方法，read、write等，Channel仅仅做了一个封装，方法中将参数直接传递给了Channel的Pipeline成员的相应方法。\n\nPipeline则是Channel里面非常重要的概念。从数据结构的角度，它是一个双向链表，每个节点均是DefaultChannelHandlerContext对象；从逻辑的角度，它则是netty的逻辑处理链，每个节点均包含一个逻辑处理器（ChannelHandler），用以实现网络通信的编/解码、处理等功能。\n\nPipeline的链表上有两种handler，Inbound Handler和Outbound handler。从Netty内部IO线程接读到IO数据，依次经过N个Handler到达最内部的逻辑处理单元，这种称之为Inbound Handler；从Channel发出IO请求，依次经过M个Handler到达Netty内部IO线程，这种称之为Outbound Handler。内部代码实现流程则是：Head -> Tail (Inbound)，Tail -> Head (Outbound)。下图截取自ChannelPipeline的注释中，简单明了：\n\n<center><div style=\"width: 60%;\">![Netty Pipeline](/images/wpid-Netty-ChannelPipeline-.png)</div></center>\n\n### 4、逻辑处理器\n\nChannelPipeline仅仅只是逻辑处理的流程，真正逻辑处理器则是ChannelHandlerInvoker。在获得链表节点后，节点会调用自己的invoker成员执行(invoke)逻辑。\n\n``` java\npublic ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {\n    DefaultChannelHandlerContext next = findContextOutbound();\n    next.invoker.invokeWriteAndFlush(next, msg, promise);\n    return promise;\n}\n```\n\n在DefaultChannelHandlerInvoker中只有一个成员(executor)，执行逻辑的过程中，Invoker会先判断当前运行线程是否是executor，如果是则直接运行相应方法，不是则启动子线程运行相应方法。\n\n``` java\nprivate void invokeWrite(ChannelHandlerContext ctx, Object msg, boolean flush, ChannelPromise promise) {\n\n    if (executor.inEventLoop()) { // 判断是否是当前线程\n        invokeWriteNow(ctx, msg, promise);\n        if (flush) {\n            invokeFlushNow(ctx);\n        }\n    } else {\n        AbstractChannel channel = (AbstractChannel) ctx.channel();\n        int size = channel.estimatorHandle().size(msg);\n        if (size > 0) {\n            ChannelOutboundBuffer buffer = channel.unsafe().outboundBuffer();\n            // Check for null as it may be set to null if the channel is closed already\n            if (buffer != null) {\n                buffer.incrementPendingOutboundBytes(size);\n            }\n        }\n        // 创建一个新的WriteTask\n        // executor.execute(task);\n        safeExecuteOutbound(WriteTask.newInstance(ctx, msg, size, flush, promise), promise, msg);\n    }\n\n}\n```\n\nexecutor继承自[EventExecutor](http://netty.io/4.0/api/io/netty/util/concurrent/EventExecutor.html)，同时，该对象实现类一般而言也是实现[EventLoop](http://netty.io/4.0/api/io/netty/channel/EventLoop.html)接口。EventLoop的实现体现了Netty 4.x的IO线程模型，非常重要，后面再详细解析。\n\n#### 5、总结\n\n至此，上面简单总结了Channel以及Pipeline的处理流程。`Channel.write -> ChannelPipeline.write -> ChannelHandlerContext.write -> ChannelHandlerInvoker.write -> ChannelHandler.write`。在这个过程中，我也是捡简单的、流程性的代码总结，像EventLoop、EventExecutor这种核心部分并没有深入总结，压后再详细解说。\n","slug":"2014/01/netty-4-x-channel-pipeline","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba921002l3x8fzjtnij0q"},{"title":"Netty 4.x学习笔记 - ByteBuf","date":"2014-01-04T16:00:05.000Z","_content":"\n### 1、前言\n\n程序员喜欢说一句话：「不要重复造轮子」，但是程序员又不太会践行这句话。这倒也不是坏事，程序员一般而言看他人代码都不会太爽，这也可能是导致程序员的世界有各式各样的轮子的原因吧。\n\n### 2、ByteBuf与Java NIO Buffer\n\nByteBuf则是Java NIO Buffer的新轮子，官方列出了一些ByteBuf的特性：\n\n* 需要的话，可以自定义buffer类型；\n* 通过组合buffer类型，可实现透明的zero-copy；\n* 提供动态的buffer类型，如StringBuffer一样，容量是按需扩展；\n* 无需调用flip()方法；\n* 常常「often」比ByteBuffer快。\n\n>  参考地址：[Rich Buffer Data Structure](http://docs.jboss.org/netty/3.1/guide/html/architecture.html#d0e1893)\n\n### 3、ByteBuf实现类\n\nByteBuf提供了一些较为丰富的实现类，逻辑上主要分为两种：HeapByteBuf和DirectByteBuf，实现机制则分为两种：PooledByteBuf和UnpooledByteBuf，除了这些之外，Netty还实现了一些衍生ByteBuf（DerivedByteBuf），如：ReadOnlyByteBuf、DuplicatedByteBuf以及SlicedByteBuf。\n\nByteBuf实现类的类图如下：\n\n<center><div style=\"width: 80%;\">![Netty ByteBuf类图](/images/bytebuf-diagram.png)</div></center>\n\nHeapByteBuf和DirectByteBuf区别在于Buffer的管理方式：HeapByteBuf由Heap管理，Heap是Java堆的意思，内部实现直接采用byte[] array；DirectByteBuf使用是堆外内存，Direct应是采用Direct I/O之意，内部实现使用java.nio.DirectByteBuffoer。\n\n* [Direct I/O](http://www.ibm.com/developerworks/cn/linux/l-cn-directio/)\n* [DirectByteBuffer](http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html)\n\nPooledByteBuf和UnpooledByteBuf，UnpooledByteBuf实现就是普通的ByteBuf了，PooledByteBuf是4.x之后的新特性，稍后再说。\n\nDerivedByteBuf是ByteBuf衍生类，实现采用装饰器模式对原有的ByteBuf进行了一些封装。ReadOnlyByteBuf是某个ByteBuf的只读引用；DuplicatedByteBuf是某个ByteBuf对象的引用；SlicedByteBuf是某个ByteBuf的部分内容。\n\nSwappedByteBuf和CompositedByteBuf我觉得也算某种程度的衍生类吧，SwappedByteBuf封装了一个ByteBuf对象和ByteOrder对象，实现某个ByteBuf对象序列的逆转；CompositedByteBuf内部实现了一个ByteBuf列表，称之为组合ByteBuf，由于不懂相关的技术业务，无法理解该类的存在意义（官方解释：A user can save bulk memory copy operations using a composite buffer at the cost of relatively expensive random access.）。这两个类从逻辑上似乎完全可以继承于DerivedByteBuf，Trustin大神为啥如此设计呢？\n\n### 4、简要的ByteBuf的实现机制\n\n<center><div style=\"width: 80%;\">![Netty 实现机制](/images/bytebuf-priciple.png)</div></center>\n\nByteBuf有两个指针，readerIndex和writerIndex，用以控制buffer数组的读写。读逻辑较为简单，不考虑边界的情况下，就是`return array[readerIndex++];`。这里简要分析一下HeapByteBuf的读逻辑。\n\n 1. AbstractByteBuf.ensureWritable(minWritableBytes);\n 2. calculateNewCapacity(writerIndex + minWritableBytes)\n   2.1 判断是否超过可写入容量 maxCapacity – writerIndex\n   2.2 超过则抛异常，否则计算新容量 writerIndex + minWritableBytes\n   2.3 判断是否超过设定阈值(4MB)，超过每次增加按阈值(4MB)递增，否则\n   2.4 初始大小为64字节(newCapacity)，新容量超过newCapacity则翻倍，直到newCapacity大于新容量为止\n   2.5 返回Min(newCapacity, maxCapacity);\n 3. UnpooledHeapByteBuf.capacity(newCapacity);\n   3.1 确保可访问，有一个`引用计数`的机制，引用计数为0，则抛异常(ensureAccessible)\n   3.2 常规操作：判断是否越界\n   3.3 如果newCapacity比原容量大，则直接创建新数组，并设置。否则\n   3.4 如果readerIndex小于新容量，将readable bytes拷贝至新的数组，反之将readerIndex和writerIndex均设置为newCapacity。\n 4. setByte(writerIndex++, value)\n   4.1 确保可访问\n   4.2 设置\n\n### 5、ByteBuf特殊机制\n\n#### 5.1 Pooled\n\n4.x开发了Pooled Buffer，实现了一个高性能的buffer池，分配策略则是结合了buddy allocation和slab allocation的jemalloc变种，代码在io.netty.buffer.PoolArena。暂未深入研读。\n\n官方说提供了以下优势：\n\n* 频繁分配、释放buffer时减少了GC压力；\n* 在初始化新buffer时减少内存带宽消耗（初始化时不可避免的要给buffer数组赋初始值）；\n* 及时的释放direct buffer。\n\n当然，官方也说了不保证没有内存泄露，所以默认情况下还是采用的UnpooledByteBufAllocator。5.x还处于beta版，~~看它的「new and\\* noteworthy」文档也没说有啥变化，哈哈哈哈，~~查看最新的[「new and noteworthy」](http://netty.io/wiki/new-and-noteworthy-in-5.0.html)文档，PooledByteBufAllocator已经设置为默认的Allocator。\n\n#### 5.2 Reference Count\n\nByteBuf的生命周期管理引入了Reference Count的机制，感觉让我回到了CPP时代。可以通过简单的继承SimpleChannelInboundHandler实现自动释放reference count。SimpleChannelInboundHandler的事件方法如下，在消费完毕msg后，可以AutoRelease之：\n\n``` java\npublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {\n    boolean release = true;\n    try {\n        if (acceptInboundMessage(msg)) {\n            @SuppressWarnings(\"unchecked\")\n            I imsg = (I) msg;\n            messageReceived(ctx, imsg);\n        } else {\n            release = false;\n            ctx.fireChannelRead(msg);\n        }\n    } finally {\n        if (autoRelease && release) {\n            ReferenceCountUtil.release(msg);\n        }\n    }\n}\n```\n\n这一小节可以单独拎出来和Pooled放在一起深入研读研读，有兴趣的可以先看看官方文档：[Reference counted objects](http://netty.io/wiki/reference-counted-objects.html)\n\n#### 5.3 Zero Copy\n\nZero-copy与传统意义的[zero-copy](http://en.wikipedia.org/wiki/Zero-copy)不太一样。传统的zero-copy是IO传输过程中，数据无需中内核态到用户态、用户态到内核态的数据拷贝，减少拷贝次数。而Netty的zero-copy则是完全在用户态，或者说传输层的zero-copy机制，可以参考下图。由于协议传输过程中，通常会有拆包、合并包的过程，一般的做法就是System.arrayCopy了，但是Netty通过ByteBuf.slice以及Unpooled.wrappedBuffer等方法拆分、合并Buffer无需拷贝数据。\n\n如何实现zero-copy的呢。slice实现就是创建一个SlicedByteBuf对象，将this对象，以及相应的数据指针传入即可，wrappedBuffer实现机制类似。\n\n<center><div style=\"width: 70%;\">![Netty Bytebuf](/images/bytebuf-combine-slice-buffer.png)</div></center>\n\n> 参考地址：[Combining and Slicing ChannelBuffers](http://netty.io/3.5/guide/#architecture.5.4)\n","source":"_posts/2014/01/netty-4-x-bytebuf.md","raw":"title: Netty 4.x学习笔记 - ByteBuf\ndate: 2014-01-05 00:00:05\ncategories: 技术分享\ntags: [Netty]\n---\n\n### 1、前言\n\n程序员喜欢说一句话：「不要重复造轮子」，但是程序员又不太会践行这句话。这倒也不是坏事，程序员一般而言看他人代码都不会太爽，这也可能是导致程序员的世界有各式各样的轮子的原因吧。\n\n### 2、ByteBuf与Java NIO Buffer\n\nByteBuf则是Java NIO Buffer的新轮子，官方列出了一些ByteBuf的特性：\n\n* 需要的话，可以自定义buffer类型；\n* 通过组合buffer类型，可实现透明的zero-copy；\n* 提供动态的buffer类型，如StringBuffer一样，容量是按需扩展；\n* 无需调用flip()方法；\n* 常常「often」比ByteBuffer快。\n\n>  参考地址：[Rich Buffer Data Structure](http://docs.jboss.org/netty/3.1/guide/html/architecture.html#d0e1893)\n\n### 3、ByteBuf实现类\n\nByteBuf提供了一些较为丰富的实现类，逻辑上主要分为两种：HeapByteBuf和DirectByteBuf，实现机制则分为两种：PooledByteBuf和UnpooledByteBuf，除了这些之外，Netty还实现了一些衍生ByteBuf（DerivedByteBuf），如：ReadOnlyByteBuf、DuplicatedByteBuf以及SlicedByteBuf。\n\nByteBuf实现类的类图如下：\n\n<center><div style=\"width: 80%;\">![Netty ByteBuf类图](/images/bytebuf-diagram.png)</div></center>\n\nHeapByteBuf和DirectByteBuf区别在于Buffer的管理方式：HeapByteBuf由Heap管理，Heap是Java堆的意思，内部实现直接采用byte[] array；DirectByteBuf使用是堆外内存，Direct应是采用Direct I/O之意，内部实现使用java.nio.DirectByteBuffoer。\n\n* [Direct I/O](http://www.ibm.com/developerworks/cn/linux/l-cn-directio/)\n* [DirectByteBuffer](http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html)\n\nPooledByteBuf和UnpooledByteBuf，UnpooledByteBuf实现就是普通的ByteBuf了，PooledByteBuf是4.x之后的新特性，稍后再说。\n\nDerivedByteBuf是ByteBuf衍生类，实现采用装饰器模式对原有的ByteBuf进行了一些封装。ReadOnlyByteBuf是某个ByteBuf的只读引用；DuplicatedByteBuf是某个ByteBuf对象的引用；SlicedByteBuf是某个ByteBuf的部分内容。\n\nSwappedByteBuf和CompositedByteBuf我觉得也算某种程度的衍生类吧，SwappedByteBuf封装了一个ByteBuf对象和ByteOrder对象，实现某个ByteBuf对象序列的逆转；CompositedByteBuf内部实现了一个ByteBuf列表，称之为组合ByteBuf，由于不懂相关的技术业务，无法理解该类的存在意义（官方解释：A user can save bulk memory copy operations using a composite buffer at the cost of relatively expensive random access.）。这两个类从逻辑上似乎完全可以继承于DerivedByteBuf，Trustin大神为啥如此设计呢？\n\n### 4、简要的ByteBuf的实现机制\n\n<center><div style=\"width: 80%;\">![Netty 实现机制](/images/bytebuf-priciple.png)</div></center>\n\nByteBuf有两个指针，readerIndex和writerIndex，用以控制buffer数组的读写。读逻辑较为简单，不考虑边界的情况下，就是`return array[readerIndex++];`。这里简要分析一下HeapByteBuf的读逻辑。\n\n 1. AbstractByteBuf.ensureWritable(minWritableBytes);\n 2. calculateNewCapacity(writerIndex + minWritableBytes)\n   2.1 判断是否超过可写入容量 maxCapacity – writerIndex\n   2.2 超过则抛异常，否则计算新容量 writerIndex + minWritableBytes\n   2.3 判断是否超过设定阈值(4MB)，超过每次增加按阈值(4MB)递增，否则\n   2.4 初始大小为64字节(newCapacity)，新容量超过newCapacity则翻倍，直到newCapacity大于新容量为止\n   2.5 返回Min(newCapacity, maxCapacity);\n 3. UnpooledHeapByteBuf.capacity(newCapacity);\n   3.1 确保可访问，有一个`引用计数`的机制，引用计数为0，则抛异常(ensureAccessible)\n   3.2 常规操作：判断是否越界\n   3.3 如果newCapacity比原容量大，则直接创建新数组，并设置。否则\n   3.4 如果readerIndex小于新容量，将readable bytes拷贝至新的数组，反之将readerIndex和writerIndex均设置为newCapacity。\n 4. setByte(writerIndex++, value)\n   4.1 确保可访问\n   4.2 设置\n\n### 5、ByteBuf特殊机制\n\n#### 5.1 Pooled\n\n4.x开发了Pooled Buffer，实现了一个高性能的buffer池，分配策略则是结合了buddy allocation和slab allocation的jemalloc变种，代码在io.netty.buffer.PoolArena。暂未深入研读。\n\n官方说提供了以下优势：\n\n* 频繁分配、释放buffer时减少了GC压力；\n* 在初始化新buffer时减少内存带宽消耗（初始化时不可避免的要给buffer数组赋初始值）；\n* 及时的释放direct buffer。\n\n当然，官方也说了不保证没有内存泄露，所以默认情况下还是采用的UnpooledByteBufAllocator。5.x还处于beta版，~~看它的「new and\\* noteworthy」文档也没说有啥变化，哈哈哈哈，~~查看最新的[「new and noteworthy」](http://netty.io/wiki/new-and-noteworthy-in-5.0.html)文档，PooledByteBufAllocator已经设置为默认的Allocator。\n\n#### 5.2 Reference Count\n\nByteBuf的生命周期管理引入了Reference Count的机制，感觉让我回到了CPP时代。可以通过简单的继承SimpleChannelInboundHandler实现自动释放reference count。SimpleChannelInboundHandler的事件方法如下，在消费完毕msg后，可以AutoRelease之：\n\n``` java\npublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {\n    boolean release = true;\n    try {\n        if (acceptInboundMessage(msg)) {\n            @SuppressWarnings(\"unchecked\")\n            I imsg = (I) msg;\n            messageReceived(ctx, imsg);\n        } else {\n            release = false;\n            ctx.fireChannelRead(msg);\n        }\n    } finally {\n        if (autoRelease && release) {\n            ReferenceCountUtil.release(msg);\n        }\n    }\n}\n```\n\n这一小节可以单独拎出来和Pooled放在一起深入研读研读，有兴趣的可以先看看官方文档：[Reference counted objects](http://netty.io/wiki/reference-counted-objects.html)\n\n#### 5.3 Zero Copy\n\nZero-copy与传统意义的[zero-copy](http://en.wikipedia.org/wiki/Zero-copy)不太一样。传统的zero-copy是IO传输过程中，数据无需中内核态到用户态、用户态到内核态的数据拷贝，减少拷贝次数。而Netty的zero-copy则是完全在用户态，或者说传输层的zero-copy机制，可以参考下图。由于协议传输过程中，通常会有拆包、合并包的过程，一般的做法就是System.arrayCopy了，但是Netty通过ByteBuf.slice以及Unpooled.wrappedBuffer等方法拆分、合并Buffer无需拷贝数据。\n\n如何实现zero-copy的呢。slice实现就是创建一个SlicedByteBuf对象，将this对象，以及相应的数据指针传入即可，wrappedBuffer实现机制类似。\n\n<center><div style=\"width: 70%;\">![Netty Bytebuf](/images/bytebuf-combine-slice-buffer.png)</div></center>\n\n> 参考地址：[Combining and Slicing ChannelBuffers](http://netty.io/3.5/guide/#architecture.5.4)\n","slug":"2014/01/netty-4-x-bytebuf","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba923002o3x8fspszqe9m"},{"title":"小菜看双11大促","id":"875","date":"2013-11-17T15:49:49.000Z","_content":"\n### 1、前言\n\n本人小菜在支付宝数据平台实习半年，主要业务均是离线场景，原以为今年是刷不上双11了，但幸运的是，运营支撑部门准备开发一套线上场景的应用，需要用到数据平台这边的系统，更幸运的是，我负责了部分数据平台这边的部分数据出口系统。该套应用双11当天也需要使用，所以呢，我也算是凑了凑双11开发的热闹。\n\n<!--more-->  \n\n在这过程虽然对于双11相关的开发事宜了解不是很多，但是或多或少有所耳闻，在这里将我的一些心得体会记录下来。\n\n### 2、内部机制\n\n我面向的这个应用的开发流程和普通的应用开发流程基本一致，提前2-3个月开发完毕，开发周期约1个半月。\n\n开发需要做的第一件事：需要与各个关联应用的owner沟通协商，沟通内容大概就是需求是否能够满足；平均TPS/峰值TPS大约多少；如果需要开发周期多长，我负责的主要数据相关的接口，结果是需求暂时无法满足，得重新开发新接口；TPS似乎有点高，得压测后评估新的服务器容量；半个人月开发一周测试。这个结果也还算okay，基本就是时间的问题，与他们协商好新接口以及联调的时间即可自行开发了。\n\n开发需要做的第二件事：具体开发过程大同小异了，开发完毕后需要进行联调以及测试，同时要对新的接口进行压测，压测完毕后才是重点——服务器容量水位评估，用以评估在当前的水平下，需要多少台机器才能够撑得住预估TPS。具体的评估方法在这里就不细说了，阿里技术嘉年华中有篇PPT，就讲了容量水位的评估，《[如何利用应用自己的数据来保证系统的稳定](http://club.alibabatech.org/resource_detail.htm?topicId=81)》，和我用的类似但不完全相同。\n\n开发需要做的第三件事：联调完毕，压测完毕，机器扩容也完毕。在我这个小菜看来，我的系统就可以发布了，后经运维再三提醒与要求，给接口加上了服务开发，方便在各个活动中进行服务的升降级。公司内部有自己的分布式资源管理组件，可很方便的实现应用开关，基本的实现思路可以看一下这篇博客：《[java分布式系统开关功能设计](http://iamzhongyong.iteye.com/blog/1897694)》\n\n由于接的这个项目并非支付宝关键链路上的系统，所以系统优先级会比较低，我上层的应用做的限流机制也比较轻便。业务方会评估我系统的应用容量，配置报警机制，在超过容量阈值的时候报警，同时人工干预进行系统限流。具体的限流实现也较为简单，分布式资源管理系统推送限流概率，比如说0.9，业务则会通过Random过滤掉约为10%的请求，直接抛弃掉，好像也很弱的样子。当然，这是属于应用级的限流，也有系统级的限流，比如淘宝那边的TMD，支付宝这边的SLA，都没有深入了解，如TMD是nginx的一个组件，应该是在接入层做的限流，如限流控制在500TPS，那么第501个请求则直接抛弃。运维方面，在双11前几天也会提前查看机器状态信息，如负载、请求量、日志情况，针对不同的情况作出不同的调整，比如动态给数据库集群增添机器、修改报警阈值等。\n\n### 3、后记\n\n以上便是我这个双11相关工作周边游荡的编外人员所接触到的双11内部机制，无论是理解还是实践都比较肤浅，不过还有机会深入双11核心工作滴，木哈哈哈！\n\n> 双11前两天我那系统的请求量还是挺可观的，比平常翻了十翻，但是双11当天请求量却掉回解放前。应该是被限流了，不是关键链路上的就是悲惨啊！！！\n","source":"_posts/2013/11/what-i-think-about-1111.md","raw":"title: 小菜看双11大促\ntags:\n  - 双11\nid: 875\ncategories:\n  - 技术分享\ndate: 2013-11-17 23:49:49\n---\n\n### 1、前言\n\n本人小菜在支付宝数据平台实习半年，主要业务均是离线场景，原以为今年是刷不上双11了，但幸运的是，运营支撑部门准备开发一套线上场景的应用，需要用到数据平台这边的系统，更幸运的是，我负责了部分数据平台这边的部分数据出口系统。该套应用双11当天也需要使用，所以呢，我也算是凑了凑双11开发的热闹。\n\n<!--more-->  \n\n在这过程虽然对于双11相关的开发事宜了解不是很多，但是或多或少有所耳闻，在这里将我的一些心得体会记录下来。\n\n### 2、内部机制\n\n我面向的这个应用的开发流程和普通的应用开发流程基本一致，提前2-3个月开发完毕，开发周期约1个半月。\n\n开发需要做的第一件事：需要与各个关联应用的owner沟通协商，沟通内容大概就是需求是否能够满足；平均TPS/峰值TPS大约多少；如果需要开发周期多长，我负责的主要数据相关的接口，结果是需求暂时无法满足，得重新开发新接口；TPS似乎有点高，得压测后评估新的服务器容量；半个人月开发一周测试。这个结果也还算okay，基本就是时间的问题，与他们协商好新接口以及联调的时间即可自行开发了。\n\n开发需要做的第二件事：具体开发过程大同小异了，开发完毕后需要进行联调以及测试，同时要对新的接口进行压测，压测完毕后才是重点——服务器容量水位评估，用以评估在当前的水平下，需要多少台机器才能够撑得住预估TPS。具体的评估方法在这里就不细说了，阿里技术嘉年华中有篇PPT，就讲了容量水位的评估，《[如何利用应用自己的数据来保证系统的稳定](http://club.alibabatech.org/resource_detail.htm?topicId=81)》，和我用的类似但不完全相同。\n\n开发需要做的第三件事：联调完毕，压测完毕，机器扩容也完毕。在我这个小菜看来，我的系统就可以发布了，后经运维再三提醒与要求，给接口加上了服务开发，方便在各个活动中进行服务的升降级。公司内部有自己的分布式资源管理组件，可很方便的实现应用开关，基本的实现思路可以看一下这篇博客：《[java分布式系统开关功能设计](http://iamzhongyong.iteye.com/blog/1897694)》\n\n由于接的这个项目并非支付宝关键链路上的系统，所以系统优先级会比较低，我上层的应用做的限流机制也比较轻便。业务方会评估我系统的应用容量，配置报警机制，在超过容量阈值的时候报警，同时人工干预进行系统限流。具体的限流实现也较为简单，分布式资源管理系统推送限流概率，比如说0.9，业务则会通过Random过滤掉约为10%的请求，直接抛弃掉，好像也很弱的样子。当然，这是属于应用级的限流，也有系统级的限流，比如淘宝那边的TMD，支付宝这边的SLA，都没有深入了解，如TMD是nginx的一个组件，应该是在接入层做的限流，如限流控制在500TPS，那么第501个请求则直接抛弃。运维方面，在双11前几天也会提前查看机器状态信息，如负载、请求量、日志情况，针对不同的情况作出不同的调整，比如动态给数据库集群增添机器、修改报警阈值等。\n\n### 3、后记\n\n以上便是我这个双11相关工作周边游荡的编外人员所接触到的双11内部机制，无论是理解还是实践都比较肤浅，不过还有机会深入双11核心工作滴，木哈哈哈！\n\n> 双11前两天我那系统的请求量还是挺可观的，比平常翻了十翻，但是双11当天请求量却掉回解放前。应该是被限流了，不是关键链路上的就是悲惨啊！！！\n","slug":"2013/11/what-i-think-about-1111","published":1,"updated":"2015-12-30T14:37:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba926002r3x8fb3ujanur"},{"title":"记一次java性能问题定位","id":"861","date":"2013-10-22T15:21:52.000Z","_content":"\n**1、前言**\n\n\t前一段时间检查集群状态时，发现某部分机器的load较高，故登录服务器查看，某几个java进程的cpu使用率为1000%，没见过这么高的cpu时间，顿时就长见识了！长完见识问题还是要解决的，故本文记录下问题定位的过程。\n\n\t<!--more-->\n\n\t**2、定位流程**\n\n\t**2.1 定位问题进程**\n\n\ttop命令可以简单定位进程pid，如下：\n\n\t[![Image](http://hongweiyi.com/wp-content/uploads/2013/10/Image_thumb.png \"Image\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image.png)\n\n\tjps -vm | grep 15195 可以查看java进程的参数或者日志地址等，如果没有显示参数的话，可以cd到/proc/15195/cwd目录，该目录便是进程的运行目录。\n\n\t**2.2 查看日志**\n\n\t常规做法就是查看日志了，但是扫了几遍日志也没发现问题，因为这个进程这几天的请求都不是很多，难道是线程空转了？\n\n\t**2.3 定位问题线程**\n\n\t既然日志没有发现异常，那只是通过查看进程内部发现问题了。man top可以看到top命令的详细信息，-H则是线程开关，传入该参数的话，top界面会显示所有单独的线程列表。\n\n\t[![Image(1)](http://hongweiyi.com/wp-content/uploads/2013/10/Image1_thumb.png \"Image(1)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image1.png)\n\n\t不看不知道，一看吓一跳啊，cpu跑满的线程挺多的，第一列便是他们的线程id。\n\n\t**2.4 Thread dump**\n\n\t拿到异常的线程id后，便可以将该进程的线程栈全部输出了，用到的工具是jstack。\n\n\tjstack 15195 &gt; jstack.15195.dump\n\n\t[![Image(2)](http://hongweiyi.com/wp-content/uploads/2013/10/Image2_thumb.png \"Image(2)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image2.png)\n\n\t快速搜索的话，可以直接拿pid转换成16进程定位，当然，也可以慢慢看那些线程处于RUNNABLE状态，不过定位问题较慢。\n\n\t通过查询异常线程pid，发现所有的都是Parallel GC Threads，实在是太奇怪了。\n\n\t**2.5 查看gc状态**\n\n\tjstat -gc 15195 获得当前进程的gc状态，会发现该线程在不断的进行FullGC操作：\n\n\t[![Image(3)](http://hongweiyi.com/wp-content/uploads/2013/10/Image3_thumb.png \"Image(3)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image3.png)\n\n\t[![Image(4)](http://hongweiyi.com/wp-content/uploads/2013/10/Image4_thumb.png \"Image(4)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image4.png)\n\n\t短短几分钟，就FGC了28次！初步定位问题为FGC问题。\n\n> 注：用jstat -gcutil $PID $INTERVAL $TIMES查询可能会更直接，我查到这里应用被停止了，就木有现场了。\n\n\t**3、问题解决**\n\n\t现在出现的问题就是表现在了full gc次数频繁，从上面的应用而言，可以发现PU(PermGen Usage)占用非常高，约为95.7%。由于Perm代过高，且CMS GC无法回收掉Perm区内容，而导致频繁GC。\n\n\tCMS GC与普通的STW Full GC不同，不会暂停应用，但是会导致CPU使用率非常高。\n\n\t解决方法有两种：1）提高Perm区大小，-XX:PermSize -XX:MaxPermSize，2）关掉Perm区收集机制，取消-XX:+CMSClassUnloadingEnabled。\n\n\t这里有关于Perm区GC时机的深入且详细的解释，[http://rednaxelafx.iteye.com/blog/1108439](http://rednaxelafx.iteye.com/blog/1108439)\n\n> CMS收集器的使用好像有很多很多的优(jiu)化(shi)点(keng)啊！\r> \n> \n> \t\t还得深入学习实践一下！\n","source":"_posts/2013/10/once-java-profiling.md","raw":"title: 记一次java性能问题定位\ntags:\n  - Java\n  - JVM\nid: 861\ncategories:\n  - 技术分享\ndate: 2013-10-22 23:21:52\n---\n\n**1、前言**\n\n\t前一段时间检查集群状态时，发现某部分机器的load较高，故登录服务器查看，某几个java进程的cpu使用率为1000%，没见过这么高的cpu时间，顿时就长见识了！长完见识问题还是要解决的，故本文记录下问题定位的过程。\n\n\t<!--more-->\n\n\t**2、定位流程**\n\n\t**2.1 定位问题进程**\n\n\ttop命令可以简单定位进程pid，如下：\n\n\t[![Image](http://hongweiyi.com/wp-content/uploads/2013/10/Image_thumb.png \"Image\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image.png)\n\n\tjps -vm | grep 15195 可以查看java进程的参数或者日志地址等，如果没有显示参数的话，可以cd到/proc/15195/cwd目录，该目录便是进程的运行目录。\n\n\t**2.2 查看日志**\n\n\t常规做法就是查看日志了，但是扫了几遍日志也没发现问题，因为这个进程这几天的请求都不是很多，难道是线程空转了？\n\n\t**2.3 定位问题线程**\n\n\t既然日志没有发现异常，那只是通过查看进程内部发现问题了。man top可以看到top命令的详细信息，-H则是线程开关，传入该参数的话，top界面会显示所有单独的线程列表。\n\n\t[![Image(1)](http://hongweiyi.com/wp-content/uploads/2013/10/Image1_thumb.png \"Image(1)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image1.png)\n\n\t不看不知道，一看吓一跳啊，cpu跑满的线程挺多的，第一列便是他们的线程id。\n\n\t**2.4 Thread dump**\n\n\t拿到异常的线程id后，便可以将该进程的线程栈全部输出了，用到的工具是jstack。\n\n\tjstack 15195 &gt; jstack.15195.dump\n\n\t[![Image(2)](http://hongweiyi.com/wp-content/uploads/2013/10/Image2_thumb.png \"Image(2)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image2.png)\n\n\t快速搜索的话，可以直接拿pid转换成16进程定位，当然，也可以慢慢看那些线程处于RUNNABLE状态，不过定位问题较慢。\n\n\t通过查询异常线程pid，发现所有的都是Parallel GC Threads，实在是太奇怪了。\n\n\t**2.5 查看gc状态**\n\n\tjstat -gc 15195 获得当前进程的gc状态，会发现该线程在不断的进行FullGC操作：\n\n\t[![Image(3)](http://hongweiyi.com/wp-content/uploads/2013/10/Image3_thumb.png \"Image(3)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image3.png)\n\n\t[![Image(4)](http://hongweiyi.com/wp-content/uploads/2013/10/Image4_thumb.png \"Image(4)\")](http://hongweiyi.com/wp-content/uploads/2013/10/Image4.png)\n\n\t短短几分钟，就FGC了28次！初步定位问题为FGC问题。\n\n> 注：用jstat -gcutil $PID $INTERVAL $TIMES查询可能会更直接，我查到这里应用被停止了，就木有现场了。\n\n\t**3、问题解决**\n\n\t现在出现的问题就是表现在了full gc次数频繁，从上面的应用而言，可以发现PU(PermGen Usage)占用非常高，约为95.7%。由于Perm代过高，且CMS GC无法回收掉Perm区内容，而导致频繁GC。\n\n\tCMS GC与普通的STW Full GC不同，不会暂停应用，但是会导致CPU使用率非常高。\n\n\t解决方法有两种：1）提高Perm区大小，-XX:PermSize -XX:MaxPermSize，2）关掉Perm区收集机制，取消-XX:+CMSClassUnloadingEnabled。\n\n\t这里有关于Perm区GC时机的深入且详细的解释，[http://rednaxelafx.iteye.com/blog/1108439](http://rednaxelafx.iteye.com/blog/1108439)\n\n> CMS收集器的使用好像有很多很多的优(jiu)化(shi)点(keng)啊！\r> \n> \n> \t\t还得深入学习实践一下！\n","slug":"2013/10/once-java-profiling","published":1,"updated":"2015-12-29T15:01:45.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba928002v3x8fivkzerg8"},{"title":"Solr Query Parser学习有感","id":"845","date":"2013-08-31T02:55:12.000Z","_content":"\n### 1、前言\n\n早上了解QueryParser的时候，发现了一个有意思的插件-SwitchQueryParser。看名字有种高端大气上档次的感觉，但是用起来好像就没这种感觉了，因为好像功能确实有限，这插件只是提供了一种更为&ldquo;友好&rdquo;的filter query功能。\n\n<!--more-->\n\n\n### 2、细说\n\n插件的语法如下：\n\n> fq={!switch case.foo=XXX case.bar=zzz case.yak=qqq}foo\n\n这个语法是官方文档里面的，这算个什么嘛，还真只是语法，一点逻辑都没有！！！我就写了带一点点逻辑的查询语句如下：\n\n> fq={!switch case.cs=city:changsha case.bj=city:beijing}cs\n\n这句话翻译过来，其实就是：\n\n> fq=city:changsha\n\n因为在SwitchQParserPlugin的parse()中，只是找到了localParams的v属性（即cs），再从SWITCH_CASE对应的字段，获得相应的查询语句后，返回子查询对象。由于Solr模块可移植性强加上逻辑实在是过于简单，所以做低版本移植也非常方便，几(M&eacute;i)分(yǒu)钟(y&agrave;n)搞(zh&egrave;ng)定(ā)。\n\n如下：\n\n![image](/images/2013/08/image.png)\n\n部分源码：\n\n``` java\n/** SwitchQParserPlugin */\n@Override\npublic Query parse() throws SyntaxError {\nString val = localParams.get(QueryParsing.V);\n\n// we don't want to wrapDefaults arround params, because then\n// clients could add their own switch options\nString subQ = localParams.get(SWITCH_DEFAULT);\nsubQ = StringUtils.isBlank(val)\n  ? localParams.get(SWITCH_CASE, subQ)\n  : localParams.get(SWITCH_CASE + &quot;.&quot; + val.trim(), subQ);\n\nif (null == subQ) {\n  throw new SyntaxError(&quot;No &quot;+SWITCH_DEFAULT+&quot;, and no switch case matching specified query string: \\&quot;&quot; + val + &quot;\\&quot;&quot;);\n}\n\nsubParser = subQuery(subQ, null);\nreturn subParser.getQuery();\n}\n```\n\n### 3、有感\n\n这个插件的使用场景我觉得更多的是在业务层使用，由业务层封装多个case，将多个case的值暴露给更上层的业务，提供一个较为友好的交互方式。比如下面这种（肯定实际不是这么做的）：\n\n![QQ图片20130831103315](/images/2013/08/QQ20130831103315.jpg)\n\n总的来说，这个插件对于我们现在的系统而言，用处不大，但是Solr的这种查询方式，这种插件式编码方式，很值得我们借鉴和学习。\n","source":"_posts/2013/08/solr-switch-query-parser.md","raw":"title: Solr Query Parser学习有感\ntags:\n  - Solr\nid: 845\ncategories:\n  - 技术分享\ndate: 2013-08-31 10:55:12\n---\n\n### 1、前言\n\n早上了解QueryParser的时候，发现了一个有意思的插件-SwitchQueryParser。看名字有种高端大气上档次的感觉，但是用起来好像就没这种感觉了，因为好像功能确实有限，这插件只是提供了一种更为&ldquo;友好&rdquo;的filter query功能。\n\n<!--more-->\n\n\n### 2、细说\n\n插件的语法如下：\n\n> fq={!switch case.foo=XXX case.bar=zzz case.yak=qqq}foo\n\n这个语法是官方文档里面的，这算个什么嘛，还真只是语法，一点逻辑都没有！！！我就写了带一点点逻辑的查询语句如下：\n\n> fq={!switch case.cs=city:changsha case.bj=city:beijing}cs\n\n这句话翻译过来，其实就是：\n\n> fq=city:changsha\n\n因为在SwitchQParserPlugin的parse()中，只是找到了localParams的v属性（即cs），再从SWITCH_CASE对应的字段，获得相应的查询语句后，返回子查询对象。由于Solr模块可移植性强加上逻辑实在是过于简单，所以做低版本移植也非常方便，几(M&eacute;i)分(yǒu)钟(y&agrave;n)搞(zh&egrave;ng)定(ā)。\n\n如下：\n\n![image](/images/2013/08/image.png)\n\n部分源码：\n\n``` java\n/** SwitchQParserPlugin */\n@Override\npublic Query parse() throws SyntaxError {\nString val = localParams.get(QueryParsing.V);\n\n// we don't want to wrapDefaults arround params, because then\n// clients could add their own switch options\nString subQ = localParams.get(SWITCH_DEFAULT);\nsubQ = StringUtils.isBlank(val)\n  ? localParams.get(SWITCH_CASE, subQ)\n  : localParams.get(SWITCH_CASE + &quot;.&quot; + val.trim(), subQ);\n\nif (null == subQ) {\n  throw new SyntaxError(&quot;No &quot;+SWITCH_DEFAULT+&quot;, and no switch case matching specified query string: \\&quot;&quot; + val + &quot;\\&quot;&quot;);\n}\n\nsubParser = subQuery(subQ, null);\nreturn subParser.getQuery();\n}\n```\n\n### 3、有感\n\n这个插件的使用场景我觉得更多的是在业务层使用，由业务层封装多个case，将多个case的值暴露给更上层的业务，提供一个较为友好的交互方式。比如下面这种（肯定实际不是这么做的）：\n\n![QQ图片20130831103315](/images/2013/08/QQ20130831103315.jpg)\n\n总的来说，这个插件对于我们现在的系统而言，用处不大，但是Solr的这种查询方式，这种插件式编码方式，很值得我们借鉴和学习。\n","slug":"2013/08/solr-switch-query-parser","published":1,"updated":"2015-12-29T16:41:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92a002z3x8flunhwmw0"},{"title":"Apache Solr —— DistributedSearch","date":"2013-04-08T10:04:00.000Z","_content":"\n### 1、前言\n\n当索引太大以致单台服务器的磁盘无法承受了，当一个简单的查询实在要耗费过多的时间，可以考虑使用Solr的分布式索引机制，或者配置一台多核机制（multicore）。当Solr配置了这样的机制，实质上就是将大索引分成了多个小索引分布在了不同服务器上，或者将请求发到多核，充分利用服务器CPU资源。\n\nSolr会将请求请求分发不同shards（理解为地址）上，并合并所有请求结果并返回给客户端。那么，这个分布式查询内部是怎么实现的呢？\n\n<!--more-->\n\n### 2、机制解析\n\n一个SearchComponent如果作为Distributed SearchComponent，需要重写以下四个方法：\n\n* distributedProcess()\n* modifyRequest()\n* handleResponses()\n* finishStage()\n\n同时，Distributed Search主要有4个主要阶段：\n\n* **Start**(ResponseBuilder.STAGE_START)\n* **Query Parse**(ResponseBuilder.STAGE_PARSE_QUERY)\n* **Execute Query**(ResponseBuilder.STAGE_EXECUTE_QUERY)\n* **Get Fields**(ResponseBuilder.STAGE_GET_FIELDS)</blockquote>\n* **Done**(ResponseBuilder.STAGE_DONE)。\n\n在SearchHandler中，基本的**分布式查询算法**如下：\n\n#### 1. 如果不在STAGE_DONE的阶段，循环以下流程；\n#### 2. 发起分布式处理的组件会不断查询是否需要进行分布式处理。如果有的话（distributedProcess），返回4个阶段中一个状态并创建一个 ShardRequest并将它添加到一个队列（rb.outgoing)中；\n\n> Components可以指定一些purpose字段，可以定制一些特殊的请求处理。如下：\n\n``` java\npublic void handleResponses(ResponseBuilder rb, ShardRequest sreq) {\n  if (!rb.doFacets )\n     return;\n\n  if ((sreq.purpose & ShardRequest.PURPOSE_GET_FACETS) != 0) {\n     countFacets(rb, sreq);\n  } else if ((sreq.purpose & ShardRequest.PURPOSE_REFINE_FACETS ) != 0) {\n     refineFacets(rb, sreq);\n  }\n}\n```\n\n> modifyRequest()是用来精简shard请求的，该方法都是加在了ResponseBuilder.addRequest()中，实例可以参考FacetComponent.modifyRequest。\n\n``` java\npublic void addRequest(SearchComponent me, ShardRequest sreq) {\n  outgoing.add(sreq);\n  if ((sreq.purpose & ShardRequest.PURPOSE_PRIVATE)==0) {\n   // if this isn't a private request, let other components modify it.\n   for (SearchComponent component : components) {\n      if (component != me) {\n        component.modifyRequest( this, me, sreq);\n      }\n    }\n  }\n}\n```\n\n#### 3. 取出队列（rb.outgoing)中所有 ShardRequest，并根据其配置将请求发到相应的 Shards；\n#### 4. 如果队列为空，则等待接收到响应，并处理（handleResponse)；\n\n> 你可以合并文档id神马的。QueryComponent.handleResponse\n\n#### 5. 进入下一轮循环之前，先调用finishStage()\n\n> 进行该轮的收尾工作，比如说将为null的doc从responseDocs中移除。\n\n``` java\n// SearchHandler.handleRequestBody()\nif (rb.shards == null) {\n  // a normal non-distributed request\n  // ...\n} else {\n  // a distributed request\n\n  HttpCommComponent comm = new HttpCommComponent();\n\n  if (rb.outgoing == null ) {\n    rb.outgoing = new LinkedList<ShardRequest>();\n  }\n  rb.finished = new ArrayList<ShardRequest>();\n\n  int nextStage = 0;\n  do {\n    rb.stage = nextStage;\n    nextStage = ResponseBuilder.STAGE_DONE;\n\n    // call all components\n    for( SearchComponent c : components ) {\n      // the next stage is the minimum of what all components report\n      nextStage = Math.min(nextStage, c.distributedProcess(rb));\n    }\n\n\n    // check the outgoing queue and send requests\n    while (rb.outgoing.size() > 0) {\n\n      // submit all current request tasks at once\n      while (rb.outgoing.size() > 0) {\n        ShardRequest sreq = rb.outgoing.remove(0);\n        sreq.actualShards = sreq.shards ;\n        if (sreq.actualShards ==ShardRequest.ALL_SHARDS) {\n          sreq.actualShards = rb.shards ;\n        }\n        sreq.responses = new ArrayList<ShardResponse>();\n\n        // TODO: map from shard to address[]\n        for (String shard : sreq.actualShards ) {\n          ModifiableSolrParams params = new ModifiableSolrParams(sreq.params );\n          params.remove(ShardParams.SHARDS);      // not a top-level request\n          params.remove( \"indent\");\n          params.remove(CommonParams.HEADER_ECHO_PARAMS);\n          params.set(ShardParams.IS_SHARD, true );  // a sub (shard) request\n          String shardHandler = req.getParams().get(ShardParams.SHARDS_QT );\n          if (shardHandler == null) {\n            params.remove(CommonParams.QT);\n          } else {\n            params.set(CommonParams.QT, shardHandler);\n          }\n          // You can see CommonsHttpSolrServer.request.\n          comm.submit(sreq, shard, params);\n        }\n      }\n\n\n      // now wait for replies, but if anyone puts more requests on\n      // the outgoing queue, send them out immediately (by exiting\n      // this loop)\n      while (rb.outgoing.size() == 0) {\n        ShardResponse srsp = comm.takeCompletedOrError();\n        if (srsp == null) break;  // no more requests to wait for\n\n        // Was there an exception?  If so, abort everything and\n        // rethrow\n        if (srsp.getException() != null) {\n          comm.cancelAll();\n          if (srsp.getException() instanceof SolrException) {\n            throw (SolrException)srsp.getException();\n          } else {\n            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, srsp.getException());\n          }\n        }\n\n        rb.finished.add(srsp.getShardRequest());\n\n        // let the components see the responses to the request\n        for(SearchComponent c : components ) {\n          c.handleResponses(rb, srsp.getShardRequest());\n        }\n      }\n    }\n\n    for(SearchComponent c : components) {\n        c.finishStage(rb);\n     }\n\n    // we are done when the next stage is MAX_VALUE\n    // BTW ResponseBuilder.STAGE_DONE == Integer.MAX_VALUE\n  } while (nextStage != Integer.MAX_VALUE);\n}\n```\n\n### 3、其他\n\n有意思的是，我从Solr 3.5的reference中看到一句这样的话：\n\n> It is up to you to get all your documents indexed on each shard of your server farm. Solr does not include out-of-the-box support for distributed indexing, but your method can be as simple as a round robin technique. Just index each document to the next server in the circle.\n\n> Solr本身并不自带分布式的一些机制，索引如何分块还是靠自己写逻辑（如：轮询、hash取模等）。难怪我测试multicore的时候，shards的地址参数还得自己写，这一阶段对于用户来说不是透明的，所谓的分布式查询，也就是每个查询都要访问多台服务器，之后将多台服务器的结果和合并起来，这样看来，对每台服务器的压力来说感觉都是一样的诶（虽然可以不同业务分响应的小集群）。\n\n不知从哪个版本（好像是4.0）推出了Solr Cloud，官方这样介绍的：\n\n> SolrCloud is the name of a set of new distributed capabilities in Solr. Passing parameters to enable these capabilities will enable you to set up a highly available, fault tolerant cluster of Solr servers. Use SolrCloud when you want high scale, fault tolerant, distributed indexing and search capabilities.\n\n似乎这样才算是分布式解决方案，嗯，以后再深入看看Solr Cloud Architecture。\n\n> 参考资料：\n* [SolrCloud](http://wiki.apache.org/solr/SolrCloud)\n* [DistributedSearchComponent](http://wiki.apache.org/solr/WritingDistributedSearchComponents)\n","source":"_posts/2013/04/apache-solr-distributed-search.md","raw":"title: Apache Solr —— DistributedSearch\ndate: 2013-04-08 18:04:00\ncategories: 技术分享\ntags: Solr\n---\n\n### 1、前言\n\n当索引太大以致单台服务器的磁盘无法承受了，当一个简单的查询实在要耗费过多的时间，可以考虑使用Solr的分布式索引机制，或者配置一台多核机制（multicore）。当Solr配置了这样的机制，实质上就是将大索引分成了多个小索引分布在了不同服务器上，或者将请求发到多核，充分利用服务器CPU资源。\n\nSolr会将请求请求分发不同shards（理解为地址）上，并合并所有请求结果并返回给客户端。那么，这个分布式查询内部是怎么实现的呢？\n\n<!--more-->\n\n### 2、机制解析\n\n一个SearchComponent如果作为Distributed SearchComponent，需要重写以下四个方法：\n\n* distributedProcess()\n* modifyRequest()\n* handleResponses()\n* finishStage()\n\n同时，Distributed Search主要有4个主要阶段：\n\n* **Start**(ResponseBuilder.STAGE_START)\n* **Query Parse**(ResponseBuilder.STAGE_PARSE_QUERY)\n* **Execute Query**(ResponseBuilder.STAGE_EXECUTE_QUERY)\n* **Get Fields**(ResponseBuilder.STAGE_GET_FIELDS)</blockquote>\n* **Done**(ResponseBuilder.STAGE_DONE)。\n\n在SearchHandler中，基本的**分布式查询算法**如下：\n\n#### 1. 如果不在STAGE_DONE的阶段，循环以下流程；\n#### 2. 发起分布式处理的组件会不断查询是否需要进行分布式处理。如果有的话（distributedProcess），返回4个阶段中一个状态并创建一个 ShardRequest并将它添加到一个队列（rb.outgoing)中；\n\n> Components可以指定一些purpose字段，可以定制一些特殊的请求处理。如下：\n\n``` java\npublic void handleResponses(ResponseBuilder rb, ShardRequest sreq) {\n  if (!rb.doFacets )\n     return;\n\n  if ((sreq.purpose & ShardRequest.PURPOSE_GET_FACETS) != 0) {\n     countFacets(rb, sreq);\n  } else if ((sreq.purpose & ShardRequest.PURPOSE_REFINE_FACETS ) != 0) {\n     refineFacets(rb, sreq);\n  }\n}\n```\n\n> modifyRequest()是用来精简shard请求的，该方法都是加在了ResponseBuilder.addRequest()中，实例可以参考FacetComponent.modifyRequest。\n\n``` java\npublic void addRequest(SearchComponent me, ShardRequest sreq) {\n  outgoing.add(sreq);\n  if ((sreq.purpose & ShardRequest.PURPOSE_PRIVATE)==0) {\n   // if this isn't a private request, let other components modify it.\n   for (SearchComponent component : components) {\n      if (component != me) {\n        component.modifyRequest( this, me, sreq);\n      }\n    }\n  }\n}\n```\n\n#### 3. 取出队列（rb.outgoing)中所有 ShardRequest，并根据其配置将请求发到相应的 Shards；\n#### 4. 如果队列为空，则等待接收到响应，并处理（handleResponse)；\n\n> 你可以合并文档id神马的。QueryComponent.handleResponse\n\n#### 5. 进入下一轮循环之前，先调用finishStage()\n\n> 进行该轮的收尾工作，比如说将为null的doc从responseDocs中移除。\n\n``` java\n// SearchHandler.handleRequestBody()\nif (rb.shards == null) {\n  // a normal non-distributed request\n  // ...\n} else {\n  // a distributed request\n\n  HttpCommComponent comm = new HttpCommComponent();\n\n  if (rb.outgoing == null ) {\n    rb.outgoing = new LinkedList<ShardRequest>();\n  }\n  rb.finished = new ArrayList<ShardRequest>();\n\n  int nextStage = 0;\n  do {\n    rb.stage = nextStage;\n    nextStage = ResponseBuilder.STAGE_DONE;\n\n    // call all components\n    for( SearchComponent c : components ) {\n      // the next stage is the minimum of what all components report\n      nextStage = Math.min(nextStage, c.distributedProcess(rb));\n    }\n\n\n    // check the outgoing queue and send requests\n    while (rb.outgoing.size() > 0) {\n\n      // submit all current request tasks at once\n      while (rb.outgoing.size() > 0) {\n        ShardRequest sreq = rb.outgoing.remove(0);\n        sreq.actualShards = sreq.shards ;\n        if (sreq.actualShards ==ShardRequest.ALL_SHARDS) {\n          sreq.actualShards = rb.shards ;\n        }\n        sreq.responses = new ArrayList<ShardResponse>();\n\n        // TODO: map from shard to address[]\n        for (String shard : sreq.actualShards ) {\n          ModifiableSolrParams params = new ModifiableSolrParams(sreq.params );\n          params.remove(ShardParams.SHARDS);      // not a top-level request\n          params.remove( \"indent\");\n          params.remove(CommonParams.HEADER_ECHO_PARAMS);\n          params.set(ShardParams.IS_SHARD, true );  // a sub (shard) request\n          String shardHandler = req.getParams().get(ShardParams.SHARDS_QT );\n          if (shardHandler == null) {\n            params.remove(CommonParams.QT);\n          } else {\n            params.set(CommonParams.QT, shardHandler);\n          }\n          // You can see CommonsHttpSolrServer.request.\n          comm.submit(sreq, shard, params);\n        }\n      }\n\n\n      // now wait for replies, but if anyone puts more requests on\n      // the outgoing queue, send them out immediately (by exiting\n      // this loop)\n      while (rb.outgoing.size() == 0) {\n        ShardResponse srsp = comm.takeCompletedOrError();\n        if (srsp == null) break;  // no more requests to wait for\n\n        // Was there an exception?  If so, abort everything and\n        // rethrow\n        if (srsp.getException() != null) {\n          comm.cancelAll();\n          if (srsp.getException() instanceof SolrException) {\n            throw (SolrException)srsp.getException();\n          } else {\n            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, srsp.getException());\n          }\n        }\n\n        rb.finished.add(srsp.getShardRequest());\n\n        // let the components see the responses to the request\n        for(SearchComponent c : components ) {\n          c.handleResponses(rb, srsp.getShardRequest());\n        }\n      }\n    }\n\n    for(SearchComponent c : components) {\n        c.finishStage(rb);\n     }\n\n    // we are done when the next stage is MAX_VALUE\n    // BTW ResponseBuilder.STAGE_DONE == Integer.MAX_VALUE\n  } while (nextStage != Integer.MAX_VALUE);\n}\n```\n\n### 3、其他\n\n有意思的是，我从Solr 3.5的reference中看到一句这样的话：\n\n> It is up to you to get all your documents indexed on each shard of your server farm. Solr does not include out-of-the-box support for distributed indexing, but your method can be as simple as a round robin technique. Just index each document to the next server in the circle.\n\n> Solr本身并不自带分布式的一些机制，索引如何分块还是靠自己写逻辑（如：轮询、hash取模等）。难怪我测试multicore的时候，shards的地址参数还得自己写，这一阶段对于用户来说不是透明的，所谓的分布式查询，也就是每个查询都要访问多台服务器，之后将多台服务器的结果和合并起来，这样看来，对每台服务器的压力来说感觉都是一样的诶（虽然可以不同业务分响应的小集群）。\n\n不知从哪个版本（好像是4.0）推出了Solr Cloud，官方这样介绍的：\n\n> SolrCloud is the name of a set of new distributed capabilities in Solr. Passing parameters to enable these capabilities will enable you to set up a highly available, fault tolerant cluster of Solr servers. Use SolrCloud when you want high scale, fault tolerant, distributed indexing and search capabilities.\n\n似乎这样才算是分布式解决方案，嗯，以后再深入看看Solr Cloud Architecture。\n\n> 参考资料：\n* [SolrCloud](http://wiki.apache.org/solr/SolrCloud)\n* [DistributedSearchComponent](http://wiki.apache.org/solr/WritingDistributedSearchComponents)\n","slug":"2013/04/apache-solr-distributed-search","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92c00333x8f5rsbmknu"},{"title":"Apache Solr —— 常用数据结构（二）","date":"2013-04-08T10:04:00.000Z","_content":"\n### 1、前言\n\n继上篇[《Apache Solr – 常用数据结构（一）》](http://hongweiyi.com/2013/04/apache-solr-data-structrue-part-1/)介绍了NamedList和DocSet之后，这篇主要介绍ResponseBuilder和Query。\n\n<!--more-->\n\n### 2、ResponseBuilder\n\n每个组件（ QueryComponent、FacetComponent、MoreLikeThisComponent、HighlightComponent、 DebugComponent等)的操作都是围绕着 ResponseBuilder的实例来进行的，因为这个实例包含了全部的请求以及响应的信息。\n\nResponseBuilder是在SearchHandler的handleRequestBody方法中创建的，主要包含两个成员，SolrQueryRequest和SolrQueryResponse。还有一个components成员，用来装各种SearchComponent，但是好像这个成员并无太大的用武之地。推测作者设计ResponseBuilder类的原因应该是为了降低组件与组件之间的耦合度，让代码逻辑更为清晰。\n\n各种Components的prepare阶段主要是丰满了rb对象，将request成员中各种参数一一取出，赋值给rb。并在process方法中将结果写入response成员。\n\n如FacetComponent的prepare方法：\n\n``` java\npublic void prepare(ResponseBuilder rb) throws IOException {\n  if (rb.req .getParams().getBool(FacetParams.FACET, false)) {\n    rb.setNeedDocSet( true);\n    rb. doFacets = true ;\n  }\n}\n```\n\n\n### 3、Query\n\nLucene提供了多种多样的Query的实现，大多数都在org.apache.lucene.search包下面，这些Query结合起来可以实现非常复杂的查询。Query类有MatchAllDocsQuery、BooleanQuery、TermQuery等，系统通过QueryParser就会将查询解析为一个Query。如\"*:*\"就会解析成MatchAllDocsQuery，\"queryStr1 OR queryStr2\"就会解析成一个BooleanQuery等。\n\nQuery是大多数场景的开始，如果没有Query的话，系统就无法查询文档更别说进行打分（Score）。Query是Scorer的催化剂，并且负责创建和协调它，而Weight则提供了一个Query的中间形态，任何Searcher的依赖状态都必须存放在Weight对象中，而不是Query对象，所以大多数Query的实现类都需要提供一个Weight的实现。\n\nQuery的深入剖析好像是需要对Solr、Lucene有一个专家级的理解了，初学者的我就不多言了。\n\n可参考文档：\n\n* [Apache Lucene - Scoring](http://lucene.apache.org/core/3_6_2/scoring.html)\n* [Package - org.apache.lucene.search - query](http://lucene.apache.org/core/3_6_2/api/core/org/apache/lucene/search/package-summary.html#query)\n* [Lucene打分机制和Similarity模块](http://my.oschina.net/BreathL/blog/51498)\n\n### 4、结尾\n\n以上包括上一篇博文我都只能粗略的理解一些，并不能深入详细的理解及运用，需要在后期的学习及实践的过程中继续深入了。\n","source":"_posts/2013/04/apache-solr-data-structrue-part-2.md","raw":"title: Apache Solr —— 常用数据结构（二）\ndate: 2013-04-08 18:04:00\ncategories: 技术分享\ntags: [Solr, 数据结构]\n---\n\n### 1、前言\n\n继上篇[《Apache Solr – 常用数据结构（一）》](http://hongweiyi.com/2013/04/apache-solr-data-structrue-part-1/)介绍了NamedList和DocSet之后，这篇主要介绍ResponseBuilder和Query。\n\n<!--more-->\n\n### 2、ResponseBuilder\n\n每个组件（ QueryComponent、FacetComponent、MoreLikeThisComponent、HighlightComponent、 DebugComponent等)的操作都是围绕着 ResponseBuilder的实例来进行的，因为这个实例包含了全部的请求以及响应的信息。\n\nResponseBuilder是在SearchHandler的handleRequestBody方法中创建的，主要包含两个成员，SolrQueryRequest和SolrQueryResponse。还有一个components成员，用来装各种SearchComponent，但是好像这个成员并无太大的用武之地。推测作者设计ResponseBuilder类的原因应该是为了降低组件与组件之间的耦合度，让代码逻辑更为清晰。\n\n各种Components的prepare阶段主要是丰满了rb对象，将request成员中各种参数一一取出，赋值给rb。并在process方法中将结果写入response成员。\n\n如FacetComponent的prepare方法：\n\n``` java\npublic void prepare(ResponseBuilder rb) throws IOException {\n  if (rb.req .getParams().getBool(FacetParams.FACET, false)) {\n    rb.setNeedDocSet( true);\n    rb. doFacets = true ;\n  }\n}\n```\n\n\n### 3、Query\n\nLucene提供了多种多样的Query的实现，大多数都在org.apache.lucene.search包下面，这些Query结合起来可以实现非常复杂的查询。Query类有MatchAllDocsQuery、BooleanQuery、TermQuery等，系统通过QueryParser就会将查询解析为一个Query。如\"*:*\"就会解析成MatchAllDocsQuery，\"queryStr1 OR queryStr2\"就会解析成一个BooleanQuery等。\n\nQuery是大多数场景的开始，如果没有Query的话，系统就无法查询文档更别说进行打分（Score）。Query是Scorer的催化剂，并且负责创建和协调它，而Weight则提供了一个Query的中间形态，任何Searcher的依赖状态都必须存放在Weight对象中，而不是Query对象，所以大多数Query的实现类都需要提供一个Weight的实现。\n\nQuery的深入剖析好像是需要对Solr、Lucene有一个专家级的理解了，初学者的我就不多言了。\n\n可参考文档：\n\n* [Apache Lucene - Scoring](http://lucene.apache.org/core/3_6_2/scoring.html)\n* [Package - org.apache.lucene.search - query](http://lucene.apache.org/core/3_6_2/api/core/org/apache/lucene/search/package-summary.html#query)\n* [Lucene打分机制和Similarity模块](http://my.oschina.net/BreathL/blog/51498)\n\n### 4、结尾\n\n以上包括上一篇博文我都只能粗略的理解一些，并不能深入详细的理解及运用，需要在后期的学习及实践的过程中继续深入了。\n","slug":"2013/04/apache-solr-data-structrue-part-2","published":1,"updated":"2015-12-30T14:35:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92d00363x8f2c7cacf2"},{"title":"Apache Solr —— 常用数据结构（一）","date":"2013-04-08T10:04:00.000Z","_content":"\n### 1、前言\n\n这一段时间看Solr源码，主要的心思都是放在了整体框架运行流程，或者称之为“算法”。这些流程中间会穿插很多有意思的接口/类，NamedList、DocSet、DocIdSet、Query、Document、FieldType、ResponseBuilder等，这些接口/类看似简单，但是里面又有各种不同的实现，在翻源码的时候，确实会让人有些招架不过来，所以就准备写这篇博文让自己梳理梳理Solr的常用“数据结构”，有了算法和数据结构才能对Solr有一个完整的理解。\n\n<!--more-->\n\n### 2、NamedList\n\n一个简单的键值对列表，与Map不同的是：\n\n* 键名可以重复；\n* 保持数据的插入顺序；\n* 元素可以通过下标访问；\n* 键与值均可为null。\n\n内部实现其实也挺简单，内部容器采用的是ArrayList。list中的内部数据格式为：\n\n* [key1, value1, key2, value2, key3, value3,...]\n\n键与值通过线性排列组合在一起，访问下标则为：\n\n* key = list.get(idx << 1);\n* value = list.get((idx << 1) +1);\n\n如需要通过键查询的话，则是遍历list中的所有元素，时间复杂为O(N)。因为设计该类主要是用来实现有序的键值对序列，通过键查询的需求较少。如果需要经常查询的话，可以用SimpleOrderedMap或者直接用Map。\n\n需要说明的是，我并没有看出SimpleOrderedMap有啥名堂，内部实现只是实现了几个构造函数（全部调用super()），和一个clone()。并无其他逻辑。\n\nNamedList主要用来保存各种结果或者参数，变量名常见于res、params、terms。\n\n### 3、DocSet\n\n<center><div style=\"width: 80%;\">![DocSet](/images/sofa-DocSet.png)</div></center>\n\n这DocSet、DocList接口是DocSet数据结构的基础，一般来说DocSet就是用来保存DocId的集合，提供了一些集合的逻辑运算，如：union、intersection、addNot等。\n\n* DocSet是一个无序的Lucene Document Id的集合。\n* DocList是一个有序的Lucene Document Id的列表，这里的有序（ordered）应该不是指的排序（sorted），而是docs是顺序有关的，不可随意打乱，因为这个顺序关联一个可选的scores成员。\n\n在DocSet基础上，Solr提供了一个抽象类DocSetBase，DocSet系列中所有的派生类均继承了该方法。该抽象类实现了一些方法，这些方法主要是提供给无该方法实现的子类。如DocSlice，该类就没有实现union方法。\n\nDocSlice就是DocList的实现，好像没有啥特别的地方。由于实现DocList的原因，DocSlice也不建议被修改，它提供了一个subset的方法，可获得其子集。\n\nHashDocSet用的Hash表方法存的docIds，在数据比较少的时候好像比较省内存（但是除了测试代码之外，Solr内部没看到有实际逻辑代码使用了该类- -）\n\nSortedIntDocSet是排序的DocSet，在构造SortedIntDocSet的时候，就已经传入了一个排序好的docs。\n\nBitDocSet是用的位图存放docIds，集合的逻辑运算实现用的lucene的OpenBitSet，这个类也是Solr用的最多的DocSet了。位图的一些知识可以参考我的博文：[《趣味数据结构 - Bitmap》](http://hongweiyi.com/2012/03/data-structure-bitmap/)\n\n> BTW：还有一个DocIdSet，我就觉得稀奇了！DocSet也是放Document Id的，DocIdSet也是放Document Id的。而DocIdSet不直接提供存放Id的容器，只提供了一个迭代器的功能，这样的设计暂时没有弄明白是为什么，先放着吧。\n\n### 4、结尾\n\n今天就只分析NamedList和DocSet了，剩下的等清明小长假后再分析吧，下午出去旅游一下了……\n","source":"_posts/2013/04/apache-solr-data-structrue-part-1.md","raw":"title: Apache Solr —— 常用数据结构（一）\ndate: 2013-04-08 18:04:00\ncategories: 技术分享\ntags: [Solr, 数据结构]\n---\n\n### 1、前言\n\n这一段时间看Solr源码，主要的心思都是放在了整体框架运行流程，或者称之为“算法”。这些流程中间会穿插很多有意思的接口/类，NamedList、DocSet、DocIdSet、Query、Document、FieldType、ResponseBuilder等，这些接口/类看似简单，但是里面又有各种不同的实现，在翻源码的时候，确实会让人有些招架不过来，所以就准备写这篇博文让自己梳理梳理Solr的常用“数据结构”，有了算法和数据结构才能对Solr有一个完整的理解。\n\n<!--more-->\n\n### 2、NamedList\n\n一个简单的键值对列表，与Map不同的是：\n\n* 键名可以重复；\n* 保持数据的插入顺序；\n* 元素可以通过下标访问；\n* 键与值均可为null。\n\n内部实现其实也挺简单，内部容器采用的是ArrayList。list中的内部数据格式为：\n\n* [key1, value1, key2, value2, key3, value3,...]\n\n键与值通过线性排列组合在一起，访问下标则为：\n\n* key = list.get(idx << 1);\n* value = list.get((idx << 1) +1);\n\n如需要通过键查询的话，则是遍历list中的所有元素，时间复杂为O(N)。因为设计该类主要是用来实现有序的键值对序列，通过键查询的需求较少。如果需要经常查询的话，可以用SimpleOrderedMap或者直接用Map。\n\n需要说明的是，我并没有看出SimpleOrderedMap有啥名堂，内部实现只是实现了几个构造函数（全部调用super()），和一个clone()。并无其他逻辑。\n\nNamedList主要用来保存各种结果或者参数，变量名常见于res、params、terms。\n\n### 3、DocSet\n\n<center><div style=\"width: 80%;\">![DocSet](/images/sofa-DocSet.png)</div></center>\n\n这DocSet、DocList接口是DocSet数据结构的基础，一般来说DocSet就是用来保存DocId的集合，提供了一些集合的逻辑运算，如：union、intersection、addNot等。\n\n* DocSet是一个无序的Lucene Document Id的集合。\n* DocList是一个有序的Lucene Document Id的列表，这里的有序（ordered）应该不是指的排序（sorted），而是docs是顺序有关的，不可随意打乱，因为这个顺序关联一个可选的scores成员。\n\n在DocSet基础上，Solr提供了一个抽象类DocSetBase，DocSet系列中所有的派生类均继承了该方法。该抽象类实现了一些方法，这些方法主要是提供给无该方法实现的子类。如DocSlice，该类就没有实现union方法。\n\nDocSlice就是DocList的实现，好像没有啥特别的地方。由于实现DocList的原因，DocSlice也不建议被修改，它提供了一个subset的方法，可获得其子集。\n\nHashDocSet用的Hash表方法存的docIds，在数据比较少的时候好像比较省内存（但是除了测试代码之外，Solr内部没看到有实际逻辑代码使用了该类- -）\n\nSortedIntDocSet是排序的DocSet，在构造SortedIntDocSet的时候，就已经传入了一个排序好的docs。\n\nBitDocSet是用的位图存放docIds，集合的逻辑运算实现用的lucene的OpenBitSet，这个类也是Solr用的最多的DocSet了。位图的一些知识可以参考我的博文：[《趣味数据结构 - Bitmap》](http://hongweiyi.com/2012/03/data-structure-bitmap/)\n\n> BTW：还有一个DocIdSet，我就觉得稀奇了！DocSet也是放Document Id的，DocIdSet也是放Document Id的。而DocIdSet不直接提供存放Id的容器，只提供了一个迭代器的功能，这样的设计暂时没有弄明白是为什么，先放着吧。\n\n### 4、结尾\n\n今天就只分析NamedList和DocSet了，剩下的等清明小长假后再分析吧，下午出去旅游一下了……\n","slug":"2013/04/apache-solr-data-structrue-part-1","published":1,"updated":"2015-12-30T14:34:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92f003b3x8fo1obvqla"},{"title":"Maven笔记 - 仓库","id":"726","date":"2013-03-18T02:13:17.000Z","_content":"\n### 1、前言\n\n上一篇[博客](http://hongweiyi.com/2013/03/maven-coordinates-dependencies/)介绍了Maven坐标和依赖。在Maven中，任何一个依赖、插件或者项目构建的输出，都可以成为构件。而坐标和依赖是任何一个构件在Maven世界中的逻辑表示方式，构件的物理表示方式是文件，Maven则通过仓库（Repositories）来统一管理这些文件。Maven仓库是通过简单文件系统存储管理的，当遇到与仓库相关的问题，可以直接查找相关文件，方便定位问题。\n\n<!--more-->\n\n### 2、仓库的分类\n\n对于Maven而言，仓库分为两类：本地仓库和远程仓库。当Maven根据坐标寻找构件的时候，它会首先查看本地仓库，如果本地仓库存在此构件，则直接使用；如果本地仓库不存在此构件，或者需要查看是否有更新的构件版本，Maven就会去远程仓库查找，发现需要的构件之后，下载到本地仓库再使用。如果本地仓库和远程仓库都有没有需要的构件Maven就会报错。\n\n这里有三个远程仓库：**中央仓库、私服和其他公共库**。中央仓库是Maven核心自带的远程仓库（http://repo1.maven.org/maven2），它包含了绝大部分的构件。私服是另一种特殊的远程仓库，为了节省带宽和时间，应该在局域网内架设一个私有的仓库服务器，用其代理所有外部的远程仓库。除了以上两种，还有很多公开的远程仓库，比如Java. Net Maven库（http://download.java.net/maven/2/）和JBoss Maven库（http://repository.jboss.com/maven2/）\n\n> **IMPORTANT**：私服最重要的功能应该不在节省速度和带宽，而是可以部署内部构件，供企业内部开发人员使用。\n\n![image](http://hongweiyi.com/wp-content/uploads/2013/03/image.png)\n\n### 3、远程仓库的配置\n\n这位弟兄将《Maven实战》这个部分都给Copy到网上了，并加以标注，可以直接参考：[http://t.cn/zYgRHb7](http://t.cn/zYgRHb7)\n\n### 4、Maven镜像\n\n如果仓库X可以提供仓库Y存储的所有内容，那么就可以认为X是Y的一个镜像。Maven中央仓库在国内经常不能正常访问，连接超时不能下载资源之类的。在公司内部还挺好，有私服可以用，但是回家之后那龟爬的网速就难说了，一般来说可以配置Maven镜像。配置需要修改~/.m2/settings.xml，内容如下：\n\n``` xml\n<settings>\n  <!–…–>\n  <mirrors>\n    <mirror>\n      <id>maven.net.cn</id>\n      <name>one of the central mirrors in China</name>\n      <url>http://maven.net.cn/content/groups/public/</url&gt;\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n  </mirrors>\n  <!–…–>\n</settings>\n```\n\n> **WARNING**: 镜像仓库完全屏蔽了被镜像仓库，当镜像仓库不稳定或者不能访问的时候，Maven将无法访问被访问镜像。\n\n> **参考资料**：《Maven实战》\n","source":"_posts/2013/03/maven-repositories.md","raw":"title: Maven笔记 - 仓库\ntags:\n  - Java\n  - Maven\nid: 726\ncategories:\n  - 技术分享\ndate: 2013-03-18 10:13:17\n---\n\n### 1、前言\n\n上一篇[博客](http://hongweiyi.com/2013/03/maven-coordinates-dependencies/)介绍了Maven坐标和依赖。在Maven中，任何一个依赖、插件或者项目构建的输出，都可以成为构件。而坐标和依赖是任何一个构件在Maven世界中的逻辑表示方式，构件的物理表示方式是文件，Maven则通过仓库（Repositories）来统一管理这些文件。Maven仓库是通过简单文件系统存储管理的，当遇到与仓库相关的问题，可以直接查找相关文件，方便定位问题。\n\n<!--more-->\n\n### 2、仓库的分类\n\n对于Maven而言，仓库分为两类：本地仓库和远程仓库。当Maven根据坐标寻找构件的时候，它会首先查看本地仓库，如果本地仓库存在此构件，则直接使用；如果本地仓库不存在此构件，或者需要查看是否有更新的构件版本，Maven就会去远程仓库查找，发现需要的构件之后，下载到本地仓库再使用。如果本地仓库和远程仓库都有没有需要的构件Maven就会报错。\n\n这里有三个远程仓库：**中央仓库、私服和其他公共库**。中央仓库是Maven核心自带的远程仓库（http://repo1.maven.org/maven2），它包含了绝大部分的构件。私服是另一种特殊的远程仓库，为了节省带宽和时间，应该在局域网内架设一个私有的仓库服务器，用其代理所有外部的远程仓库。除了以上两种，还有很多公开的远程仓库，比如Java. Net Maven库（http://download.java.net/maven/2/）和JBoss Maven库（http://repository.jboss.com/maven2/）\n\n> **IMPORTANT**：私服最重要的功能应该不在节省速度和带宽，而是可以部署内部构件，供企业内部开发人员使用。\n\n![image](http://hongweiyi.com/wp-content/uploads/2013/03/image.png)\n\n### 3、远程仓库的配置\n\n这位弟兄将《Maven实战》这个部分都给Copy到网上了，并加以标注，可以直接参考：[http://t.cn/zYgRHb7](http://t.cn/zYgRHb7)\n\n### 4、Maven镜像\n\n如果仓库X可以提供仓库Y存储的所有内容，那么就可以认为X是Y的一个镜像。Maven中央仓库在国内经常不能正常访问，连接超时不能下载资源之类的。在公司内部还挺好，有私服可以用，但是回家之后那龟爬的网速就难说了，一般来说可以配置Maven镜像。配置需要修改~/.m2/settings.xml，内容如下：\n\n``` xml\n<settings>\n  <!–…–>\n  <mirrors>\n    <mirror>\n      <id>maven.net.cn</id>\n      <name>one of the central mirrors in China</name>\n      <url>http://maven.net.cn/content/groups/public/</url&gt;\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n  </mirrors>\n  <!–…–>\n</settings>\n```\n\n> **WARNING**: 镜像仓库完全屏蔽了被镜像仓库，当镜像仓库不稳定或者不能访问的时候，Maven将无法访问被访问镜像。\n\n> **参考资料**：《Maven实战》\n","slug":"2013/03/maven-repositories","published":1,"updated":"2015-12-30T12:08:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92g003f3x8fugob9x2z"},{"title":"Maven笔记 – 坐标与依赖","id":"707","date":"2013-03-17T12:47:05.000Z","_content":"\n**1、前言**\n\n\tMaven [meivin]，原意为内行、专家，用在Java开发中则是用于项目构建、依赖管理和项目信息管理。仔细总结一下，我们会发现，除了编写源代码，我们每天都有相当一部分时间花在了编译、运行单元测试、生成文档、打包和部署等繁琐且不起眼的工作上，这就是构建了。通过Maven可以通过一个简单的命令，让所有繁琐的步骤都能够自动完成，很方便的得到最终结果。\n\n\t<!--more-->\n\n\t这篇就介绍Maven比较重要的两个基本概念，坐标与依赖。以下大多数都是总结自《Maven实战》的笔记：\n\n\t&nbsp;\n\n\t**2、Maven****坐标元素**\n\n\t在开发项目的时候，开发者会到处收集第三方构件，而第三方构件版本各异并且难以收集，会导致大量时间花费在搜索、浏览网页等工作上。Maven则提供了一种统一的规范，让开发者通过简单的标识来表示一个构件并可以自动找到这个构件，这个标识就是坐标。通过以下坐标元素，可以唯一定义一个构件：\n\n\t**groupId****：**定义当前Maven项目隶属的实际项目\n\n\t**artifactId****：**定义实际项目中的一个Maven项目（模块），推荐使用实际项目名称作为artifactId的前缀，方便寻找实际构件。如nexus-indexer是nexus的indexer模块。\n\n\t**version****：**Maven项目当前所处的版本。\n\n\t**packaging(optional)****：**该元素定义Maven项目的打包方式（jar、war）\n\n\t**classsifier(optional)****：**附属构件，如javadocs、sources等，TestNG就有一个为jdk5的附属构件。BTW，不能直接定义项目的classifer，需要附加的插件帮助生成。\n\n\t[code lang=\"xml\"]&lt;groupId&gt;com.hongweiyi&lt;/groupId&gt;\n&lt;artifactId&gt;HelloWorld&lt;/artifactId&gt;\n&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n&lt;packaging&gt;jar&lt;/packaging&gt;\n\n\t[/code]\n\n\t&nbsp;\n\n\t**3、依赖配置**\n\n\t**groupId****、artifactId、version：**这三个基本坐标，必须有。\n\n\t**type****：**依赖的类型，对应于项目坐标定义的packaging，默认值为jar。\n\n\t**scope****：**依赖范围\n\n\tMaven在编译项目住代码、编译执行测试、实际运行项目的时候都会用相应的一套classpath\n\n<table border=\"1\" cellpadding=\"0\" cellspacing=\"0\">\n\t<tbody>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**依赖范围**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**对于编译classpath有效**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**对于测试classpath有效**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**对于运行时classpath有效**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**例子**\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tcompile\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tspring-core\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\ttest\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t-\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tJUnit\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tprovided\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tservlet-api\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\truntime\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tJDBC 驱动实现\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tsystem\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t本地的，Maven仓库之外的类库文件\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\timport\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t导入依赖范围，并无实际影响\n\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\t&nbsp;\n\n\t传递依赖，在使用某开源项目的时候，经常使用了某三方包后，发现该三方包还依赖其他包，这就不得不一个个导入相应的依赖包。而Maven则很方便，不用考虑三方包依赖了什么，Maven会直接解析各个直接依赖的POM，并将那些必要的间接依赖，以传递性依赖的形式引入到当前的项目中。\n\n\t传递依赖也会有问题，比如：A-&gt;B-&gt;C-&gt;X(1.0)；A-&gt;D-&gt;X(2.0)，在这种情况下，Maven依赖解调第一原则是：路径最近者优先。上面那个例子，X(2.0)路径较近，所以会优先解析它。那么如果路径长度一样怎么解析呢？如：A-&gt;B-&gt;X(1.0) A-&gt;C-&gt;X(2.0)，这个就要用Maven解调的第二原则：第一声明者优先。在路径长度相等的前提下，POM中依赖声明的顺序决定了谁会被解析使用。\n\n> BTW：依赖范围不仅可以控制依赖与三种classpath的关系，还对依赖性传递产生影响，这个影响有点绕，笔者还没弄太明白，就先不发了。T_T\n\n\t**optional****：**可选依赖。 (false / true)\n\n\tA-&gt;B, B-&gt;X(optional), B-&gt;Y(optional)\n\n\tA如果要用X或者Y的话，需要在POM中显示声明该Dependency。\n\n\t[code lang=\"xml\"]&lt;project&gt;\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n&lt;dependecies&gt;\n   &lt;dependecy&gt;\n      &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n      &lt;artifactId&gt;project-b&lt;/artifactId&gt;\n      &lt;version&gt;1.0.0&lt;/version&gt;\n   &lt;/dependency&gt;\n   &lt;dependency&gt;        &lt;!-- 显示声明X (Y) --&gt;\n     &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n     &lt;artifactId&gt;project-X&lt;/artifactId&gt; &lt;!--artifactId&gt;project-Y&lt;/artifactId--&gt;\n     &lt;version&gt;1.1.0&lt;/version&gt;\n   &lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n\n\t[/code]\n\n\t**exclusions：**排除传递性依赖。\n\n\t传递性会给项目隐式地引入很多依赖，简化项目依赖管理的同时，也会带来一些风险。如果有一个三方包依赖，而这三方包依赖了另一个类库的SNAPSHOT版本，这个版本就成为了当前项目的传递性依赖，而SNAPSHOT的不稳定性也会直接影响到当前项目。所以需要将这个SNAPSHOT的版本给排除掉，并显示声明一个好用的版本。\n\n\t[code lang=\"xml\"]&lt;project&gt;\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n&lt;dependecies&gt;\n   &lt;dependecy&gt;\n      &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n      &lt;artifactId&gt;project-b&lt;/artifactId&gt;\n      &lt;version&gt;1.0.0&lt;/version&gt;\n      &lt;exclusions&gt;\n         &lt;exclusion&gt;      &lt;!-- 排除project-c --&gt;\n            &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n            &lt;artifactId&gt;project-c&lt;/artifactId&gt;\n          &lt;/exclusion&gt;\n      &lt;exclusions&gt;\n   &lt;/dependency&gt;\n   &lt;dependency&gt;        &lt;!-- 重新声明project-c 1.1.0版 --&gt;\n     &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n     &lt;artifactId&gt;project-c&lt;/artifactId&gt;\n     &lt;version&gt;1.1.0&lt;/version&gt;\n   &lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n\n\t[/code]\n\n\t&nbsp;\n\n> 参考资料：《Maven实战》\n","source":"_posts/2013/03/maven-coordinates-dependencies.md","raw":"title: Maven笔记 – 坐标与依赖\ntags:\n  - Java\n  - Maven\nid: 707\ncategories:\n  - 技术分享\ndate: 2013-03-17 20:47:05\n---\n\n**1、前言**\n\n\tMaven [meivin]，原意为内行、专家，用在Java开发中则是用于项目构建、依赖管理和项目信息管理。仔细总结一下，我们会发现，除了编写源代码，我们每天都有相当一部分时间花在了编译、运行单元测试、生成文档、打包和部署等繁琐且不起眼的工作上，这就是构建了。通过Maven可以通过一个简单的命令，让所有繁琐的步骤都能够自动完成，很方便的得到最终结果。\n\n\t<!--more-->\n\n\t这篇就介绍Maven比较重要的两个基本概念，坐标与依赖。以下大多数都是总结自《Maven实战》的笔记：\n\n\t&nbsp;\n\n\t**2、Maven****坐标元素**\n\n\t在开发项目的时候，开发者会到处收集第三方构件，而第三方构件版本各异并且难以收集，会导致大量时间花费在搜索、浏览网页等工作上。Maven则提供了一种统一的规范，让开发者通过简单的标识来表示一个构件并可以自动找到这个构件，这个标识就是坐标。通过以下坐标元素，可以唯一定义一个构件：\n\n\t**groupId****：**定义当前Maven项目隶属的实际项目\n\n\t**artifactId****：**定义实际项目中的一个Maven项目（模块），推荐使用实际项目名称作为artifactId的前缀，方便寻找实际构件。如nexus-indexer是nexus的indexer模块。\n\n\t**version****：**Maven项目当前所处的版本。\n\n\t**packaging(optional)****：**该元素定义Maven项目的打包方式（jar、war）\n\n\t**classsifier(optional)****：**附属构件，如javadocs、sources等，TestNG就有一个为jdk5的附属构件。BTW，不能直接定义项目的classifer，需要附加的插件帮助生成。\n\n\t[code lang=\"xml\"]&lt;groupId&gt;com.hongweiyi&lt;/groupId&gt;\n&lt;artifactId&gt;HelloWorld&lt;/artifactId&gt;\n&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n&lt;packaging&gt;jar&lt;/packaging&gt;\n\n\t[/code]\n\n\t&nbsp;\n\n\t**3、依赖配置**\n\n\t**groupId****、artifactId、version：**这三个基本坐标，必须有。\n\n\t**type****：**依赖的类型，对应于项目坐标定义的packaging，默认值为jar。\n\n\t**scope****：**依赖范围\n\n\tMaven在编译项目住代码、编译执行测试、实际运行项目的时候都会用相应的一套classpath\n\n<table border=\"1\" cellpadding=\"0\" cellspacing=\"0\">\n\t<tbody>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**依赖范围**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**对于编译classpath有效**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**对于测试classpath有效**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**对于运行时classpath有效**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**例子**\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tcompile\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tspring-core\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\ttest\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t-\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tJUnit\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tprovided\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tservlet-api\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\truntime\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tJDBC 驱动实现\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\tsystem\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**Y**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t本地的，Maven仓库之外的类库文件\n\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\timport\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t**-**\n\n\t\t\t</td>\n\t\t\t<td valign=\"top\">\n\n\t\t\t\t\t导入依赖范围，并无实际影响\n\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n\t&nbsp;\n\n\t传递依赖，在使用某开源项目的时候，经常使用了某三方包后，发现该三方包还依赖其他包，这就不得不一个个导入相应的依赖包。而Maven则很方便，不用考虑三方包依赖了什么，Maven会直接解析各个直接依赖的POM，并将那些必要的间接依赖，以传递性依赖的形式引入到当前的项目中。\n\n\t传递依赖也会有问题，比如：A-&gt;B-&gt;C-&gt;X(1.0)；A-&gt;D-&gt;X(2.0)，在这种情况下，Maven依赖解调第一原则是：路径最近者优先。上面那个例子，X(2.0)路径较近，所以会优先解析它。那么如果路径长度一样怎么解析呢？如：A-&gt;B-&gt;X(1.0) A-&gt;C-&gt;X(2.0)，这个就要用Maven解调的第二原则：第一声明者优先。在路径长度相等的前提下，POM中依赖声明的顺序决定了谁会被解析使用。\n\n> BTW：依赖范围不仅可以控制依赖与三种classpath的关系，还对依赖性传递产生影响，这个影响有点绕，笔者还没弄太明白，就先不发了。T_T\n\n\t**optional****：**可选依赖。 (false / true)\n\n\tA-&gt;B, B-&gt;X(optional), B-&gt;Y(optional)\n\n\tA如果要用X或者Y的话，需要在POM中显示声明该Dependency。\n\n\t[code lang=\"xml\"]&lt;project&gt;\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n&lt;dependecies&gt;\n   &lt;dependecy&gt;\n      &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n      &lt;artifactId&gt;project-b&lt;/artifactId&gt;\n      &lt;version&gt;1.0.0&lt;/version&gt;\n   &lt;/dependency&gt;\n   &lt;dependency&gt;        &lt;!-- 显示声明X (Y) --&gt;\n     &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n     &lt;artifactId&gt;project-X&lt;/artifactId&gt; &lt;!--artifactId&gt;project-Y&lt;/artifactId--&gt;\n     &lt;version&gt;1.1.0&lt;/version&gt;\n   &lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n\n\t[/code]\n\n\t**exclusions：**排除传递性依赖。\n\n\t传递性会给项目隐式地引入很多依赖，简化项目依赖管理的同时，也会带来一些风险。如果有一个三方包依赖，而这三方包依赖了另一个类库的SNAPSHOT版本，这个版本就成为了当前项目的传递性依赖，而SNAPSHOT的不稳定性也会直接影响到当前项目。所以需要将这个SNAPSHOT的版本给排除掉，并显示声明一个好用的版本。\n\n\t[code lang=\"xml\"]&lt;project&gt;\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n&lt;dependecies&gt;\n   &lt;dependecy&gt;\n      &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n      &lt;artifactId&gt;project-b&lt;/artifactId&gt;\n      &lt;version&gt;1.0.0&lt;/version&gt;\n      &lt;exclusions&gt;\n         &lt;exclusion&gt;      &lt;!-- 排除project-c --&gt;\n            &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n            &lt;artifactId&gt;project-c&lt;/artifactId&gt;\n          &lt;/exclusion&gt;\n      &lt;exclusions&gt;\n   &lt;/dependency&gt;\n   &lt;dependency&gt;        &lt;!-- 重新声明project-c 1.1.0版 --&gt;\n     &lt;groupId&gt;com.hongweiyi.blog&lt;/groupId&gt;\n     &lt;artifactId&gt;project-c&lt;/artifactId&gt;\n     &lt;version&gt;1.1.0&lt;/version&gt;\n   &lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n\n\t[/code]\n\n\t&nbsp;\n\n> 参考资料：《Maven实战》\n","slug":"2013/03/maven-coordinates-dependencies","published":1,"updated":"2015-12-29T15:04:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92h003k3x8ff96p6t4n"},{"title":"Apache Solr —— Facet Pivot实现与低版本移植","date":"2013-03-27T15:12:00.000Z","_content":"\n### 1、前言\n\nSolr升级到4.0后便有了一个新功能，就是facet.pivot，关于pivot的介绍可以看上一篇文章：[《Apache Solr – Facet介绍》](http://hongweiyi.com/2013/03/apache-solr-facet-introduction/) 这篇主要讲述Pivot的内部实现机制，以及如何将这个功能移植到低版本的Solr中来。\n\n<!--more-->\n\n### 2、Pivot实现机制\n\n> `http://...?q=*:*&facet=true&&facet.field=province&facet.field=city&facet.pivot=province,city&wt=json`\n\n#### 2.1 获得所有pivots参数列表\n\n> e.g.: [\"province,city\", …]\n\n#### 2.2 将pivots交给PivotFacetHelper处理\n\n``` java\n// FacetComponent.java -> process\n// e.g.: [\"province,city\"]\nString[] pivots = params.getParams(FacetParams.FACET_PIVOT);\nif (pivots != null && pivots.length > 0) {\n\tPivotFacetHelper pivotHelper = new PivotFacetHelper(rb.req,\n\t\t\t\t\t\trb.getResults().docSet, params, rb);\n\t// process each pivots individually\n\tNamedList v = pivotHelper.process(pivots);\n\tif (v != null) {\n\t\tcounts.add(PIVOT_KEY, v);\n\t}\n}\n```\n\n#### 2.3 解析pivot，获得每个field –> pivot: field subField fnames\n\n``` Java\n// PivotFacetHelper.java -> process\nfor (String pivot : pivots) {\n\t// …\n\n\tString[] fields = pivot.split(\",\");\n\n\tif (fields.length < 2) {\n\t    throw new SolrException(ErrorCode.BAD_REQUEST,\n\t\t\t\"Pivot Facet needs at least two fields: \" + pivot);\n\t}\n\n\tString field = fields[0];\n\tString subField = fields[1];\n\n\t// the rest of fields\n\tDeque<String> fnames = new LinkedList<String>();\n\tfor (int i = fields.length - 1; i > 1; i--) {\n\t\tfnames.push(fields[i]);\n\t}\n\t// …\n}\n```\n\n#### 2.4 获得第一级field的facets\n\n``` java\n// PivotFacetHelper.java -> process\n// e.g.: field: province\n//        superFacets : [\"Jiangsu\", 4, \"Hunan\", 3, \"Guangdong\", 2, \"Beijing\", 1, \"Zhejiang\", 1]\nNamedList<Integer> superFacets = this.getTermCounts(field);\n```\n\n#### 2.5 递归调用pivotResult = doPivot(superFacets, field, subField, fnames, docs)\n\n``` java\n        // …\n        // super.key usually == pivot unless local-param 'key' used\n        pivotResponse.add(key,\n        doPivots(superFacets, field, subField, fnames, base));\n\t}\n\treturn pivotResponse;\n}\n\n// PivotFacetHelper.java -> doPivots\nprotected List<NamedList<Object>> doPivots(NamedList<Integer> superFacets, String field, String subField, Deque<String> fnames, DocSet docs) throws IOException {\n\n\t// …\n\t// 遍历该级所有facet，获得该facet下所有下一级facet数据…\n    // [\"Jiangsu\", \"Hunan\", \"Guangdong\", \"Beijing\", \"Zhejiang\"]\n\tfor (Map.Entry<String, Integer> kv : superFacets) {\n\t\t// Only sub-facet if parent facet has positive count - still may not\n\t\t// be any values for the sub-field though\n\t\tif (kv.getValue() >= minMatch) {\n\n\t\t\t// …\n\t\t\t// pivot就是该级facet，添加它相关数据（field、value、count）\n\t\t\tSimpleOrderedMap<Object> pivot = new SimpleOrderedMap<Object>();\n\t\t\tpivot.add(\"field\", field);\n\t\t\tif (null == fieldValue) {\n\t\t\t\tpivot.add(\"value\", null);\n\t\t\t} else {\n\t\t\t    // termval：当前facets、field下，所包含的值\n                // e.g.: “Jiangsu”\n\t\t\t\ttermval = new BytesRef();\n\t\t\t\tftype.readableToIndexed(fieldValue, termval);\n\t\t\t\tpivot.add(\"value\", ftype.toObject(sfield, termval));\n\t\t\t}\n\t\t\tpivot.add(\"count\", kv.getValue());\n\n\t\t\t// subField为空，这一级facet的这个pivot递归遍历完毕\n\t\t\tif (subField == null) {\n\t\t\t\tvalues.add(pivot);\n\t\t\t// subField不为空，继续递归遍历\n\t\t\t} else {\n\t\t\t    // subset: 当前facet下，包含field:value的文档子集\n\t\t\t    // field:value -> “province”:“Jiangsu”\n\t\t\t\tDocSet subset = null;\n\n\t\t\t\tif (null == termval) {\n\t\t\t\t\tDocSet hasVal = searcher.getDocSet(new TermRangeQuery(\n\t\t\t\t\t\t\tfield, null, null, false, false));\n\t\t\t\t\tsubset = docs.andNot(hasVal);\n\t\t\t\t} else {\n\t\t\t\t\tString term = new String(new String(termval.bytes,\n\t\t\t\t\t\t\ttermval.offset, termval.length));\n\n\t\t\t\t\tQuery query = new TermQuery(new Term(field, term));\n\t\t\t\t\tsubset = searcher.getDocSet(query, docs);\n\t\t\t\t}\n\t\t\t\tsuper.docs = subset;// used by getTermCounts()\n\n                 // nl: 获得当前docs下，下一级field的facets。\n                 // nl我理解为next level\n                 // e.g.: subField: “city”\n                 //         nl: [“Suzhou”, 3, “Nanjing”, 1]\n\t\t\t\tNamedList<Integer> nl = this.getTermCounts(subField);\n\t\t\t\tif (nl.size() >= minMatch) {\n                     // 继续下一层迭代\n\t\t\t\t\tpivot.add(\t\"pivot\",doPivots(nl, subField, nextField, fnames, subset));\n\t\t\t\t\tvalues.add(pivot); // only add response if there are\n\t\t\t\t\t\t\t\t\t\t\t// some counts\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t\t// put the field back on the list\n\tfnames.push(nextField);\n\treturn values;\n}\n```\n\n> 新版添加新机制方式可以参考，并不是在源码基础上大刀阔斧的改。而是添加了一个新类，只需原有获取facet流程逻辑上，添加少数几行代码即可，这也算是模块化思想么？\n\n### 3、移植Pivot功能到低级版本\n\n> 这里是从solr4.0移植到solr3.5\n\n移植步骤倒也还方便，主要是添加PivotFacetHelper类，剩下的就是一步一步修复红叉叉了。\n\n``` java\nIndex: core/src/java/org/apache/solr/handler/component/FacetComponent.java\n===================================================================\n--- core/src/java/org/apache/solr/handler/component/FacetComponent.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/handler/component/FacetComponent.java\t(Version unknown new)\n\n46a47,48\n> \tstatic final String PIVOT_KEY = \"facet_pivot\";\n>\n66c68,79\n<\n---\n> \t\t\tNamedList<Object> counts = f.getFacetCounts();\n> \t\t\t// e.g.: [\"province,city\"]\n> \t\t\tString[] pivots = params.getParams(FacetParams.FACET_PIVOT);\n> \t\t\tif (pivots != null && pivots.length > 0) {\n> \t\t\t\tPivotFacetHelper pivotHelper = new PivotFacetHelper(rb.req,\n> \t\t\t\t\t\trb.getResults().docSet, params, rb);\n> \t\t\t\t// process each pivots individually\n> \t\t\t\tNamedList v = pivotHelper.process(pivots);\n> \t\t\t\tif (v != null) {\n> \t\t\t\t\tcounts.add(PIVOT_KEY, v);\n> \t\t\t\t}\n> \t\t\t}\n68c81\n< \t\t\trb.rsp.add(\"facet_counts\", f.getFacetCounts());\n---\n> \t\t\trb.rsp.add(\"facet_counts\", counts);\n```\n\n``` java\nIndex: solrj/src/java/org/apache/solr/common/params/FacetParams.java\n===================================================================\n--- solrj/src/java/org/apache/solr/common/params/FacetParams.java\t(Version unknown old)\n+++ solrj/src/java/org/apache/solr/common/params/FacetParams.java\t(Version unknown new)\n\n92a93,98\n> \t/**\n> \t * Comma separated list of fields to pivot\n> \t *\n> \t * example: author,type (for types by author / types within author)\n> \t */\n> \tpublic static final String FACET_PIVOT = FACET + \".pivot\";\n94a101,106\n> \t * Minimum number of docs that need to match to be included in the sublist\n> \t *\n> \t * default value is 1\n> \t */\n> \tpublic static final String FACET_PIVOT_MINCOUNT = FACET_PIVOT + \".mincount\";\n> \t/**\n```\n\n``` java\nIndex: core/src/java/org/apache/solr/request/SimpleFacets.java\n===================================================================\n--- core/src/java/org/apache/solr/request/SimpleFacets.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/request/SimpleFacets.java\t(Version unknown new)\n\n74,76c74,77\n< \tString facetValue; // the field to or query to facet on (minus local params)\n< \tDocSet base; // the base docset for this particular facet\n< \tString key; // what name should the results be stored under\n---\n> \tprotected String facetValue; // the field to or query to facet on (minus\n> \t\t\t\t\t\t\t\t\t// local params)\n> \tprotected DocSet base; // the base docset for this particular facet\n> \tprotected String key; // what name should the results be stored under\n92,93c93,94\n< \tvoid parseParams(String type, String param) throws ParseException,\n< \t\t\tIOException {\n---\n> \tprotected void parseParams(String type, String param)\n> \t\t\tthrows ParseException, IOException {\n```\n\n```\nIndex: core/src/java/org/apache/solr/schema/FieldType.java\n===================================================================\n--- core/src/java/org/apache/solr/schema/FieldType.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/schema/FieldType.java\t(Version unknown new)\n394a399,411\n> \tpublic Object toObject(SchemaField sf, BytesRef term) {\n> \t\tfinal CharsRef ref = new CharsRef(term.length);\n> \t\tindexedToReadable(term, ref);\n> \t\tfinal Fieldable f =  createField(sf, ref.toString(), 1.0f);\n> \t\treturn toObject(f);\n> \t}\n>\n> \t/** Given an indexed term, append the human readable representation */\n> \tpublic CharsRef indexedToReadable(BytesRef input, CharsRef output) {\n> \t\tUnicodeUtil.UTF8toUTF16(input, output);\n> \t\treturn output;\n> \t}\n>\n418a436,441\n> \t/** Given the readable value, return the term value that will match it. */\n> \tpublic void readableToIndexed(CharSequence val, BytesRef result) {\n> \t\tfinal String internal = readableToIndexed(val.toString());\n> \t\tUnicodeUtil.UTF16toUTF8(internal, 0, internal.length(), result);\n> \t}\n>\n```\n\n```\nIndex: core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\n===================================================================\n--- core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\t(Version unknown new)\n\n144c152,155\n<        Query query = new TermQuery(new Term(field, termval));\n---\n>        String term = new String(new String(termval.bytes,\n>                       termval.offset, termval.length));\n>\n>        Query query = new TermQuery(new Term(field, term));\n147,148c158,159\n<        super.docs = subset;// used by getTermCounts()\n<\n---\n>        super.base = subset;// used by getTermCounts()\n>\n```\n\n> 没弄过patch，上面的就将就着看吧。\n\n> **WARNING**:\n>\n> 3.x中，SimpleFacet组件中有两个成员，docs/base。在4.x中，这两个成员改名为docsOrig/docs。名字倒是清晰了，就是移植的时候没注意倒是挺麻烦的。\n\n> **WARNING**:\n>\n> BytesRef这个类，在4.x中改动比较大，而且Term也原生支持BytesRef。为了偷懒，就直接将BytesRef.bytes转换成了String。\n\n> **WARNING**:\n>\n>  上面还有一些小类，没有写上，有红叉叉自己改改应该问题不大。</blockquote>\n","source":"_posts/2013/03/apache-solr-facet-pivot-implementation-tranplant.md","raw":"title: Apache Solr —— Facet Pivot实现与低版本移植\ndate: 2013-03-27 23:12:00\ncategories: 技术分享\ntags: Solr\n---\n\n### 1、前言\n\nSolr升级到4.0后便有了一个新功能，就是facet.pivot，关于pivot的介绍可以看上一篇文章：[《Apache Solr – Facet介绍》](http://hongweiyi.com/2013/03/apache-solr-facet-introduction/) 这篇主要讲述Pivot的内部实现机制，以及如何将这个功能移植到低版本的Solr中来。\n\n<!--more-->\n\n### 2、Pivot实现机制\n\n> `http://...?q=*:*&facet=true&&facet.field=province&facet.field=city&facet.pivot=province,city&wt=json`\n\n#### 2.1 获得所有pivots参数列表\n\n> e.g.: [\"province,city\", …]\n\n#### 2.2 将pivots交给PivotFacetHelper处理\n\n``` java\n// FacetComponent.java -> process\n// e.g.: [\"province,city\"]\nString[] pivots = params.getParams(FacetParams.FACET_PIVOT);\nif (pivots != null && pivots.length > 0) {\n\tPivotFacetHelper pivotHelper = new PivotFacetHelper(rb.req,\n\t\t\t\t\t\trb.getResults().docSet, params, rb);\n\t// process each pivots individually\n\tNamedList v = pivotHelper.process(pivots);\n\tif (v != null) {\n\t\tcounts.add(PIVOT_KEY, v);\n\t}\n}\n```\n\n#### 2.3 解析pivot，获得每个field –> pivot: field subField fnames\n\n``` Java\n// PivotFacetHelper.java -> process\nfor (String pivot : pivots) {\n\t// …\n\n\tString[] fields = pivot.split(\",\");\n\n\tif (fields.length < 2) {\n\t    throw new SolrException(ErrorCode.BAD_REQUEST,\n\t\t\t\"Pivot Facet needs at least two fields: \" + pivot);\n\t}\n\n\tString field = fields[0];\n\tString subField = fields[1];\n\n\t// the rest of fields\n\tDeque<String> fnames = new LinkedList<String>();\n\tfor (int i = fields.length - 1; i > 1; i--) {\n\t\tfnames.push(fields[i]);\n\t}\n\t// …\n}\n```\n\n#### 2.4 获得第一级field的facets\n\n``` java\n// PivotFacetHelper.java -> process\n// e.g.: field: province\n//        superFacets : [\"Jiangsu\", 4, \"Hunan\", 3, \"Guangdong\", 2, \"Beijing\", 1, \"Zhejiang\", 1]\nNamedList<Integer> superFacets = this.getTermCounts(field);\n```\n\n#### 2.5 递归调用pivotResult = doPivot(superFacets, field, subField, fnames, docs)\n\n``` java\n        // …\n        // super.key usually == pivot unless local-param 'key' used\n        pivotResponse.add(key,\n        doPivots(superFacets, field, subField, fnames, base));\n\t}\n\treturn pivotResponse;\n}\n\n// PivotFacetHelper.java -> doPivots\nprotected List<NamedList<Object>> doPivots(NamedList<Integer> superFacets, String field, String subField, Deque<String> fnames, DocSet docs) throws IOException {\n\n\t// …\n\t// 遍历该级所有facet，获得该facet下所有下一级facet数据…\n    // [\"Jiangsu\", \"Hunan\", \"Guangdong\", \"Beijing\", \"Zhejiang\"]\n\tfor (Map.Entry<String, Integer> kv : superFacets) {\n\t\t// Only sub-facet if parent facet has positive count - still may not\n\t\t// be any values for the sub-field though\n\t\tif (kv.getValue() >= minMatch) {\n\n\t\t\t// …\n\t\t\t// pivot就是该级facet，添加它相关数据（field、value、count）\n\t\t\tSimpleOrderedMap<Object> pivot = new SimpleOrderedMap<Object>();\n\t\t\tpivot.add(\"field\", field);\n\t\t\tif (null == fieldValue) {\n\t\t\t\tpivot.add(\"value\", null);\n\t\t\t} else {\n\t\t\t    // termval：当前facets、field下，所包含的值\n                // e.g.: “Jiangsu”\n\t\t\t\ttermval = new BytesRef();\n\t\t\t\tftype.readableToIndexed(fieldValue, termval);\n\t\t\t\tpivot.add(\"value\", ftype.toObject(sfield, termval));\n\t\t\t}\n\t\t\tpivot.add(\"count\", kv.getValue());\n\n\t\t\t// subField为空，这一级facet的这个pivot递归遍历完毕\n\t\t\tif (subField == null) {\n\t\t\t\tvalues.add(pivot);\n\t\t\t// subField不为空，继续递归遍历\n\t\t\t} else {\n\t\t\t    // subset: 当前facet下，包含field:value的文档子集\n\t\t\t    // field:value -> “province”:“Jiangsu”\n\t\t\t\tDocSet subset = null;\n\n\t\t\t\tif (null == termval) {\n\t\t\t\t\tDocSet hasVal = searcher.getDocSet(new TermRangeQuery(\n\t\t\t\t\t\t\tfield, null, null, false, false));\n\t\t\t\t\tsubset = docs.andNot(hasVal);\n\t\t\t\t} else {\n\t\t\t\t\tString term = new String(new String(termval.bytes,\n\t\t\t\t\t\t\ttermval.offset, termval.length));\n\n\t\t\t\t\tQuery query = new TermQuery(new Term(field, term));\n\t\t\t\t\tsubset = searcher.getDocSet(query, docs);\n\t\t\t\t}\n\t\t\t\tsuper.docs = subset;// used by getTermCounts()\n\n                 // nl: 获得当前docs下，下一级field的facets。\n                 // nl我理解为next level\n                 // e.g.: subField: “city”\n                 //         nl: [“Suzhou”, 3, “Nanjing”, 1]\n\t\t\t\tNamedList<Integer> nl = this.getTermCounts(subField);\n\t\t\t\tif (nl.size() >= minMatch) {\n                     // 继续下一层迭代\n\t\t\t\t\tpivot.add(\t\"pivot\",doPivots(nl, subField, nextField, fnames, subset));\n\t\t\t\t\tvalues.add(pivot); // only add response if there are\n\t\t\t\t\t\t\t\t\t\t\t// some counts\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t\t// put the field back on the list\n\tfnames.push(nextField);\n\treturn values;\n}\n```\n\n> 新版添加新机制方式可以参考，并不是在源码基础上大刀阔斧的改。而是添加了一个新类，只需原有获取facet流程逻辑上，添加少数几行代码即可，这也算是模块化思想么？\n\n### 3、移植Pivot功能到低级版本\n\n> 这里是从solr4.0移植到solr3.5\n\n移植步骤倒也还方便，主要是添加PivotFacetHelper类，剩下的就是一步一步修复红叉叉了。\n\n``` java\nIndex: core/src/java/org/apache/solr/handler/component/FacetComponent.java\n===================================================================\n--- core/src/java/org/apache/solr/handler/component/FacetComponent.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/handler/component/FacetComponent.java\t(Version unknown new)\n\n46a47,48\n> \tstatic final String PIVOT_KEY = \"facet_pivot\";\n>\n66c68,79\n<\n---\n> \t\t\tNamedList<Object> counts = f.getFacetCounts();\n> \t\t\t// e.g.: [\"province,city\"]\n> \t\t\tString[] pivots = params.getParams(FacetParams.FACET_PIVOT);\n> \t\t\tif (pivots != null && pivots.length > 0) {\n> \t\t\t\tPivotFacetHelper pivotHelper = new PivotFacetHelper(rb.req,\n> \t\t\t\t\t\trb.getResults().docSet, params, rb);\n> \t\t\t\t// process each pivots individually\n> \t\t\t\tNamedList v = pivotHelper.process(pivots);\n> \t\t\t\tif (v != null) {\n> \t\t\t\t\tcounts.add(PIVOT_KEY, v);\n> \t\t\t\t}\n> \t\t\t}\n68c81\n< \t\t\trb.rsp.add(\"facet_counts\", f.getFacetCounts());\n---\n> \t\t\trb.rsp.add(\"facet_counts\", counts);\n```\n\n``` java\nIndex: solrj/src/java/org/apache/solr/common/params/FacetParams.java\n===================================================================\n--- solrj/src/java/org/apache/solr/common/params/FacetParams.java\t(Version unknown old)\n+++ solrj/src/java/org/apache/solr/common/params/FacetParams.java\t(Version unknown new)\n\n92a93,98\n> \t/**\n> \t * Comma separated list of fields to pivot\n> \t *\n> \t * example: author,type (for types by author / types within author)\n> \t */\n> \tpublic static final String FACET_PIVOT = FACET + \".pivot\";\n94a101,106\n> \t * Minimum number of docs that need to match to be included in the sublist\n> \t *\n> \t * default value is 1\n> \t */\n> \tpublic static final String FACET_PIVOT_MINCOUNT = FACET_PIVOT + \".mincount\";\n> \t/**\n```\n\n``` java\nIndex: core/src/java/org/apache/solr/request/SimpleFacets.java\n===================================================================\n--- core/src/java/org/apache/solr/request/SimpleFacets.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/request/SimpleFacets.java\t(Version unknown new)\n\n74,76c74,77\n< \tString facetValue; // the field to or query to facet on (minus local params)\n< \tDocSet base; // the base docset for this particular facet\n< \tString key; // what name should the results be stored under\n---\n> \tprotected String facetValue; // the field to or query to facet on (minus\n> \t\t\t\t\t\t\t\t\t// local params)\n> \tprotected DocSet base; // the base docset for this particular facet\n> \tprotected String key; // what name should the results be stored under\n92,93c93,94\n< \tvoid parseParams(String type, String param) throws ParseException,\n< \t\t\tIOException {\n---\n> \tprotected void parseParams(String type, String param)\n> \t\t\tthrows ParseException, IOException {\n```\n\n```\nIndex: core/src/java/org/apache/solr/schema/FieldType.java\n===================================================================\n--- core/src/java/org/apache/solr/schema/FieldType.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/schema/FieldType.java\t(Version unknown new)\n394a399,411\n> \tpublic Object toObject(SchemaField sf, BytesRef term) {\n> \t\tfinal CharsRef ref = new CharsRef(term.length);\n> \t\tindexedToReadable(term, ref);\n> \t\tfinal Fieldable f =  createField(sf, ref.toString(), 1.0f);\n> \t\treturn toObject(f);\n> \t}\n>\n> \t/** Given an indexed term, append the human readable representation */\n> \tpublic CharsRef indexedToReadable(BytesRef input, CharsRef output) {\n> \t\tUnicodeUtil.UTF8toUTF16(input, output);\n> \t\treturn output;\n> \t}\n>\n418a436,441\n> \t/** Given the readable value, return the term value that will match it. */\n> \tpublic void readableToIndexed(CharSequence val, BytesRef result) {\n> \t\tfinal String internal = readableToIndexed(val.toString());\n> \t\tUnicodeUtil.UTF16toUTF8(internal, 0, internal.length(), result);\n> \t}\n>\n```\n\n```\nIndex: core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\n===================================================================\n--- core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\t(Version unknown old)\n+++ core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\t(Version unknown new)\n\n144c152,155\n<        Query query = new TermQuery(new Term(field, termval));\n---\n>        String term = new String(new String(termval.bytes,\n>                       termval.offset, termval.length));\n>\n>        Query query = new TermQuery(new Term(field, term));\n147,148c158,159\n<        super.docs = subset;// used by getTermCounts()\n<\n---\n>        super.base = subset;// used by getTermCounts()\n>\n```\n\n> 没弄过patch，上面的就将就着看吧。\n\n> **WARNING**:\n>\n> 3.x中，SimpleFacet组件中有两个成员，docs/base。在4.x中，这两个成员改名为docsOrig/docs。名字倒是清晰了，就是移植的时候没注意倒是挺麻烦的。\n\n> **WARNING**:\n>\n> BytesRef这个类，在4.x中改动比较大，而且Term也原生支持BytesRef。为了偷懒，就直接将BytesRef.bytes转换成了String。\n\n> **WARNING**:\n>\n>  上面还有一些小类，没有写上，有红叉叉自己改改应该问题不大。</blockquote>\n","slug":"2013/03/apache-solr-facet-pivot-implementation-tranplant","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92k003o3x8f706hbn6w"},{"title":"Apache Solr —— Facet介绍","date":"2013-03-23T06:31:00.000Z","_content":"\n### 1、什么是Faceted Search\n\n`Facet['fæsɪt]`很难翻译，只能靠例子来理解了。Solr作者Yonik Seeley也给出更为直接的名字：导航（Guided Navigation）、参数化查询（Paramatic Search）。\n\n<!--more-->\n\n<center><div style=\"width: 80%;\">![facet-1](/images/facet-1.png)</div></center>\n\n上面是比较直接的Faceted Search例子，品牌、产品特征、卖家，均是Facet。而Apple、Lenovo等品牌，就是Facet values或者说Constraints，而Facet values所带的统计值就是Facet count/Constraint count。\n\n### 2、Facet使用\n> q = 超级本\nfacet = true\nfacet.field = 产品特性\nfacet.field = 品牌\nfacet.field = 卖家\n> `http://.../select?q=超级本&facet=true&wt=json&facet.field=品牌&facet.field=产品特性&facet.field=卖家`\n\n```\n\"facet_counts\": {\n\"facet_fields\": {\n  \"品牌\": [\n    \"Apple\", 4,\n    \"Lenovo\", 39\n      …]\n  \"产品特性\": [\n    \"显卡\", 42,\n    \"酷睿\", 38\n      …]\n\n  …}}\n```\n\n也可以提交查询条件，设置fq(filter query)。\n\n> q = 电脑\nfacet = true\nfq = 价格:[8000 TO \\*]\nfacet.mincount = 1 // fq将不符合的字段过滤后，会显示count为0\nfacet.field = 产品特性\nfacet.field = 品牌\nfacet.field = 卖家</blockquote>\n> `http://.../select?q=超级本&facet=true&wt=json&fq=价格:[8000 TO *]&facet.mincount=1&facet.field=品牌&facet.field=产品特性&facet.field=卖家\n\n``` json\n\"facet_counts\": {\n\"facet_fields\": {\n  \"品牌\": [\n    \"Apple\", 4,\n    \"Lenovo\", 10\n      …]\n  \"产品特性\": [\n    \"显卡\", 11,\n    \"酷睿\", 20\n      …]\n\n  …}}\n```\n\n如果用户选择了Apple这个分类，查询条件中需要添加另外一个fq查询条件，并移除Apple所在的facet.field。\n\n> `http://.../select?q=超级本&facet=true&wt=json&fq=价格:[8000 TO *]&fq=品牌:Apple&facet.mincount=1 &facet.field=产品特性&facet.field=卖家`\n\n### 3、Facet参数\n\n* **facet.prefix**: 限制constaints的前缀\n* **facet.mincount=0**: 限制constants count的最小返回值，默认为0\n* **facet.sort=count**: 排序的方式，根据count或者index\n* **facet.offset=0**: 表示在当前排序情况下的偏移，可以做分页\n* **facet.limit=100**: constraints返回的数目\n* **facet.missing=false**: 是否返回没有值的field\n* **facet.date**: Deprecated, use facet.range\n* **facet.query**: 指定一个查询字符串作为Facet Constraint\n\n```\nfacet.query = rank:[* TO 20]\nfacet.query = rank:[21 TO *]\n```\n\n``` xml\n  <result numFound=\"27\" ... />\n  ...\n  <lst name=\"facet_counts\">\n  <lst name=\"facet_queries\">\n    <int name=\"rank:[* TO 20]\">2</int>\n    <int name=\"rank:[21 TO *]\">15</int>\n  </lst>\n ...\n```\n\n* **facet.range:\n\n`http://.../select?&facet=true&facet.range=price&facet.range.start=5000&facet.range.end=8000&facet.range.gap=1000`\n\n``` json\n \"facet_counts\":{\n  \"facet_ranges\":{\n    \"price\":{\n      \"counts”:[\n        \"5000.0”,5,\n        \"6000.0”,2,\n        \"7000.0”,3,],\n      \"gap\":1000.0,\n      \"start\":5000.0,\n      \"end\":8000.0}}}}\n```\n\n> **WARNING:**\n>\n> range范围是左闭右开，`[start, end)`\n\n* **facet.pivot**\n\n这个是Solr 4.0的新特性，pivot和facet一样难理解，还是用例子来讲吧。\n\n> **Syntax:** facet.pivot=field1,field2,field3...\n> **e.g.:** facet.pivot=comment_user, grade\n\n|   |#docs|#docs grade:好|#docs 等级:中|#docs 等级:差|\n|---|----|--------------|------------|-----------|\n|comment_user:1|10|8|1|1|\n|comment_user:2|20|18|2|0|\n|comment_user:3|15|12|2|1|\n|comment_user:4|18|15|2|1|\n\n```\n  \"facet_counts\":{\n  \"facet_pivot\":{\n   \"comment_user, grade \":[{\n     \"field\":\"comment_user\",\n     \"value\":\"1\",\n     \"count\":10,\n     \"pivot\":[{\n       \"field\":\"grade\",\n       \"value\":\"好\",\n       \"count\":8}, {\n       \"field\":\"grade\",\n       \"value\":\"中\",\n       \"count\":1}, {\n       \"field\":\"grade\",\n       \"value\":\"差\",\n       \"count\":1}]\n     }, {\n       \"field\":\" comment_user \",\n       \"value\":\"2\",\n       \"count\":20,\n       \"pivot\":[{\n       ...\n```\n\n没有pivot机制的话，要做到上面那点可能需要多次查询：\n> `http://...q=comment&fq=grade:好&facet=true&facet.field=comment_user`\n> `http://...q=comment&fq=grade:中&facet=true&facet.field=comment_user`\n> `http://...q=comment&fq=grade:差&facet=true&facet.field=comment_user`\n\n> Facet.pivot - Computes a Matrix of Constraint Counts across multiple Facet Fields. by Yonik Seeley.\n上面那个解释很不错，只能理解不能翻译。\n\n> 参考资料：\n> * [The Many Facets of Apache Solr](http://2011.lucene-eurocon.org/attachments/0002/8835/Seeley_Eurocon_SolrFacets_1_.pdf)\n> * [SimpleFacetParameters Wiki](http://wiki.apache.org/solr/SimpleFacetParameters)\n","source":"_posts/2013/03/apache-solr-facet-introduction.md","raw":"title: Apache Solr —— Facet介绍\ndate: 2013-03-23 14:31:00\ncategories: 技术分享\ntags: Solr\n---\n\n### 1、什么是Faceted Search\n\n`Facet['fæsɪt]`很难翻译，只能靠例子来理解了。Solr作者Yonik Seeley也给出更为直接的名字：导航（Guided Navigation）、参数化查询（Paramatic Search）。\n\n<!--more-->\n\n<center><div style=\"width: 80%;\">![facet-1](/images/facet-1.png)</div></center>\n\n上面是比较直接的Faceted Search例子，品牌、产品特征、卖家，均是Facet。而Apple、Lenovo等品牌，就是Facet values或者说Constraints，而Facet values所带的统计值就是Facet count/Constraint count。\n\n### 2、Facet使用\n> q = 超级本\nfacet = true\nfacet.field = 产品特性\nfacet.field = 品牌\nfacet.field = 卖家\n> `http://.../select?q=超级本&facet=true&wt=json&facet.field=品牌&facet.field=产品特性&facet.field=卖家`\n\n```\n\"facet_counts\": {\n\"facet_fields\": {\n  \"品牌\": [\n    \"Apple\", 4,\n    \"Lenovo\", 39\n      …]\n  \"产品特性\": [\n    \"显卡\", 42,\n    \"酷睿\", 38\n      …]\n\n  …}}\n```\n\n也可以提交查询条件，设置fq(filter query)。\n\n> q = 电脑\nfacet = true\nfq = 价格:[8000 TO \\*]\nfacet.mincount = 1 // fq将不符合的字段过滤后，会显示count为0\nfacet.field = 产品特性\nfacet.field = 品牌\nfacet.field = 卖家</blockquote>\n> `http://.../select?q=超级本&facet=true&wt=json&fq=价格:[8000 TO *]&facet.mincount=1&facet.field=品牌&facet.field=产品特性&facet.field=卖家\n\n``` json\n\"facet_counts\": {\n\"facet_fields\": {\n  \"品牌\": [\n    \"Apple\", 4,\n    \"Lenovo\", 10\n      …]\n  \"产品特性\": [\n    \"显卡\", 11,\n    \"酷睿\", 20\n      …]\n\n  …}}\n```\n\n如果用户选择了Apple这个分类，查询条件中需要添加另外一个fq查询条件，并移除Apple所在的facet.field。\n\n> `http://.../select?q=超级本&facet=true&wt=json&fq=价格:[8000 TO *]&fq=品牌:Apple&facet.mincount=1 &facet.field=产品特性&facet.field=卖家`\n\n### 3、Facet参数\n\n* **facet.prefix**: 限制constaints的前缀\n* **facet.mincount=0**: 限制constants count的最小返回值，默认为0\n* **facet.sort=count**: 排序的方式，根据count或者index\n* **facet.offset=0**: 表示在当前排序情况下的偏移，可以做分页\n* **facet.limit=100**: constraints返回的数目\n* **facet.missing=false**: 是否返回没有值的field\n* **facet.date**: Deprecated, use facet.range\n* **facet.query**: 指定一个查询字符串作为Facet Constraint\n\n```\nfacet.query = rank:[* TO 20]\nfacet.query = rank:[21 TO *]\n```\n\n``` xml\n  <result numFound=\"27\" ... />\n  ...\n  <lst name=\"facet_counts\">\n  <lst name=\"facet_queries\">\n    <int name=\"rank:[* TO 20]\">2</int>\n    <int name=\"rank:[21 TO *]\">15</int>\n  </lst>\n ...\n```\n\n* **facet.range:\n\n`http://.../select?&facet=true&facet.range=price&facet.range.start=5000&facet.range.end=8000&facet.range.gap=1000`\n\n``` json\n \"facet_counts\":{\n  \"facet_ranges\":{\n    \"price\":{\n      \"counts”:[\n        \"5000.0”,5,\n        \"6000.0”,2,\n        \"7000.0”,3,],\n      \"gap\":1000.0,\n      \"start\":5000.0,\n      \"end\":8000.0}}}}\n```\n\n> **WARNING:**\n>\n> range范围是左闭右开，`[start, end)`\n\n* **facet.pivot**\n\n这个是Solr 4.0的新特性，pivot和facet一样难理解，还是用例子来讲吧。\n\n> **Syntax:** facet.pivot=field1,field2,field3...\n> **e.g.:** facet.pivot=comment_user, grade\n\n|   |#docs|#docs grade:好|#docs 等级:中|#docs 等级:差|\n|---|----|--------------|------------|-----------|\n|comment_user:1|10|8|1|1|\n|comment_user:2|20|18|2|0|\n|comment_user:3|15|12|2|1|\n|comment_user:4|18|15|2|1|\n\n```\n  \"facet_counts\":{\n  \"facet_pivot\":{\n   \"comment_user, grade \":[{\n     \"field\":\"comment_user\",\n     \"value\":\"1\",\n     \"count\":10,\n     \"pivot\":[{\n       \"field\":\"grade\",\n       \"value\":\"好\",\n       \"count\":8}, {\n       \"field\":\"grade\",\n       \"value\":\"中\",\n       \"count\":1}, {\n       \"field\":\"grade\",\n       \"value\":\"差\",\n       \"count\":1}]\n     }, {\n       \"field\":\" comment_user \",\n       \"value\":\"2\",\n       \"count\":20,\n       \"pivot\":[{\n       ...\n```\n\n没有pivot机制的话，要做到上面那点可能需要多次查询：\n> `http://...q=comment&fq=grade:好&facet=true&facet.field=comment_user`\n> `http://...q=comment&fq=grade:中&facet=true&facet.field=comment_user`\n> `http://...q=comment&fq=grade:差&facet=true&facet.field=comment_user`\n\n> Facet.pivot - Computes a Matrix of Constraint Counts across multiple Facet Fields. by Yonik Seeley.\n上面那个解释很不错，只能理解不能翻译。\n\n> 参考资料：\n> * [The Many Facets of Apache Solr](http://2011.lucene-eurocon.org/attachments/0002/8835/Seeley_Eurocon_SolrFacets_1_.pdf)\n> * [SimpleFacetParameters Wiki](http://wiki.apache.org/solr/SimpleFacetParameters)\n","slug":"2013/03/apache-solr-facet-introduction","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92l003r3x8fraw7plxd"},{"title":"Shell常用命令 文本操作篇","id":"647","date":"2013-01-19T14:41:44.000Z","_content":"\n### 3.0 前言\n\n《Linux/Unix设计思想》中有一条准则：采用纯文本文件来存储数据，原因如下：\n\n1. 文本是通用的可转换格式\r2. 文本文件易于阅读和编辑\n3. 文本数据文件简化了Unix工具的使用\n4. 可移植性的提高克服了速度的不足\n5. 速度欠佳的缺点会被明年的机器克服\n\n在实战中有能感受到，文本文件的方便，也有非常多的工具辅助开发者处理、查看、编辑它们，如：awk、sed等，这篇博文就介绍与文本操作相关的常用命令。\n\n<!--more-->\n\n### 3.1 输入输出流\n\n#### echo：\n\necho如同python中的print，不过不需要引号包住字符串，echo后接的除管理和重定向字符外均会输出。\n\n#### cat（Concatenate）：\n\n它主要的功能是将文件的内容连续（concatenate）的输出到屏幕上，在上篇博文中就已经看到了，用来合并多个split的文件。\n\n#### |：\n\n管道符号，接收管道左边命令的输出，并输入到右边的命令。在Linux中，管道是一种使用非常频繁的通信机制。从本质上说，管道也是一种文件，但它又和一般的文件有所不同。在 Linux 中，管道的实现并没有使用专门的数据结构，而是借助了文件系统的file结构和VFS的索引节点inode。通过将两个 file 结构指向同一个临时的 VFS 索引节点，而这个 VFS 索引节点又指向一个物理页面而实现的。\n\n这样，左边命令将数据写到索引节点，右边读取索引节点，读完之后方能继续写。详细实现机制可参考：[Linux管道的实现机制](http://oss.org.cn/kernel-book/ch07/7.1.1.htm)\n\n#### 输出重定向：\n\n`>`: 将左边的输出输入到右边的文件\n\n`>>`: 将左边命令的输出追加输入到右边的文件\n\n#### 标准输出流重定向：\n\n0、1、2分别代表了标准输入、标准输出、标准错误信息输出，实战中经常会将错误输出重定向到标准输出中，可用 2>&1。同时，linux中有特殊的文件，/dev/null可理解为回收站，所有输出输入到该文件中均会“消失”。\n\n#### read:\n\n`Usage: read [option] [var(default:REPLY)]`\n\n从标准输出流中读取一行，并给一个变量赋值。\n\n```\n>> $ echo \"hello world\" > new.txt\n>> $ cat new.txt\nhello world\n>> $ echo \"again\" >> new.txt\n>> $ cat new.txt\nhello world\nagain\n>> $ echo \"col1 col2\" | awk '{print $1}'\ncol1\n>> $ read newline < new.txt; echo $newline\nhello world\n>> $ read < new.txt; echo $REPLY\nhello world\n```\n\n> TIPS1: read最常见的应该是用在while循环中了，cat file | while read line; do do_sth; done;\r>\n> TIPS2: 2>&1常见的地方是用在定时任务中，将所有输出结果均输出到/dev/null中，/bin/sh your.sh > /dev/null 2>&1\n>\n> TIPS3: Linux中最快的创建空文件的方法不是touch，而是\"> newfile\"\n\n### 3.2 文本查看\n\ncat、head、tail、more、less均是查看文本的好工具，有时候结合起来效果会更好。\n\n#### head(tail)：\n\n查看文件首（尾）十行\n\n#### more：\n\n先显示文件一屏，再等待用户输入，回车是再显示下一行，空格是下一屏，q(ctrl+c)则退出\n\n#### less：\n\n类似more，不过可以往上翻。可用上下键，也可以用u(up)，d(down)键。\n\n```\n>> $ tail -n 1000 file | more    # 查看文件的最后1k行\n>> $ head -n 10010 file | tail     # 查看文件的第10k行\n>> $ head -c 1(b|k|m) file    # 查看文件的前1B|K|M数据\n```\n\n\n### 3.3 文本字符集\n\n字符集一直开发者的一个痛，按照东犇的说法是，要废除除utf8外所有字符集。此话虽然暴力，但是也有道理，不过要实现这个想法也基本是不太可能。开发者就只能一步一个脚印的走了。\n\n#### file:\n\n这是查看文本字符串的命令，不过个人觉得，基本不靠谱。因为要查看的数据都是代码生成的，输出的数据也不会有BOM信息，file也没有实现自检测文本字符功能。所以，现实中一般就是连蒙带猜了，不是gbk就是utf8。哈哈\n\n#### iconv:\n\n字符集转换工具\n\n```\n>> $ iconv -f from_code -t to_code file\n```\n\n文本字符集转换在这篇文章就只能点到为止了，字符集的问题三两句话基本讲不清楚，改天有机会单独写篇文章讲讲。\n\n> TIPS1: 输出文本到屏幕，显示的字符集是由终端所设置的。当出现乱码的时候，不要误认为是数据出错了，很有可能就是你的字符集设置有问题。\r>\n>\n> TIPS2: 按照东犇的说法，建议开发者统一使用utf8编码。他也提供了一种较为方便的查看gbk文本的方法。\r\r```\n>> $ echo -e \"\\n\"alias conv_gbk=\\\"iconv -f gbk -t utf8\\\" >> ~/.bashrc\r>> $ source ~/.bashrc\r>> $ cat file | conv_gbk\r```\n\n### 3.4 文本其他工具\n\n#### grep:\n\n查找命令，是开发中使用频率非常高的命令。像我们这边，代码就是文档，最好的翻阅文档工具就是grep了。\n\n```\n>> $ grep [pattern] [file]    # grep是基于正则表达式的，查找file中符合pattern的行\n>> $ grep -v pattern file    # 查找file中不符合pattern的行\n>> $ grep pattern . -r    # -r recursive 查找当前目录下所有符合pattern的文件\n```\n\n#### wc:\n\nwordcount工具，表示中国文字基本用不到。可以用其中一个：\n\n```\n>> $ wc -l file    # 统计file的行数\n>> $ ls . | wc -l    # 统计当前目录下文件的数目\n>> $ grep pattern file | wc -l    # 统计file中符合pattern的行数\n```\n\n#### sort:\n\n排序工具：\n\n```\n>> $ sort -k n (-n) (-r) file   # 按照第n列，给file排序。-n numeric-sort -r reverse\n>> $ ls -l . | sort -k 5 -n    # 列出当前目录的文件，并按文件大小排序\n>> $ ls -l . | sort -k5n -k6    # 列出当前目录的文件，并按文件大小及时间排序\n```\n\n#### uniq:\n\n去除连续重复行：\n\n```\n>> $ cat file\ntest1\ntest2\ntest2\ntest1\n>> $ uniq file\ntest1\ntest2\ntest1\n>> $ sort file | uniq     # 去除所有重复行\ntest1\ntest2\n```\n\n#### diff & patch:\n\n文本比较工具，和svn里面的diff一样。diff的文件较小的话，结果还可以忍受，如果过大，那结果就不是给人看的了，需要让机器来做一些工作了，如做增量拷贝。这个命令就是patch。\n\n命令较为繁琐，且个人用得较少，就直接引用其他文章了：[用diff和patch工具维护源码](https://www.ibm.com/developerworks/cn/linux/l-diffp/)\n\n#### awk & sed:\n\nsed(stream editor)意为流编辑器，我的理解就是它是按行处理。而awk则不仅仅可以按行，还可以按列。想必前面的文章，也看到了不少用awk的命令。两者各有所长，且博大精深啊，也有关于这两个工具的书。笔者在这里也只介绍我常用到的功能。\n\n```\n>> $ echo \"col1 col2 col3\" | awk '{print $1\"\\t\"$3}'    # $0：输入串 $1：col1 $2：col2 $3：col3 …\ncol1[tab]col3\n>> $ echo \"col1 col2 col3\" | awk '{print $1, $3}'\ncol1[space]col3\n>> $ sed -i \"s/pattern/replace_pattern/g\" file    # 替换字符串\n```\n\n> WARNING: 当文本为中文的时候，sed -i \"s/char/replace_char/g\"替换ascii标点的时候，需要特别注意。因为gbk编码中有些字节会小于128，而sed替换也是逐字节对比替换，所以有时候会导致中文乱码。\n","source":"_posts/2013/01/linux-text-shell.md","raw":"title: Shell常用命令 文本操作篇\ntags:\n  - Linux\n  - Shell\nid: 647\ncategories:\n  - 技术分享\ndate: 2013-01-19 22:41:44\n---\n\n### 3.0 前言\n\n《Linux/Unix设计思想》中有一条准则：采用纯文本文件来存储数据，原因如下：\n\n1. 文本是通用的可转换格式\r2. 文本文件易于阅读和编辑\n3. 文本数据文件简化了Unix工具的使用\n4. 可移植性的提高克服了速度的不足\n5. 速度欠佳的缺点会被明年的机器克服\n\n在实战中有能感受到，文本文件的方便，也有非常多的工具辅助开发者处理、查看、编辑它们，如：awk、sed等，这篇博文就介绍与文本操作相关的常用命令。\n\n<!--more-->\n\n### 3.1 输入输出流\n\n#### echo：\n\necho如同python中的print，不过不需要引号包住字符串，echo后接的除管理和重定向字符外均会输出。\n\n#### cat（Concatenate）：\n\n它主要的功能是将文件的内容连续（concatenate）的输出到屏幕上，在上篇博文中就已经看到了，用来合并多个split的文件。\n\n#### |：\n\n管道符号，接收管道左边命令的输出，并输入到右边的命令。在Linux中，管道是一种使用非常频繁的通信机制。从本质上说，管道也是一种文件，但它又和一般的文件有所不同。在 Linux 中，管道的实现并没有使用专门的数据结构，而是借助了文件系统的file结构和VFS的索引节点inode。通过将两个 file 结构指向同一个临时的 VFS 索引节点，而这个 VFS 索引节点又指向一个物理页面而实现的。\n\n这样，左边命令将数据写到索引节点，右边读取索引节点，读完之后方能继续写。详细实现机制可参考：[Linux管道的实现机制](http://oss.org.cn/kernel-book/ch07/7.1.1.htm)\n\n#### 输出重定向：\n\n`>`: 将左边的输出输入到右边的文件\n\n`>>`: 将左边命令的输出追加输入到右边的文件\n\n#### 标准输出流重定向：\n\n0、1、2分别代表了标准输入、标准输出、标准错误信息输出，实战中经常会将错误输出重定向到标准输出中，可用 2>&1。同时，linux中有特殊的文件，/dev/null可理解为回收站，所有输出输入到该文件中均会“消失”。\n\n#### read:\n\n`Usage: read [option] [var(default:REPLY)]`\n\n从标准输出流中读取一行，并给一个变量赋值。\n\n```\n>> $ echo \"hello world\" > new.txt\n>> $ cat new.txt\nhello world\n>> $ echo \"again\" >> new.txt\n>> $ cat new.txt\nhello world\nagain\n>> $ echo \"col1 col2\" | awk '{print $1}'\ncol1\n>> $ read newline < new.txt; echo $newline\nhello world\n>> $ read < new.txt; echo $REPLY\nhello world\n```\n\n> TIPS1: read最常见的应该是用在while循环中了，cat file | while read line; do do_sth; done;\r>\n> TIPS2: 2>&1常见的地方是用在定时任务中，将所有输出结果均输出到/dev/null中，/bin/sh your.sh > /dev/null 2>&1\n>\n> TIPS3: Linux中最快的创建空文件的方法不是touch，而是\"> newfile\"\n\n### 3.2 文本查看\n\ncat、head、tail、more、less均是查看文本的好工具，有时候结合起来效果会更好。\n\n#### head(tail)：\n\n查看文件首（尾）十行\n\n#### more：\n\n先显示文件一屏，再等待用户输入，回车是再显示下一行，空格是下一屏，q(ctrl+c)则退出\n\n#### less：\n\n类似more，不过可以往上翻。可用上下键，也可以用u(up)，d(down)键。\n\n```\n>> $ tail -n 1000 file | more    # 查看文件的最后1k行\n>> $ head -n 10010 file | tail     # 查看文件的第10k行\n>> $ head -c 1(b|k|m) file    # 查看文件的前1B|K|M数据\n```\n\n\n### 3.3 文本字符集\n\n字符集一直开发者的一个痛，按照东犇的说法是，要废除除utf8外所有字符集。此话虽然暴力，但是也有道理，不过要实现这个想法也基本是不太可能。开发者就只能一步一个脚印的走了。\n\n#### file:\n\n这是查看文本字符串的命令，不过个人觉得，基本不靠谱。因为要查看的数据都是代码生成的，输出的数据也不会有BOM信息，file也没有实现自检测文本字符功能。所以，现实中一般就是连蒙带猜了，不是gbk就是utf8。哈哈\n\n#### iconv:\n\n字符集转换工具\n\n```\n>> $ iconv -f from_code -t to_code file\n```\n\n文本字符集转换在这篇文章就只能点到为止了，字符集的问题三两句话基本讲不清楚，改天有机会单独写篇文章讲讲。\n\n> TIPS1: 输出文本到屏幕，显示的字符集是由终端所设置的。当出现乱码的时候，不要误认为是数据出错了，很有可能就是你的字符集设置有问题。\r>\n>\n> TIPS2: 按照东犇的说法，建议开发者统一使用utf8编码。他也提供了一种较为方便的查看gbk文本的方法。\r\r```\n>> $ echo -e \"\\n\"alias conv_gbk=\\\"iconv -f gbk -t utf8\\\" >> ~/.bashrc\r>> $ source ~/.bashrc\r>> $ cat file | conv_gbk\r```\n\n### 3.4 文本其他工具\n\n#### grep:\n\n查找命令，是开发中使用频率非常高的命令。像我们这边，代码就是文档，最好的翻阅文档工具就是grep了。\n\n```\n>> $ grep [pattern] [file]    # grep是基于正则表达式的，查找file中符合pattern的行\n>> $ grep -v pattern file    # 查找file中不符合pattern的行\n>> $ grep pattern . -r    # -r recursive 查找当前目录下所有符合pattern的文件\n```\n\n#### wc:\n\nwordcount工具，表示中国文字基本用不到。可以用其中一个：\n\n```\n>> $ wc -l file    # 统计file的行数\n>> $ ls . | wc -l    # 统计当前目录下文件的数目\n>> $ grep pattern file | wc -l    # 统计file中符合pattern的行数\n```\n\n#### sort:\n\n排序工具：\n\n```\n>> $ sort -k n (-n) (-r) file   # 按照第n列，给file排序。-n numeric-sort -r reverse\n>> $ ls -l . | sort -k 5 -n    # 列出当前目录的文件，并按文件大小排序\n>> $ ls -l . | sort -k5n -k6    # 列出当前目录的文件，并按文件大小及时间排序\n```\n\n#### uniq:\n\n去除连续重复行：\n\n```\n>> $ cat file\ntest1\ntest2\ntest2\ntest1\n>> $ uniq file\ntest1\ntest2\ntest1\n>> $ sort file | uniq     # 去除所有重复行\ntest1\ntest2\n```\n\n#### diff & patch:\n\n文本比较工具，和svn里面的diff一样。diff的文件较小的话，结果还可以忍受，如果过大，那结果就不是给人看的了，需要让机器来做一些工作了，如做增量拷贝。这个命令就是patch。\n\n命令较为繁琐，且个人用得较少，就直接引用其他文章了：[用diff和patch工具维护源码](https://www.ibm.com/developerworks/cn/linux/l-diffp/)\n\n#### awk & sed:\n\nsed(stream editor)意为流编辑器，我的理解就是它是按行处理。而awk则不仅仅可以按行，还可以按列。想必前面的文章，也看到了不少用awk的命令。两者各有所长，且博大精深啊，也有关于这两个工具的书。笔者在这里也只介绍我常用到的功能。\n\n```\n>> $ echo \"col1 col2 col3\" | awk '{print $1\"\\t\"$3}'    # $0：输入串 $1：col1 $2：col2 $3：col3 …\ncol1[tab]col3\n>> $ echo \"col1 col2 col3\" | awk '{print $1, $3}'\ncol1[space]col3\n>> $ sed -i \"s/pattern/replace_pattern/g\" file    # 替换字符串\n```\n\n> WARNING: 当文本为中文的时候，sed -i \"s/char/replace_char/g\"替换ascii标点的时候，需要特别注意。因为gbk编码中有些字节会小于128，而sed替换也是逐字节对比替换，所以有时候会导致中文乱码。\n","slug":"2013/01/linux-text-shell","published":1,"updated":"2016-01-02T16:14:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92n003u3x8ft9onmyvf"},{"title":"Shell常用命令 终端优化前篇","id":"630","date":"2013-01-17T15:18:50.000Z","_content":"\n在黑框框shell下作业也一年多了，其中实战占了9个月，从最开始的翻阅各种书籍，到后面问谷歌，也算是能熟练“掌握”这门工具，也搜集了一些“奇技淫巧”。现在好好的总结一下吧，顺带也查漏补缺……\n\n<!--more-->\n\n### 1.1 终端设置\n\n下载一个好用的终端，如[xshell](http://www.netsarang.com/products/xsh_overview.html)，[putty](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)。\n\n设置Encoding、Terminal Type，一般来说在终端中设置一下即可。Encoding为utf8，Terminal Type设置为xterm、VT100、linux都可以，设置其他的类型有的可能ls文件会没有颜色（这个情况在screen命令中也会出现，到时候再讲解决方案吧），或者Home/End键不好用（代替快捷键：Ctrl+a/e）之类的。\n\n#### 1.2 shell提示符设置\n\n终端提示符默认的就是白色的用户名，看着非常不舒服，可以设置一下当前环境的提示符颜色，变量是PS1：\n\n![image](http://hongweiyi.com/wp-content/uploads/2013/01/image.png)\n\n```\n>> $ echo $PS1\n[\\e[31;1m]\\u[\\e[0m]@[\\e[32;1m]your ip[\\e[0m]:[\\e[35;1m]\\w[\\e[0m]\\$\n```\n\n你也可以不填写ip，用命令获得当前ip：\n\n```\n>> $ export PS1=\"[\\e[31;1m]\\u[\\e[0m]@[\\e[32;1m]/sbin/ifconfig eth1|grep \"inet&nbsp;&nbsp; addr:\"|cut -d: -f 2|cut -d\" \" -f1[\\e[0m]:[\\e[35;1m]\\w[\\e[0m]\\$ \"\n```\n\n其他颜色可参考：[Linux终端下的颜色设置](http://unix-cd.com/unixcd12/article_7281.html)\n\n上面的PS1变量可以设置在用户配置中，文件为~/.bashrc，在该文件中添加一行：`export PS1=`即可。如果再次登陆的时候，该命令没有生效，则添加~/.bash_profile文件，当shell被打开时，该文件会被执行一次。在文件中添加命令即可加载~/.bashrc：\n\n```\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n```\n\n如果你想该文件立即生效，可以用source ~/.bashrc命令立即加载执行该文件。\n\n#### 1.3 别名设置\n\n可以用一些简单的别名代替一些复杂的命令，如下：\n\n```\nalias ..=\"cd ..\"\nalias …=\"cd ../..\"\nalias cd..=\"cd ..\"\nalias ll=\"ls -l\"\nalias la=\"ls -a\"\nalias tree=\"~/code/tree\"\n```\n\n将alias添加进~/.bashrc中。~/code/tree是一个shell脚本，脚本内容如下：\n\n```\n#!/bin/sh\nfunction tree_files()\n{\n    find . -print 2>/dev/null|\\\n    awk '!/\\.$/ {for (i=1;i<NF;i++){\\\n    d=length($i);if ( d < 5 && i != 1 )d=5;printf(\"%\"d\"s\",\"|\")}print \"—\"$NF}' \\\n    FS='/' | more\n}\nfunction tree_dirs()\n{\n    find . -type d -print 2>/dev/null|\\\n    awk '!/\\.$/ {for (i=1;i<NF;i++){\\\n    d=length($i);if ( d < 5 && i != 1 )d=5;printf(\"%\"d\"s\",\"|\")}print \"—\"$NF}' \\\n    FS='/' | more\n}\nfunction help()\n{\n    echo \"Usage: tree [-f|-d]\"\n    echo \"  -f: list files of current directory\"\n    echo \"  -d: list direcotries of current directory\"\n}\nif [ $# = 0 ]; then\n    tree_dirs\nelif [ $# = 1 ]; then\n    if [ $1 = “-f” ]; then\n        tree_files\n    elif [ $1 = “-d” ]; then\n        tree_dirs\n    else\n        help\n    fi\nfi\n```\n\n### 1.4 文档编辑器设置\n\n我用的是vim，配置文件为~/.vimrc。由于工作机较多，所以没有一个个去配置，不可缺少的是syntax on（开启高亮），indent都没太大必要了。当然，专用开发机需要配置一个比较好用的，因人而异，如需要较好配置文件的可自行谷歌。\n\n有了以上四步，基本上一个简单方便好用的终端就有了，接下来我会一一介绍常用的shell命令：文件/目录操作、输入/输出操作、文本操作、进程操作等。\n\n> 个人不太喜欢装各种各样的辅助工具（tmux、vim相关、make相关。tree的工具也不想装，直接写shell多方便），因为环境多变，如果突然没了插件可能会不知所措，喜欢最原始最朴素的命令和工具\n","source":"_posts/2013/01/linux-shell-term-tuning.md","raw":"title: Shell常用命令 终端优化前篇\ntags:\n  - Linux\n  - Shell\nid: 630\ncategories:\n  - 技术分享\ndate: 2013-01-17 23:18:50\n---\n\n在黑框框shell下作业也一年多了，其中实战占了9个月，从最开始的翻阅各种书籍，到后面问谷歌，也算是能熟练“掌握”这门工具，也搜集了一些“奇技淫巧”。现在好好的总结一下吧，顺带也查漏补缺……\n\n<!--more-->\n\n### 1.1 终端设置\n\n下载一个好用的终端，如[xshell](http://www.netsarang.com/products/xsh_overview.html)，[putty](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)。\n\n设置Encoding、Terminal Type，一般来说在终端中设置一下即可。Encoding为utf8，Terminal Type设置为xterm、VT100、linux都可以，设置其他的类型有的可能ls文件会没有颜色（这个情况在screen命令中也会出现，到时候再讲解决方案吧），或者Home/End键不好用（代替快捷键：Ctrl+a/e）之类的。\n\n#### 1.2 shell提示符设置\n\n终端提示符默认的就是白色的用户名，看着非常不舒服，可以设置一下当前环境的提示符颜色，变量是PS1：\n\n![image](http://hongweiyi.com/wp-content/uploads/2013/01/image.png)\n\n```\n>> $ echo $PS1\n[\\e[31;1m]\\u[\\e[0m]@[\\e[32;1m]your ip[\\e[0m]:[\\e[35;1m]\\w[\\e[0m]\\$\n```\n\n你也可以不填写ip，用命令获得当前ip：\n\n```\n>> $ export PS1=\"[\\e[31;1m]\\u[\\e[0m]@[\\e[32;1m]/sbin/ifconfig eth1|grep \"inet&nbsp;&nbsp; addr:\"|cut -d: -f 2|cut -d\" \" -f1[\\e[0m]:[\\e[35;1m]\\w[\\e[0m]\\$ \"\n```\n\n其他颜色可参考：[Linux终端下的颜色设置](http://unix-cd.com/unixcd12/article_7281.html)\n\n上面的PS1变量可以设置在用户配置中，文件为~/.bashrc，在该文件中添加一行：`export PS1=`即可。如果再次登陆的时候，该命令没有生效，则添加~/.bash_profile文件，当shell被打开时，该文件会被执行一次。在文件中添加命令即可加载~/.bashrc：\n\n```\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n```\n\n如果你想该文件立即生效，可以用source ~/.bashrc命令立即加载执行该文件。\n\n#### 1.3 别名设置\n\n可以用一些简单的别名代替一些复杂的命令，如下：\n\n```\nalias ..=\"cd ..\"\nalias …=\"cd ../..\"\nalias cd..=\"cd ..\"\nalias ll=\"ls -l\"\nalias la=\"ls -a\"\nalias tree=\"~/code/tree\"\n```\n\n将alias添加进~/.bashrc中。~/code/tree是一个shell脚本，脚本内容如下：\n\n```\n#!/bin/sh\nfunction tree_files()\n{\n    find . -print 2>/dev/null|\\\n    awk '!/\\.$/ {for (i=1;i<NF;i++){\\\n    d=length($i);if ( d < 5 && i != 1 )d=5;printf(\"%\"d\"s\",\"|\")}print \"—\"$NF}' \\\n    FS='/' | more\n}\nfunction tree_dirs()\n{\n    find . -type d -print 2>/dev/null|\\\n    awk '!/\\.$/ {for (i=1;i<NF;i++){\\\n    d=length($i);if ( d < 5 && i != 1 )d=5;printf(\"%\"d\"s\",\"|\")}print \"—\"$NF}' \\\n    FS='/' | more\n}\nfunction help()\n{\n    echo \"Usage: tree [-f|-d]\"\n    echo \"  -f: list files of current directory\"\n    echo \"  -d: list direcotries of current directory\"\n}\nif [ $# = 0 ]; then\n    tree_dirs\nelif [ $# = 1 ]; then\n    if [ $1 = “-f” ]; then\n        tree_files\n    elif [ $1 = “-d” ]; then\n        tree_dirs\n    else\n        help\n    fi\nfi\n```\n\n### 1.4 文档编辑器设置\n\n我用的是vim，配置文件为~/.vimrc。由于工作机较多，所以没有一个个去配置，不可缺少的是syntax on（开启高亮），indent都没太大必要了。当然，专用开发机需要配置一个比较好用的，因人而异，如需要较好配置文件的可自行谷歌。\n\n有了以上四步，基本上一个简单方便好用的终端就有了，接下来我会一一介绍常用的shell命令：文件/目录操作、输入/输出操作、文本操作、进程操作等。\n\n> 个人不太喜欢装各种各样的辅助工具（tmux、vim相关、make相关。tree的工具也不想装，直接写shell多方便），因为环境多变，如果突然没了插件可能会不知所措，喜欢最原始最朴素的命令和工具\n","slug":"2013/01/linux-shell-term-tuning","published":1,"updated":"2016-01-02T06:09:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92o00403x8f6pmfhcof"},{"title":"Shell常用命令 进程操作篇","id":"656","date":"2013-01-22T13:57:57.000Z","_content":"\n### 4.0 前言\n\n这篇算是尾篇了，命令较多也较繁杂，每个都是点到为止，基本也够用。有兴趣深入的朋友可自行谷歌学习。\n\n<!--more-->\n\n### 4.1 进程执行\n\n进程执行无外乎前台运行，但是前台运行需要用户等待程序运行完毕才可继续，所以就有了后台运行进程这一说。后台进程又分两种，当前终端后台进程，以及托管给OS的后台进程。这里就稍微介绍一下如何运行这些个进程吧。\n\n> BTW: 以上分类都是我瞎分的，仅供参考。\n\n#### 后台运行进程:\n\n在运行命令末尾加上`&`符号，该程序就会在后台运行，如果忘记敲`&`命令的话，也可以同过ctrl+z，暂停当前进程，再用bg(backgroud)命令即可让暂停任务变成后台运行。\n\n```\n>> $ /bin/sh your.sh &\n[1]    pid        # 输出进程在当前终端的序号以及pid\n>> $ /bin/sh your.sh       # ctrl + z\n[1]+  Stopped            /bin/sh your.sh\n>> $ bg %1   # %1 代表当前终端的第一个进程\n[1]+  /bin/sh your.sh &\n```\n\n> TIPS: The plus sign shows the most recently invoked job; the minus sign shows the next most recently invoked job.- <Learning the Korn Shell, 2nd Edition>\r>\n> WARNING: 当运行某个有大量输出的进程，如果直接让其进入后台运行是很不明智的，一般来说会将输出写入文件中。如： ` $ /bin/sh your.sh > ./your.log 2>&1 &`\n\n#### 前台运行进程：\n\n有了后台，就有相应的前台运行进程了。fg(foreground)就是了，但一般来说，很少需要将后台进程推到前台的需求，我一般是这样用的：\n\n```\n>> $ vim your_file      # ctrl + z\n[1]+   Stopped            vim your_file\n>> $ fg %1\n```\n\n#### 托管进程：\n\n上面的命令均是在当前终端运行，如果终端关闭，相应的进程也关闭了，如果想进程继续运行，则需要将进程托管给系统管理。主要命令是nohup。\n\n```\n>> $ nohup /bin/sh your.sh &\n[1]    pid\nnohup: appending output to ‘nohup.out'\n```\n\n实战的时候，更多的情况是这样的：突然有事要走开一下，又生怕跑了半天的程序因为终端挂了，想将其托管给系统。那么就可以用disown命令。\n\n```\n>> $ /bin/sh your.sh &\n[1]    pid\n>> disown %1\n```\n\n> 看这个解释得更为详细，我对其理解较为肤浅：[Linux技巧：让后台在后台可靠运行的几种方法](http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/)。\n\n#### screen:\n\n上面两个命令都不是最好用的，最好用的应该是screen命令了。screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen用好了，感觉也不比tmux差多少。\n\n要同时跑多个screen，并想快速切换的换，可以添加几个screen的alias。\n\n```\nalias s=\"screen\"\nalias sr=\"screen -r\"     # 连接到某个模拟器\nalias sl=\"screen -ls\"     # 显示当前所有模拟器\nalias sl=\"screen -d\"     # 强制断开某个模拟器连接\nctrl+a :sessionname MyName   # 在screen模拟器中，修改模拟器名字\nctrl+a :kill    # 强制关闭模拟器（模拟器有时候会莫名其妙的没响应）\nctrl+a :encoding gbk     # 设置screen编码\nctrl+a d     # detached模拟器（就是临时退出）\n```\n\n> TIPS: screen终端ls文件没有颜色，是因为screen终端类型比较特殊，echo $TERM显示为screen或者screen.linux类型，解决方案是修改/etc/DIR_COLORS或者复制/etc/DIR_COLORS到~/.dir_colors，加入一句：&quot;TERM screen&quot;注销重登即可。\n\n#### jobs:\n\n列出当前终端运行的后台进程。\n\n#### ps:\n\n列出系统正在运行的程序。\n\n#### top:\n\n理解为win下面的任务管理器，需要注意的地方应该是load average了，Load Average表示了CPU的Load，它所包含的信息不是 CPU的使用率状况，而是在一段时间内CPU正在处理以及等待CPU处理的进程数之和的统计信息，也就是 CPU使用队列的长度的统计信息。load average: 0.06, 0.60, 0.48，三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。需要注意，load average如果超过一定的数的话，系统负载较高。\n\n`Load Average < CPU个数 * 核数 *0.7`\n\n我常用的命令也不多，如下：\n\n```\n>> $ top\n…\n…\n>> $ top -p pid [-p pid2 …]\n…    # 只查看pid这个进程\n```\n\n#### crontab:\n\n定时任务命令，这个东西有时候也折磨了好一段时间。内容格式说着也简单：\n\n```\n>> $ crontab -e    # 编辑定时任务列表\ncmd > /dev/null 2>&1     # 5个分别代表分、时、日、月、周。需要将输出写到某个文件或者/dev/null中，因为它的所有输出均会按邮件发到服务器上，日积月累也是个负担\n/2 * cmd > /dev/null 2>&1     # 代表每两分钟执行一次cmd\n>> $ crontab -l    # 显示定时任务列表\n>> $ crontab -r    # 删除定时任务列表（- -，不知道为什么要有这个选项）\n```\n\n> 偷懒，直接贴其他人的帖子吧：[Crontab 错误分析及不执行原因](http://www.cnblogs.com/cosiray/archive/2012/03/09/2387361.html)\n\n#### history:\n\n查看命令历史，以下是常用方法和有用的设置：\n\n```\n>> $ history | more\n>> $ history | grep ls\n>> $ vi ~/.bashrc\nHISTFILESIZE=2000     # history 记录长度\nHISTTIMEFORMAT='%F %T '     # 给记录添加时间戳\n```\n\n> TIPS1: 输入命令前，输入一个空格，该记录不添加进history中\r>\n> TIPS2: ctrl + r是找历史记录，查找过程中继续按ctrl+r是当前查找结果的上一条\n\n#### ssh:\n\n最后一个说ssh，好像也没啥说的。远程登录，远程拷贝\n\n```\n>> $ ssh -l user ip [-p port]\n>> $ ssh user@ip\n>> $ scp user@ip:/src/dir/or/file user@ip:/dest/dir/or/file\n```\n\n> FINAL TIPS: 拷贝的时候需要输入对方的用户密码，可以添加相关的密钥一劳永逸。但是添加密钥的方式有点微麻烦，所以可以用其他方式代替，那就是用expect命令。下面这段脚本可以实现无间断的远程push，可以自己改成远程pull。\n``` sh\n#!/usr/bin/expect\nset user [lindex $argv 0]\nset passwd [lindex $argv 1]\nset ip [lindex $argv 2]\nset src [lindex $argv 3]\nset des [lindex $argv 4]\nset timeout 60\nspawn /usr/local/bin/scp -r ${src} ${user}@${ip}:${des}\nexpect {\n    \"*assword:\" {\n     send \"${passwd}\\n\"\n     exp_continue\n    }\n    \"fcr_parse_raw\" {\n        close\n        exit -2\n    }\n    timeout {\n        close\n        exit -1\n    }\n    eof {\n        catch wait result\n        exit [lindex $result 3]\n    }\n    -re . {\n        exp_continue\n    }\n}\n```\n","source":"_posts/2013/01/linux-process-shell.md","raw":"title: Shell常用命令 进程操作篇\ntags:\n  - Linux\n  - Shell\nid: 656\ncategories:\n  - 技术分享\ndate: 2013-01-22 21:57:57\n---\n\n### 4.0 前言\n\n这篇算是尾篇了，命令较多也较繁杂，每个都是点到为止，基本也够用。有兴趣深入的朋友可自行谷歌学习。\n\n<!--more-->\n\n### 4.1 进程执行\n\n进程执行无外乎前台运行，但是前台运行需要用户等待程序运行完毕才可继续，所以就有了后台运行进程这一说。后台进程又分两种，当前终端后台进程，以及托管给OS的后台进程。这里就稍微介绍一下如何运行这些个进程吧。\n\n> BTW: 以上分类都是我瞎分的，仅供参考。\n\n#### 后台运行进程:\n\n在运行命令末尾加上`&`符号，该程序就会在后台运行，如果忘记敲`&`命令的话，也可以同过ctrl+z，暂停当前进程，再用bg(backgroud)命令即可让暂停任务变成后台运行。\n\n```\n>> $ /bin/sh your.sh &\n[1]    pid        # 输出进程在当前终端的序号以及pid\n>> $ /bin/sh your.sh       # ctrl + z\n[1]+  Stopped            /bin/sh your.sh\n>> $ bg %1   # %1 代表当前终端的第一个进程\n[1]+  /bin/sh your.sh &\n```\n\n> TIPS: The plus sign shows the most recently invoked job; the minus sign shows the next most recently invoked job.- <Learning the Korn Shell, 2nd Edition>\r>\n> WARNING: 当运行某个有大量输出的进程，如果直接让其进入后台运行是很不明智的，一般来说会将输出写入文件中。如： ` $ /bin/sh your.sh > ./your.log 2>&1 &`\n\n#### 前台运行进程：\n\n有了后台，就有相应的前台运行进程了。fg(foreground)就是了，但一般来说，很少需要将后台进程推到前台的需求，我一般是这样用的：\n\n```\n>> $ vim your_file      # ctrl + z\n[1]+   Stopped            vim your_file\n>> $ fg %1\n```\n\n#### 托管进程：\n\n上面的命令均是在当前终端运行，如果终端关闭，相应的进程也关闭了，如果想进程继续运行，则需要将进程托管给系统管理。主要命令是nohup。\n\n```\n>> $ nohup /bin/sh your.sh &\n[1]    pid\nnohup: appending output to ‘nohup.out'\n```\n\n实战的时候，更多的情况是这样的：突然有事要走开一下，又生怕跑了半天的程序因为终端挂了，想将其托管给系统。那么就可以用disown命令。\n\n```\n>> $ /bin/sh your.sh &\n[1]    pid\n>> disown %1\n```\n\n> 看这个解释得更为详细，我对其理解较为肤浅：[Linux技巧：让后台在后台可靠运行的几种方法](http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/)。\n\n#### screen:\n\n上面两个命令都不是最好用的，最好用的应该是screen命令了。screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen用好了，感觉也不比tmux差多少。\n\n要同时跑多个screen，并想快速切换的换，可以添加几个screen的alias。\n\n```\nalias s=\"screen\"\nalias sr=\"screen -r\"     # 连接到某个模拟器\nalias sl=\"screen -ls\"     # 显示当前所有模拟器\nalias sl=\"screen -d\"     # 强制断开某个模拟器连接\nctrl+a :sessionname MyName   # 在screen模拟器中，修改模拟器名字\nctrl+a :kill    # 强制关闭模拟器（模拟器有时候会莫名其妙的没响应）\nctrl+a :encoding gbk     # 设置screen编码\nctrl+a d     # detached模拟器（就是临时退出）\n```\n\n> TIPS: screen终端ls文件没有颜色，是因为screen终端类型比较特殊，echo $TERM显示为screen或者screen.linux类型，解决方案是修改/etc/DIR_COLORS或者复制/etc/DIR_COLORS到~/.dir_colors，加入一句：&quot;TERM screen&quot;注销重登即可。\n\n#### jobs:\n\n列出当前终端运行的后台进程。\n\n#### ps:\n\n列出系统正在运行的程序。\n\n#### top:\n\n理解为win下面的任务管理器，需要注意的地方应该是load average了，Load Average表示了CPU的Load，它所包含的信息不是 CPU的使用率状况，而是在一段时间内CPU正在处理以及等待CPU处理的进程数之和的统计信息，也就是 CPU使用队列的长度的统计信息。load average: 0.06, 0.60, 0.48，三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。需要注意，load average如果超过一定的数的话，系统负载较高。\n\n`Load Average < CPU个数 * 核数 *0.7`\n\n我常用的命令也不多，如下：\n\n```\n>> $ top\n…\n…\n>> $ top -p pid [-p pid2 …]\n…    # 只查看pid这个进程\n```\n\n#### crontab:\n\n定时任务命令，这个东西有时候也折磨了好一段时间。内容格式说着也简单：\n\n```\n>> $ crontab -e    # 编辑定时任务列表\ncmd > /dev/null 2>&1     # 5个分别代表分、时、日、月、周。需要将输出写到某个文件或者/dev/null中，因为它的所有输出均会按邮件发到服务器上，日积月累也是个负担\n/2 * cmd > /dev/null 2>&1     # 代表每两分钟执行一次cmd\n>> $ crontab -l    # 显示定时任务列表\n>> $ crontab -r    # 删除定时任务列表（- -，不知道为什么要有这个选项）\n```\n\n> 偷懒，直接贴其他人的帖子吧：[Crontab 错误分析及不执行原因](http://www.cnblogs.com/cosiray/archive/2012/03/09/2387361.html)\n\n#### history:\n\n查看命令历史，以下是常用方法和有用的设置：\n\n```\n>> $ history | more\n>> $ history | grep ls\n>> $ vi ~/.bashrc\nHISTFILESIZE=2000     # history 记录长度\nHISTTIMEFORMAT='%F %T '     # 给记录添加时间戳\n```\n\n> TIPS1: 输入命令前，输入一个空格，该记录不添加进history中\r>\n> TIPS2: ctrl + r是找历史记录，查找过程中继续按ctrl+r是当前查找结果的上一条\n\n#### ssh:\n\n最后一个说ssh，好像也没啥说的。远程登录，远程拷贝\n\n```\n>> $ ssh -l user ip [-p port]\n>> $ ssh user@ip\n>> $ scp user@ip:/src/dir/or/file user@ip:/dest/dir/or/file\n```\n\n> FINAL TIPS: 拷贝的时候需要输入对方的用户密码，可以添加相关的密钥一劳永逸。但是添加密钥的方式有点微麻烦，所以可以用其他方式代替，那就是用expect命令。下面这段脚本可以实现无间断的远程push，可以自己改成远程pull。\n``` sh\n#!/usr/bin/expect\nset user [lindex $argv 0]\nset passwd [lindex $argv 1]\nset ip [lindex $argv 2]\nset src [lindex $argv 3]\nset des [lindex $argv 4]\nset timeout 60\nspawn /usr/local/bin/scp -r ${src} ${user}@${ip}:${des}\nexpect {\n    \"*assword:\" {\n     send \"${passwd}\\n\"\n     exp_continue\n    }\n    \"fcr_parse_raw\" {\n        close\n        exit -2\n    }\n    timeout {\n        close\n        exit -1\n    }\n    eof {\n        catch wait result\n        exit [lindex $result 3]\n    }\n    -re . {\n        exp_continue\n    }\n}\n```\n","slug":"2013/01/linux-process-shell","published":1,"updated":"2016-01-02T06:05:29.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92q00443x8fc98tuxba"},{"title":"Shell常用命令 文件/目录操作篇","id":"637","date":"2013-01-18T12:38:24.000Z","_content":"\n### 2.1 基本操作\n\nls(list), cd(change directory), rm(remove), touch, mkdir(make directory), rmdir(remove directory), pwd(print working dir)\n\n> TIPS: cd - 可以返回上次所处目录。\r\r> WARNING: 慎用rm -rf \\*，想当年某同学就rm掉了工作目录，不过代码有备份。\n\n<!--more-->\n\n\n### 2.2 文件查找\n\n#### find\n\n最常用的当属find命令了。find的使用格式及我常用例子如下：\n\n```\n>> $ find -help\nUsage: find [path] expression # commented by wikie\n>> $ find . -name \".txt\"    # 查找当前目录下所有符合\".txt\"的文件\n>> $ find . -type f|d|l    # 查找当前目录下所有普通文件|目录|链接符号\n>> $ find . -size 1024(c)    # 查找当前目录大小为1024个块（字节）的文件\n>> $ find . -maxdepth 1 -name \".txt\"    # 设置最大查找深入，1表示为当前目录\n>> $ find . -a(m)time -1    # 查找当前目录下最后24小时访问（修改）的文件\n>> $ find . -name \".tmp\" -exec(-ok) rm {} \\;   # 删除当前目录的所有tmp文件，-ok表示执行前先询问用户\n```\n\n> TIPS1: 如不太喜欢用-exec，也可以用 find . | while read line; do do_sth to $line; done;\r> TIPS2: 最好的参考文档莫过于man find了\n\n#### locate\n\nlocate命令其实是`find -name`的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。不过需要root权限。\n\n```\n>> $ locate your_file\n```\n\n#### whereis\n\nwhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。\n\n```\n>> $ whereis whereis\nwhereis: /usr/bin/whereis /usr/share/man/man1/whereis.1.gz\n```\n\n#### which\n\nwhich命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n\n```\n>> $ which gcc\n/usr/bin/gcc\n```\n\n> 以上多数文字摘抄自阮一峰的文章 - 《[Linux的五个查找命令](http://www.ruanyifeng.com/blog/2009/10/5_ways_to_search_for_files_using_the_terminal.html)》\n\n### 2.3 磁盘操作\n\ndu(disk usage)查看目录大小，df(disk free)查看磁盘状况。均有-h参数，代表human-readable，即数据小大会显示单位。\n\n```\n>> $ du -sh .     # -s summay\n10G     .\n>> $ df -h\nFilesystem            Size   Used   Avail   Use%   Mounted on\n/dev/hda1             62G    21G    42G    34%     /\ntmpfs                   502M    0     502M    0%     /dev/shm\n>> $ df -ih          # -i inode usage\nFilesystem            Inodes   IUsed   IFree   IUse%   Mounted on\n/dev/hda1              63M      197K    62M    1%   /\ntmpfs                    126K       1       126K    1%   /dev/shm\n```\n\n>  df是针对整个文件系统的，它通过系统调用statfs从文件系统的超级块中获取整个文件系统的磁盘使用情况，它没有没法针对任意目录来统计，但统计磁盘大小速度更快。如果需要统计磁盘大小，可以用df代替du：`df -h | grep mounted_disk | awk '{print $4}'`\r\r### 2.4 权限管理\n\nchmod(change mode), chown(change owner)\n\n说实在的，这两个命令在非管理员的情况下一般用不到。用得多的是chmod +x file，给文件添加执行权限。其他的还有需求自行谷歌。\n\n### 2.5 文件压缩、裁剪\n\n压缩用的是tar命令，也有用zip的但是不多。tar的参数实在是太多了，列不过来，就说两个常用的压缩解压缩吧：\n\n```\n>> $ tar zcvf file.tar.gz file                   # z 以gzip的压缩方式压缩  c 压缩\n>> $ tar zxvf file.tar.gz [-C untar_dir]\n```\n\n> WARNING: 需要注意gzip压缩解压缩非常耗时且耗cpu\r> TIPS: 在远程拷贝（scp）文件的时候，如果文件非常大，需要考虑拷贝中断所带来的痛苦。所以适当的将文件裁剪成多个再scp，拷贝中断带来的痛苦系数会大大减小。\n\n```\n>> $ split –help\nUsage: split [OPTION] [INPUT [PREFIX]]\n>> $ split -b 1024b(k)(m) filename (splited_filename)   # 按大小裁剪\n>> $ split -l 1000 filename (splited_filename)   # 按行数裁剪\n>> $ cat splited_filename* > filename   # 合并被裁剪文件\n```\n","source":"_posts/2013/01/linux-file-directory-shell.md","raw":"title: Shell常用命令 文件/目录操作篇\ntags:\n  - Linux\n  - Shell\nid: 637\ncategories:\n  - 技术分享\ndate: 2013-01-18 20:38:24\n---\n\n### 2.1 基本操作\n\nls(list), cd(change directory), rm(remove), touch, mkdir(make directory), rmdir(remove directory), pwd(print working dir)\n\n> TIPS: cd - 可以返回上次所处目录。\r\r> WARNING: 慎用rm -rf \\*，想当年某同学就rm掉了工作目录，不过代码有备份。\n\n<!--more-->\n\n\n### 2.2 文件查找\n\n#### find\n\n最常用的当属find命令了。find的使用格式及我常用例子如下：\n\n```\n>> $ find -help\nUsage: find [path] expression # commented by wikie\n>> $ find . -name \".txt\"    # 查找当前目录下所有符合\".txt\"的文件\n>> $ find . -type f|d|l    # 查找当前目录下所有普通文件|目录|链接符号\n>> $ find . -size 1024(c)    # 查找当前目录大小为1024个块（字节）的文件\n>> $ find . -maxdepth 1 -name \".txt\"    # 设置最大查找深入，1表示为当前目录\n>> $ find . -a(m)time -1    # 查找当前目录下最后24小时访问（修改）的文件\n>> $ find . -name \".tmp\" -exec(-ok) rm {} \\;   # 删除当前目录的所有tmp文件，-ok表示执行前先询问用户\n```\n\n> TIPS1: 如不太喜欢用-exec，也可以用 find . | while read line; do do_sth to $line; done;\r> TIPS2: 最好的参考文档莫过于man find了\n\n#### locate\n\nlocate命令其实是`find -name`的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。不过需要root权限。\n\n```\n>> $ locate your_file\n```\n\n#### whereis\n\nwhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。\n\n```\n>> $ whereis whereis\nwhereis: /usr/bin/whereis /usr/share/man/man1/whereis.1.gz\n```\n\n#### which\n\nwhich命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n\n```\n>> $ which gcc\n/usr/bin/gcc\n```\n\n> 以上多数文字摘抄自阮一峰的文章 - 《[Linux的五个查找命令](http://www.ruanyifeng.com/blog/2009/10/5_ways_to_search_for_files_using_the_terminal.html)》\n\n### 2.3 磁盘操作\n\ndu(disk usage)查看目录大小，df(disk free)查看磁盘状况。均有-h参数，代表human-readable，即数据小大会显示单位。\n\n```\n>> $ du -sh .     # -s summay\n10G     .\n>> $ df -h\nFilesystem            Size   Used   Avail   Use%   Mounted on\n/dev/hda1             62G    21G    42G    34%     /\ntmpfs                   502M    0     502M    0%     /dev/shm\n>> $ df -ih          # -i inode usage\nFilesystem            Inodes   IUsed   IFree   IUse%   Mounted on\n/dev/hda1              63M      197K    62M    1%   /\ntmpfs                    126K       1       126K    1%   /dev/shm\n```\n\n>  df是针对整个文件系统的，它通过系统调用statfs从文件系统的超级块中获取整个文件系统的磁盘使用情况，它没有没法针对任意目录来统计，但统计磁盘大小速度更快。如果需要统计磁盘大小，可以用df代替du：`df -h | grep mounted_disk | awk '{print $4}'`\r\r### 2.4 权限管理\n\nchmod(change mode), chown(change owner)\n\n说实在的，这两个命令在非管理员的情况下一般用不到。用得多的是chmod +x file，给文件添加执行权限。其他的还有需求自行谷歌。\n\n### 2.5 文件压缩、裁剪\n\n压缩用的是tar命令，也有用zip的但是不多。tar的参数实在是太多了，列不过来，就说两个常用的压缩解压缩吧：\n\n```\n>> $ tar zcvf file.tar.gz file                   # z 以gzip的压缩方式压缩  c 压缩\n>> $ tar zxvf file.tar.gz [-C untar_dir]\n```\n\n> WARNING: 需要注意gzip压缩解压缩非常耗时且耗cpu\r> TIPS: 在远程拷贝（scp）文件的时候，如果文件非常大，需要考虑拷贝中断所带来的痛苦。所以适当的将文件裁剪成多个再scp，拷贝中断带来的痛苦系数会大大减小。\n\n```\n>> $ split –help\nUsage: split [OPTION] [INPUT [PREFIX]]\n>> $ split -b 1024b(k)(m) filename (splited_filename)   # 按大小裁剪\n>> $ split -l 1000 filename (splited_filename)   # 按行数裁剪\n>> $ cat splited_filename* > filename   # 合并被裁剪文件\n```\n","slug":"2013/01/linux-file-directory-shell","published":1,"updated":"2016-01-02T06:04:00.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92u00483x8fa5z2sw25"},{"title":"2013学习计划","id":"623","date":"2013-01-03T15:20:47.000Z","_content":"\n2012年也过去了，去年这个博客基本坚持了至少一个月一篇博文，但是12月学习的知识实在是太零散了（文本分类、自然语言处理、分布式系统、流处理系统、并发编程，甚至还看了一小会儿汇编），同时自己理解也过于肤浅，今年找个时间再整理出来吧。BTW：13年也是我一个发力之年啊，3月份会去一个新的地方发展，主要的应该关注在分布式系统和实时计算上，到时候的心得应该会比现在更多。\n\n<!--more-->\n\n好吧，列列我的学习清单吧：\n\n1）**分布式相关**：solr、hbase的深入理解与应用，hadoop就继续翻源码吧；\n\n2）**流处理相关**：主要是storm，虽然storm看了一些，也写了一些小topologies，但是没有实战过；\n\n3）**实时处理相关**：Dremel打头阵了，Google论文一出，业内一片欢声一片哀声，欢的是有此神功我等有救了，哀的是欲练神功必先自宫！现有的HDFS要上Dremel这类应用，不大刀阔斧的改可能难见成效；\n\n4）**计算机基础知识**：算法与数据结构深入、Linux内核了解、I/O机制深入、并发编程深入……这些知识都是上面的基础，也是IT人士的内功心法，我这半路科班人士，就得更加的深入学习了；\n\n5）**机器学习与数据挖掘**：文本分类、LDA、信息论。这快一年的实习，与这个有一定的接触，虽然主要兴趣不在于此，但是觉得这门学科博大精深，特别是看了东犇基于LDA的小实验结果，效果惊人，不继续了解了解有可能会遗憾终生啊。\n\n\n看了上面的学习计划，好像我挺大挺贪的，但是只要不半途而废的话，还是可以理解到某些方向的精髓滴。突然又想到了一条，最近朋友的mac air放我这了，体验非常非常好，所以再加上一条：\n\n6）**mac os深入了解**：不过这条前提是买mac本，T_T。\n","source":"_posts/2013/01/2013-learning-plan.md","raw":"title: 2013学习计划\ntags:\n  - 生活\nid: 623\ncategories:\n  - 生活分享\ndate: 2013-01-03 23:20:47\n---\n\n2012年也过去了，去年这个博客基本坚持了至少一个月一篇博文，但是12月学习的知识实在是太零散了（文本分类、自然语言处理、分布式系统、流处理系统、并发编程，甚至还看了一小会儿汇编），同时自己理解也过于肤浅，今年找个时间再整理出来吧。BTW：13年也是我一个发力之年啊，3月份会去一个新的地方发展，主要的应该关注在分布式系统和实时计算上，到时候的心得应该会比现在更多。\n\n<!--more-->\n\n好吧，列列我的学习清单吧：\n\n1）**分布式相关**：solr、hbase的深入理解与应用，hadoop就继续翻源码吧；\n\n2）**流处理相关**：主要是storm，虽然storm看了一些，也写了一些小topologies，但是没有实战过；\n\n3）**实时处理相关**：Dremel打头阵了，Google论文一出，业内一片欢声一片哀声，欢的是有此神功我等有救了，哀的是欲练神功必先自宫！现有的HDFS要上Dremel这类应用，不大刀阔斧的改可能难见成效；\n\n4）**计算机基础知识**：算法与数据结构深入、Linux内核了解、I/O机制深入、并发编程深入……这些知识都是上面的基础，也是IT人士的内功心法，我这半路科班人士，就得更加的深入学习了；\n\n5）**机器学习与数据挖掘**：文本分类、LDA、信息论。这快一年的实习，与这个有一定的接触，虽然主要兴趣不在于此，但是觉得这门学科博大精深，特别是看了东犇基于LDA的小实验结果，效果惊人，不继续了解了解有可能会遗憾终生啊。\n\n\n看了上面的学习计划，好像我挺大挺贪的，但是只要不半途而废的话，还是可以理解到某些方向的精髓滴。突然又想到了一条，最近朋友的mac air放我这了，体验非常非常好，所以再加上一条：\n\n6）**mac os深入了解**：不过这条前提是买mac本，T_T。\n","slug":"2013/01/2013-learning-plan","published":1,"updated":"2016-01-02T06:03:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92w004c3x8fh2ojdyxf"},{"title":"Zookeeper Ephemeral结点使用心得","id":"619","date":"2012-11-05T12:33:28.000Z","_content":"\n公司里面在拿Zookeeper做命名服务，通过使用ZK，前端只需要根据指定的ZK地址获得相应的资源或服务的后端服务器地址即可，而后端服务器需要做的仅仅是将自己的地址注册到ZK上作为一个Ephemeral结点即可。（虽然是挺方便后端扩容，但是我个人不太建议直接上ZK，否则开发成本会增加）\n<!--more-->  \n\n> Ephemeral结点在Apache Zookeeper中是一个临时结点，这些结点只要创建它的结点session不挂，它就一直存在，当session中止了，结点也就删除了。  \n\n### 问题\n\n在开发的时候遇到了一个奇怪的问题，当某个后端快速重启之后，该后端的结点信息过一段时间后会被删除，这样就导致了后端服务永远无法被前端访问到。\n\n### 原因\n\n查了资料后得知，如果在你的session中，ephemeral结点不是由你创建的，你的session就不会拥有该结点，所以当拥有该结点的session终止（expire）了，该结点也就销毁了。那么，如果不是你显式的删除该结点的话，就只能由ZK帮你终止它，在会话超时之后ZK就自动删除结点。如果在会话还未超时的过程中（一般是30s），你重启后端服务器的话，就会导致我所说的情况。\n\n解决方案：\n\nApache提供了几个patch，也有人提供了一些解决方案，均是显式的终止session。但是后端服务器挂了，显式终止一般是没用的。找到的这个方法是比较靠谱的，那就是在创建结点前，先删除之前的结点：\n\n```\ntry {\n   zk.delete(path)\n} catch {\n   e: NoNodeException => // do nothing\n}\nzk.create(path, data, CreateMode.EPHEMERAL)\n```\n\n> 参考资料：\n>\n> [A Gotcha When Using ZooKeeper Ephemeral Nodes](http://developers.blog.box.com/2012/04/10/a-gotcha-when-using-zookeeper-ephemeral-nodes/)\n>\n> [ephemerals handling after restart](http://zookeeper-user.578899.n2.nabble.com/ephemerals-handling-after-restart-td1084715.html)\n","source":"_posts/2012/11/zookeeper-ephemeral-nodes-experience.md","raw":"title: Zookeeper Ephemeral结点使用心得\ntags:\n  - ZooKeeper\nid: 619\ncategories:\n  - 技术分享\ndate: 2012-11-05 20:33:28\n---\n\n公司里面在拿Zookeeper做命名服务，通过使用ZK，前端只需要根据指定的ZK地址获得相应的资源或服务的后端服务器地址即可，而后端服务器需要做的仅仅是将自己的地址注册到ZK上作为一个Ephemeral结点即可。（虽然是挺方便后端扩容，但是我个人不太建议直接上ZK，否则开发成本会增加）\n<!--more-->  \n\n> Ephemeral结点在Apache Zookeeper中是一个临时结点，这些结点只要创建它的结点session不挂，它就一直存在，当session中止了，结点也就删除了。  \n\n### 问题\n\n在开发的时候遇到了一个奇怪的问题，当某个后端快速重启之后，该后端的结点信息过一段时间后会被删除，这样就导致了后端服务永远无法被前端访问到。\n\n### 原因\n\n查了资料后得知，如果在你的session中，ephemeral结点不是由你创建的，你的session就不会拥有该结点，所以当拥有该结点的session终止（expire）了，该结点也就销毁了。那么，如果不是你显式的删除该结点的话，就只能由ZK帮你终止它，在会话超时之后ZK就自动删除结点。如果在会话还未超时的过程中（一般是30s），你重启后端服务器的话，就会导致我所说的情况。\n\n解决方案：\n\nApache提供了几个patch，也有人提供了一些解决方案，均是显式的终止session。但是后端服务器挂了，显式终止一般是没用的。找到的这个方法是比较靠谱的，那就是在创建结点前，先删除之前的结点：\n\n```\ntry {\n   zk.delete(path)\n} catch {\n   e: NoNodeException => // do nothing\n}\nzk.create(path, data, CreateMode.EPHEMERAL)\n```\n\n> 参考资料：\n>\n> [A Gotcha When Using ZooKeeper Ephemeral Nodes](http://developers.blog.box.com/2012/04/10/a-gotcha-when-using-zookeeper-ephemeral-nodes/)\n>\n> [ephemerals handling after restart](http://zookeeper-user.578899.n2.nabble.com/ephemerals-handling-after-restart-td1084715.html)\n","slug":"2012/11/zookeeper-ephemeral-nodes-experience","published":1,"updated":"2016-01-02T05:46:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92x004g3x8f69chd0lm"},{"title":"Map/Reduce Task源码分析","id":"603","date":"2012-10-18T14:22:02.000Z","_content":"\n### 1、序言\n\n这篇文章从十一前开始写，陆陆续续看源码并理解其中的原理。主要了解了Map/Reduce的运行流程，并仔细分析了Map流程以及一些细节，但是没有分析仔细Reduce Task，因为和一个朋友@[lidonghua1990](http://weibo.com/getix2010)一起分析的，他分析ReduceTask，这篇文章的Reduce的注释部分也是由他添加。等到他分析完Reduce之后，再将链接填上。\n\n<!--more-->\n\n### 2、源码流程分析\n\n![clip_image0014](/images/2012/10/clip_image0014.jpg)\n\n```\n-----------------------------Start-----------------------------------\n【Map Phrase】\n\n// MapTask\n\n1. map.run();\n\n  |- map(getCurrentKey(), getCurrentValue(), context);\n\n// MapTask$NewOutputCollector\n\n2. context.write(key, value);\n\n  |- collector.collect(key, value, partioner.getPartition());\n\n// MapTask$MapOutputBuffer\n\n3. startSpill();\n\n  |- spillReady.signal(); // spillThread is waiting\n\n  |- spillThread.sortAndSpill();\n\n  |— sorter.sort();       // default: QuickSort.class\n\n  |— if (combiner != null) combiner.combine();\n\n  |— writer.close();     // flush data\n\n// MapTask$NewOutputCollector\n\n// MapTask$MapOutputBuffer\n\n4. output.close(context);\n\n  |- collector.flush();\n\n  |— SortAndSpill();    // output last mem data\n\n  |— MergeParts();\n\n  |—– Merge.merge();  // merge and sort\n\n  |—– combinerRunner.combine(kvIter, combineCollector);\n```\n\n\n![Image](/images/2012/10/Image.jpg)\n\n```\n----------------------Tmp Data(On disk)-------------------------------\n【Reduce Phrase】\n\n// LocalJobRunner$Job\n\n0. reduce.run(localConf, this);\n\n// ReduceTask\n\n1. reduceCopier.fetchOutputs(); // only if data is on HDFS\n\n  |- copier.start(); // mapred.reduce.parallel.copies MapOutputCopiers\n\n  |— copyOutput(loc); // loc is the location in buffer\n\n  |—– getMapOutput(); // from remote host to a ramfs/localFS file\n\n  |——- // setup connection, validates header\n\n  |——- boolean shuffleInMemory = ramManager.canFitInMemory(decompressedLength); // check if data fit in mem else use localFS\n\n  |——- shuffleInMemory(); / shuffleToDisk(); // return a MapOutput\n\n  |—– // add to list (if in mem) / rename to final name (if in localFS)\n\n  |- localFSMergerThread.start(); // ReduceTask$ReduceCopier$LocalFSMerger.run()\n\n  |— // wait if number of files < 2*ioSortFactor - 1\n\n  |— Merger.merge(sortSegments==true); // merge io.sort.factor files ino 1\n\n  |- inMemFSMergeThread.start(); // ReduceTask$ReduceCopier$InMemFSMergeThread.run()\n\n  |— ramManager.waitForDataToMerge();\n\n  |— doInMemMerge();\n\n  |—– createInMemorySegments(…);\n\n  |—– Merger.merge(sortSegments==false);\n\n  |— if (combinerRunner != null) combinerRunner.combine(rIter, combineCollector);\n\n  |- // schedule until get all required outputs (using exp-back-off for retries on failures)\n\n// multi-pass (factor segments/pass), using hadoop.util.PriorityQueue\n\n2. Merger.merge();\n\n  |- factor = getPassFactor(); // btw: first pass is special\n\n  |- // set segmentsToMerge (sorted) and put them into PriorityQueue\n\n  |- // merge into a temp file, add to MergeQueue.segments, and sort\n\n  |- // loop until number of segments < factor\n\n3. runReducer();\n```\n\n### 3、部分问题分析\n\n1）如何排序并输出的？\n\nsortAndSpill();\n\nmapper接收到map端的输出后，会将所有的输出数据写入一个缓存中，当缓存大小超过一定阈值的时候，就会锁住部分数据，将这些数据写入磁盘中。没被锁住的数据则可继续写入，不受写操作影响。阈值等于io.sort.mb(100MB) * io.sort.spill.percent(0.8)。\n\n缓存采用的circle buffer，看似简单，但是hadoop中还是会有点小技巧，详细的可以看caibinbupt的博客（[分析1](http://caibinbupt.iteye.com/blog/402849)，[分析2](http://caibinbupt.iteye.com/blog/402214)），里面比较详细。\n\n缓存一般是用byte数组存，因为这样可以严格控制缓存大小。当然，如果记录大小一致的话，可以开相应的对象数组。但是，map中的缓存kv数据大小不一致，这样要排序的话，就会有很多问题：\n\n如何快速定位其中的排序键；定位了快速键之后，由于记录大小不一，原地排序会带来大量的数据交换。\n\n为了避免这样的问题出现，mapreduce实现中提供了两个索引记录，第一个为kvindices（kvpair1[partion1, key1_start, value1_start], kvpair2[partition2, key2_start, value2_start]），这个索引指向缓存中记录的起始位置；第二个为kvoffsets，记录kvindices中kvpair的位置，只需要比较kvoffsets中所对应的partition值以及key值再交换kvoffsets中的值即可完成排序。\n\n![image](/images/2012/10/clip_image0054.jpg)\n\n![Image](/images/2012/10/Image1.jpg)\n\n2）combine什么时候执行的？\n\n* 在map端内存溢写到磁盘的时候会执行combine（可配置不执行，min.num.spills.for.combine默认为3，当spill数少于3的时候，就不会执行）；\n* 在map端合并磁盘溢写文件的时候会执行combine；\n* 在reduce端合并内存拉取文件的时候会执行combine（inMemFSMergeThread）。\n\n为什么在localFSMergerThread中不执行combine呢？因为这个时候执行的combine就是reduce过程了。\n\n3）segment和group是啥？\n\nsegment\n\n每个map端划分出来的partition所对应的数据块为一个segment。如下，partition0/1/2所对应spill.out的一段数据均为一个segment。\n\n即segment是map端merge spills，以及reduce端merge从map端copy过来的数据的逻辑单元。\n\n![Image](/images/2012/10/Image2.jpg)\n\n个人理解就是reduce端进入一个reduce()方法的数据称之为一个group。默认按key分组。一般来说，用户涉及到group也就是二次排序的时候需要用到，因为需要自定义分组。可以参见《Hadoop权威指南》第8章的辅助排序。\n\n4）如何合并文件？\n\nMap阶段的合并发生在spill完所有文件之后，而Reduce阶段则发生在copyPhrase结束之后，两者逻辑是一直的，所以hadoop将合并写成了通用组件，即Merger。在分析Merger的前，需要了解segment（Merger$Segment）的概念，可以参见前文。\n\n将合并过程简单化：即有一些已经排好序的文件（Segment），需要对其进行合并并排序。需要和解决方案都很明显，用多路归并排序。\n\nMerger类实现了一个merge方法，该方法生成了一个MergeQueue实例，并调用了该实例的merge方法。MergeQueue继承了PriorityQueue。归并排序的时候需要取多个文件的最小值，hadoop实现是采用的小根堆，比较方法是Merger中的lessThan(a,b)，它会读取segment中当前key，并使用用户自定义类的comparator进行比较。归并路数根据io.sort.factor(10)设置。\n\n### 5、我之前的的认识误区\n\n1）map输出记录格式是怎样的？\n\nmap的输出为：(key1, value1); (key1, value2); (key1, value3)，而不是：(key1, list(value1, value2, value3))，这个只是逻辑上的格式。\n\n为什么这样呢：\n\n猜测： 一个key对应的list过大的话，内存放不下；不如来一条记录，输出一条记录。所以如果设置了combiner的话，最后对数据的压缩是很可观的。\n\n2）是否可以将mr中的临时数据不写入磁盘？\n\n从源码的角度来说，是不可能的。可以考虑[Spark](http://www.spark-project.org/)以及[Storm](https://github.com/nathanmarz/storm)的实现。\n\n### 6、参考资料\n\n> [MapReduce: 详解Shuffle流程](http://langyu.iteye.com/blog/992916)\n>\n> [caibinbupt的博客](http://caibinbupt.iteye.com/blog/401374)\n>\n> 《hadoop权威指南》  \n>\n> 源码版本 0.20.203.0\n","source":"_posts/2012/10/mapreduce-task-src-analysis.md","raw":"title: Map/Reduce Task源码分析\ntags:\n  - Hadoop\n  - MapReduce\nid: 603\ncategories:\n  - 技术分享\ndate: 2012-10-18 22:22:02\n---\n\n### 1、序言\n\n这篇文章从十一前开始写，陆陆续续看源码并理解其中的原理。主要了解了Map/Reduce的运行流程，并仔细分析了Map流程以及一些细节，但是没有分析仔细Reduce Task，因为和一个朋友@[lidonghua1990](http://weibo.com/getix2010)一起分析的，他分析ReduceTask，这篇文章的Reduce的注释部分也是由他添加。等到他分析完Reduce之后，再将链接填上。\n\n<!--more-->\n\n### 2、源码流程分析\n\n![clip_image0014](/images/2012/10/clip_image0014.jpg)\n\n```\n-----------------------------Start-----------------------------------\n【Map Phrase】\n\n// MapTask\n\n1. map.run();\n\n  |- map(getCurrentKey(), getCurrentValue(), context);\n\n// MapTask$NewOutputCollector\n\n2. context.write(key, value);\n\n  |- collector.collect(key, value, partioner.getPartition());\n\n// MapTask$MapOutputBuffer\n\n3. startSpill();\n\n  |- spillReady.signal(); // spillThread is waiting\n\n  |- spillThread.sortAndSpill();\n\n  |— sorter.sort();       // default: QuickSort.class\n\n  |— if (combiner != null) combiner.combine();\n\n  |— writer.close();     // flush data\n\n// MapTask$NewOutputCollector\n\n// MapTask$MapOutputBuffer\n\n4. output.close(context);\n\n  |- collector.flush();\n\n  |— SortAndSpill();    // output last mem data\n\n  |— MergeParts();\n\n  |—– Merge.merge();  // merge and sort\n\n  |—– combinerRunner.combine(kvIter, combineCollector);\n```\n\n\n![Image](/images/2012/10/Image.jpg)\n\n```\n----------------------Tmp Data(On disk)-------------------------------\n【Reduce Phrase】\n\n// LocalJobRunner$Job\n\n0. reduce.run(localConf, this);\n\n// ReduceTask\n\n1. reduceCopier.fetchOutputs(); // only if data is on HDFS\n\n  |- copier.start(); // mapred.reduce.parallel.copies MapOutputCopiers\n\n  |— copyOutput(loc); // loc is the location in buffer\n\n  |—– getMapOutput(); // from remote host to a ramfs/localFS file\n\n  |——- // setup connection, validates header\n\n  |——- boolean shuffleInMemory = ramManager.canFitInMemory(decompressedLength); // check if data fit in mem else use localFS\n\n  |——- shuffleInMemory(); / shuffleToDisk(); // return a MapOutput\n\n  |—– // add to list (if in mem) / rename to final name (if in localFS)\n\n  |- localFSMergerThread.start(); // ReduceTask$ReduceCopier$LocalFSMerger.run()\n\n  |— // wait if number of files < 2*ioSortFactor - 1\n\n  |— Merger.merge(sortSegments==true); // merge io.sort.factor files ino 1\n\n  |- inMemFSMergeThread.start(); // ReduceTask$ReduceCopier$InMemFSMergeThread.run()\n\n  |— ramManager.waitForDataToMerge();\n\n  |— doInMemMerge();\n\n  |—– createInMemorySegments(…);\n\n  |—– Merger.merge(sortSegments==false);\n\n  |— if (combinerRunner != null) combinerRunner.combine(rIter, combineCollector);\n\n  |- // schedule until get all required outputs (using exp-back-off for retries on failures)\n\n// multi-pass (factor segments/pass), using hadoop.util.PriorityQueue\n\n2. Merger.merge();\n\n  |- factor = getPassFactor(); // btw: first pass is special\n\n  |- // set segmentsToMerge (sorted) and put them into PriorityQueue\n\n  |- // merge into a temp file, add to MergeQueue.segments, and sort\n\n  |- // loop until number of segments < factor\n\n3. runReducer();\n```\n\n### 3、部分问题分析\n\n1）如何排序并输出的？\n\nsortAndSpill();\n\nmapper接收到map端的输出后，会将所有的输出数据写入一个缓存中，当缓存大小超过一定阈值的时候，就会锁住部分数据，将这些数据写入磁盘中。没被锁住的数据则可继续写入，不受写操作影响。阈值等于io.sort.mb(100MB) * io.sort.spill.percent(0.8)。\n\n缓存采用的circle buffer，看似简单，但是hadoop中还是会有点小技巧，详细的可以看caibinbupt的博客（[分析1](http://caibinbupt.iteye.com/blog/402849)，[分析2](http://caibinbupt.iteye.com/blog/402214)），里面比较详细。\n\n缓存一般是用byte数组存，因为这样可以严格控制缓存大小。当然，如果记录大小一致的话，可以开相应的对象数组。但是，map中的缓存kv数据大小不一致，这样要排序的话，就会有很多问题：\n\n如何快速定位其中的排序键；定位了快速键之后，由于记录大小不一，原地排序会带来大量的数据交换。\n\n为了避免这样的问题出现，mapreduce实现中提供了两个索引记录，第一个为kvindices（kvpair1[partion1, key1_start, value1_start], kvpair2[partition2, key2_start, value2_start]），这个索引指向缓存中记录的起始位置；第二个为kvoffsets，记录kvindices中kvpair的位置，只需要比较kvoffsets中所对应的partition值以及key值再交换kvoffsets中的值即可完成排序。\n\n![image](/images/2012/10/clip_image0054.jpg)\n\n![Image](/images/2012/10/Image1.jpg)\n\n2）combine什么时候执行的？\n\n* 在map端内存溢写到磁盘的时候会执行combine（可配置不执行，min.num.spills.for.combine默认为3，当spill数少于3的时候，就不会执行）；\n* 在map端合并磁盘溢写文件的时候会执行combine；\n* 在reduce端合并内存拉取文件的时候会执行combine（inMemFSMergeThread）。\n\n为什么在localFSMergerThread中不执行combine呢？因为这个时候执行的combine就是reduce过程了。\n\n3）segment和group是啥？\n\nsegment\n\n每个map端划分出来的partition所对应的数据块为一个segment。如下，partition0/1/2所对应spill.out的一段数据均为一个segment。\n\n即segment是map端merge spills，以及reduce端merge从map端copy过来的数据的逻辑单元。\n\n![Image](/images/2012/10/Image2.jpg)\n\n个人理解就是reduce端进入一个reduce()方法的数据称之为一个group。默认按key分组。一般来说，用户涉及到group也就是二次排序的时候需要用到，因为需要自定义分组。可以参见《Hadoop权威指南》第8章的辅助排序。\n\n4）如何合并文件？\n\nMap阶段的合并发生在spill完所有文件之后，而Reduce阶段则发生在copyPhrase结束之后，两者逻辑是一直的，所以hadoop将合并写成了通用组件，即Merger。在分析Merger的前，需要了解segment（Merger$Segment）的概念，可以参见前文。\n\n将合并过程简单化：即有一些已经排好序的文件（Segment），需要对其进行合并并排序。需要和解决方案都很明显，用多路归并排序。\n\nMerger类实现了一个merge方法，该方法生成了一个MergeQueue实例，并调用了该实例的merge方法。MergeQueue继承了PriorityQueue。归并排序的时候需要取多个文件的最小值，hadoop实现是采用的小根堆，比较方法是Merger中的lessThan(a,b)，它会读取segment中当前key，并使用用户自定义类的comparator进行比较。归并路数根据io.sort.factor(10)设置。\n\n### 5、我之前的的认识误区\n\n1）map输出记录格式是怎样的？\n\nmap的输出为：(key1, value1); (key1, value2); (key1, value3)，而不是：(key1, list(value1, value2, value3))，这个只是逻辑上的格式。\n\n为什么这样呢：\n\n猜测： 一个key对应的list过大的话，内存放不下；不如来一条记录，输出一条记录。所以如果设置了combiner的话，最后对数据的压缩是很可观的。\n\n2）是否可以将mr中的临时数据不写入磁盘？\n\n从源码的角度来说，是不可能的。可以考虑[Spark](http://www.spark-project.org/)以及[Storm](https://github.com/nathanmarz/storm)的实现。\n\n### 6、参考资料\n\n> [MapReduce: 详解Shuffle流程](http://langyu.iteye.com/blog/992916)\n>\n> [caibinbupt的博客](http://caibinbupt.iteye.com/blog/401374)\n>\n> 《hadoop权威指南》  \n>\n> 源码版本 0.20.203.0\n","slug":"2012/10/mapreduce-task-src-analysis","published":1,"updated":"2015-12-31T12:24:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba92y004k3x8ff38avcou"},{"title":"MapReduce优化（二） —— 善用Writable","id":"588","date":"2012-09-27T13:44:55.000Z","_content":"\n### 1、简述\n\n上文主要是数据压缩的角度来分析了MapReduce压缩临时数据的优化，参见：[MapReduce优化（一）](http://www.hongweiyi.com/2012/02/mapred-optimize/)。而这篇会更多的从代码层面说MR任务优化。\n\nMapReduce大多数任务都是做日志分析，而一般的日志分析也就是高级点的WordCount程序：读入一段文本 -> 获取需要的信息 -> 统计输出。\n\n<!--more-->  \n\n我这里的任务会从SequenceFile中读取文档（每个文档4M），每条文档里面有许多行记录，每个记录有一个词和词频。任务需要统计记录中所有词出现的绝对频率以及文档频率。格式如下：   \n```\nword1 freq1\\n\nword2 freq2\\n\nword3 freq3\\n\n```\n\n### 2、善用Writable\n\n程序逻辑应该是这样的：\n\n1. 根据换行符分隔文档；\n2. 读取每一行数据；\n3. 并输出绝对频率(word, freq)与文档频率(word, ONE)两条记录。\n\n简单明了的程序处理方式应该是这样的：\n\n```\nString str = value.toString();\nString[] ln = str.split(\"\\n\");\nfor(String l : ln) {\n  String[] words = ln.split(\" \");\n  context.write(new Text(words[0]), new LongWritable(1));\n  context.write(new Text(words[0]), new LongWritable(Long.parseLong(words[1]));\n}\n```\n\n上面代码可优化的地方有三：\n\n1. 在1行代码中，value的toString方法会对数据进行decode，decode效率很慢。而且该方法会分配内存空间；\n\n2. 在2行代码中，和`split(\"\\n\")`效率低下；\n\n3. 5和6行中，每次都会“新”创建“两对”Text，LongWritable对象，GC频繁。\n\n在上面的问题中，问题1和问题2可以一起解决，避免decode和分配内存就需要直接处理byte数组重写Text这个Writable类。只需要继承Text类并添加nextLine()方法即可。添加nextLine()方法是为了逻辑清晰，如果为了更高的效率的话，可以添加nextWord()与nextFreq()方法。\n\n问题3比较常见，在很多资料以及Hadoop自带的Example里面可以看到，输出的键值对均是复用的。用一个全局的KEY和VALUE，直接将新数据set进KEY、VALUE中即可，无需每次新创建相应对象。上面还有一个小地方可以优化的就是，将LongWritable改为IntWritable，减少数据输出。\n\n上面的解决方案似乎还不错了，但是map输出的临时数据依然很大。回查代码，发现context.write()数据太多了，最后的解决方案是将文档频率和绝对频率合并起来，简单点的格式就是：d_freq#freq。这样临时数据一下就减少了一半。\n\n以下是修改后的代码流程：\n\n```\nwhile (value.hasNext()) {\n   KEY.set(value.nextWord());\n   VALUE.set(\"1#\" + value.nextFreq());\n   context.write(KEY, VALUE);\n}\n```\n\n### 3、优化结果\n\n通过上面的优化，处理300G的数据，单个map task平均时间从3'30''降到45''，FILE_BYTE_WRITEN从1000G降到了450G，任务总时间从4小时降到了30分钟。\n\n> 20台节点。\n\n### 4、后记\n\n后来我在这个任务里面使用了Gzip压缩，压缩率约为44%。但是对效率的影响太大了，单个map task平均时间从45''升到1'20''，打了一半的折扣，着实让人难以接受。由于对集群没有完全的管理权限，所以无法在这个任务上面尝试Lzo压缩编码，有机会在尝试吧。\n","source":"_posts/2012/09/mapred-optimize-writable.md","raw":"title: MapReduce优化（二） —— 善用Writable\ntags:\n  - Hadoop\n  - MapReduce\nid: 588\ncategories:\n  - 技术分享\ndate: 2012-09-27 21:44:55\n---\n\n### 1、简述\n\n上文主要是数据压缩的角度来分析了MapReduce压缩临时数据的优化，参见：[MapReduce优化（一）](http://www.hongweiyi.com/2012/02/mapred-optimize/)。而这篇会更多的从代码层面说MR任务优化。\n\nMapReduce大多数任务都是做日志分析，而一般的日志分析也就是高级点的WordCount程序：读入一段文本 -> 获取需要的信息 -> 统计输出。\n\n<!--more-->  \n\n我这里的任务会从SequenceFile中读取文档（每个文档4M），每条文档里面有许多行记录，每个记录有一个词和词频。任务需要统计记录中所有词出现的绝对频率以及文档频率。格式如下：   \n```\nword1 freq1\\n\nword2 freq2\\n\nword3 freq3\\n\n```\n\n### 2、善用Writable\n\n程序逻辑应该是这样的：\n\n1. 根据换行符分隔文档；\n2. 读取每一行数据；\n3. 并输出绝对频率(word, freq)与文档频率(word, ONE)两条记录。\n\n简单明了的程序处理方式应该是这样的：\n\n```\nString str = value.toString();\nString[] ln = str.split(\"\\n\");\nfor(String l : ln) {\n  String[] words = ln.split(\" \");\n  context.write(new Text(words[0]), new LongWritable(1));\n  context.write(new Text(words[0]), new LongWritable(Long.parseLong(words[1]));\n}\n```\n\n上面代码可优化的地方有三：\n\n1. 在1行代码中，value的toString方法会对数据进行decode，decode效率很慢。而且该方法会分配内存空间；\n\n2. 在2行代码中，和`split(\"\\n\")`效率低下；\n\n3. 5和6行中，每次都会“新”创建“两对”Text，LongWritable对象，GC频繁。\n\n在上面的问题中，问题1和问题2可以一起解决，避免decode和分配内存就需要直接处理byte数组重写Text这个Writable类。只需要继承Text类并添加nextLine()方法即可。添加nextLine()方法是为了逻辑清晰，如果为了更高的效率的话，可以添加nextWord()与nextFreq()方法。\n\n问题3比较常见，在很多资料以及Hadoop自带的Example里面可以看到，输出的键值对均是复用的。用一个全局的KEY和VALUE，直接将新数据set进KEY、VALUE中即可，无需每次新创建相应对象。上面还有一个小地方可以优化的就是，将LongWritable改为IntWritable，减少数据输出。\n\n上面的解决方案似乎还不错了，但是map输出的临时数据依然很大。回查代码，发现context.write()数据太多了，最后的解决方案是将文档频率和绝对频率合并起来，简单点的格式就是：d_freq#freq。这样临时数据一下就减少了一半。\n\n以下是修改后的代码流程：\n\n```\nwhile (value.hasNext()) {\n   KEY.set(value.nextWord());\n   VALUE.set(\"1#\" + value.nextFreq());\n   context.write(KEY, VALUE);\n}\n```\n\n### 3、优化结果\n\n通过上面的优化，处理300G的数据，单个map task平均时间从3'30''降到45''，FILE_BYTE_WRITEN从1000G降到了450G，任务总时间从4小时降到了30分钟。\n\n> 20台节点。\n\n### 4、后记\n\n后来我在这个任务里面使用了Gzip压缩，压缩率约为44%。但是对效率的影响太大了，单个map task平均时间从45''升到1'20''，打了一半的折扣，着实让人难以接受。由于对集群没有完全的管理权限，所以无法在这个任务上面尝试Lzo压缩编码，有机会在尝试吧。\n","slug":"2012/09/mapred-optimize-writable","published":1,"updated":"2015-12-31T12:19:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba930004q3x8fv69ihm4a"},{"title":"发现Hadoop小bug一枚","id":"584","date":"2012-09-18T13:52:55.000Z","_content":"\n最近在做一个任务，合并大小文本文件，每个不到5M，合并之后再进行统计分析。但是统计分析结果不对，本来只有60w个文件，但是最后的结果竟然到了200w了，断断续续debug了几天，从合并、map过程、combine过程、reduce过程，都一一过了一遍。最后发现在map过程中的数据就有重复，而且重复的形式很奇怪。\n\n<!--more-->  \n\n举例来说，SequenceFile中两个记录一个4M一个为1M，map第一个4M的记录没出现问题，但是map第二个1M的记录出问题了，记录大小为1M，但是getBytes()之后会获得4M的数据，前面1M是正常数据，而后面3M是非法的前一个记录的后3M数据。\n\n开始以为是jvm没有将内存给销毁，但是觉着不对劲，就直接跟进代码瞅了瞅几眼。原来在map过程中，每次获得的key和value均会复用，而不会销毁，而Text的数据均存在了一个bytes数组，Text对象不销毁，bytes数组也不会销毁。可以参见：LineRecordReader or SequenceFileRecordReader的nextKeyValue()方法，该方法由Mapper的run方法间接调用。\n\n继续刚才的那个问题，我在读取Text中的数据时，会调用text.getBytes()方法，逻辑上它应该是要返回length长度的bytes给开发者，但是它却将整个bytes数组返回了。\n\n心想着可以提交一个patch，但后来翻了一下新版本的源码，发现hadoop已经提供了一种解决方案——copyBytes()。具体实现见下图：\n\n![Image](/images/2012/09/Image.png)\n\n* Hadoop会在不变动原有逻辑的基础上进行修改，这样的话可以最大限度的减少对用户的影响，并且可以往下兼容。值得学习啊，给我的话，就直接修改getBytes()方法了。\n\n* 我是基于0.20.203.0做的实验，这个问题在0.22之后的版本均提供了解决方案。\n","source":"_posts/2012/09/hadoop-bug-in-text.md","raw":"title: 发现Hadoop小bug一枚\ntags:\n  - Hadoop\nid: 584\ncategories:\n  - 技术分享\ndate: 2012-09-18 21:52:55\n---\n\n最近在做一个任务，合并大小文本文件，每个不到5M，合并之后再进行统计分析。但是统计分析结果不对，本来只有60w个文件，但是最后的结果竟然到了200w了，断断续续debug了几天，从合并、map过程、combine过程、reduce过程，都一一过了一遍。最后发现在map过程中的数据就有重复，而且重复的形式很奇怪。\n\n<!--more-->  \n\n举例来说，SequenceFile中两个记录一个4M一个为1M，map第一个4M的记录没出现问题，但是map第二个1M的记录出问题了，记录大小为1M，但是getBytes()之后会获得4M的数据，前面1M是正常数据，而后面3M是非法的前一个记录的后3M数据。\n\n开始以为是jvm没有将内存给销毁，但是觉着不对劲，就直接跟进代码瞅了瞅几眼。原来在map过程中，每次获得的key和value均会复用，而不会销毁，而Text的数据均存在了一个bytes数组，Text对象不销毁，bytes数组也不会销毁。可以参见：LineRecordReader or SequenceFileRecordReader的nextKeyValue()方法，该方法由Mapper的run方法间接调用。\n\n继续刚才的那个问题，我在读取Text中的数据时，会调用text.getBytes()方法，逻辑上它应该是要返回length长度的bytes给开发者，但是它却将整个bytes数组返回了。\n\n心想着可以提交一个patch，但后来翻了一下新版本的源码，发现hadoop已经提供了一种解决方案——copyBytes()。具体实现见下图：\n\n![Image](/images/2012/09/Image.png)\n\n* Hadoop会在不变动原有逻辑的基础上进行修改，这样的话可以最大限度的减少对用户的影响，并且可以往下兼容。值得学习啊，给我的话，就直接修改getBytes()方法了。\n\n* 我是基于0.20.203.0做的实验，这个问题在0.22之后的版本均提供了解决方案。\n","slug":"2012/09/hadoop-bug-in-text","published":1,"updated":"2015-12-31T12:16:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba931004u3x8fianviynf"},{"title":"Apache Hadoop YARN - 背景及概述","id":"578","date":"2012-09-06T14:41:59.000Z","_content":"\n虽然yahoo!关于YARN作为下一代（Next-gen）MapReduce框架的文章（[点这里](http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/)）去年就看过了，但是那个看到是“下一代”，竟然以为只是一个设想，没想到早就发布了版本，导致对于Hadoop的认识还停留在0.20×版本上，真是罪过罪过。由于最近比较忙，闲暇时间扫了扫国内外博客，发现0.23、1.×，以及最近发布的2.×，hadoop的变化非常之大。比如说HDFS Federation（联邦）支持多NameNode并存，也有HA的BackupNode，想多了解的可以看[这里](http://ai-longyu.iteye.com/blog/1566619)以及[官方文档](http://hadoop.apache.org/common/docs/r0.23.0/hadoop-yarn/hadoop-yarn-site/Federation.html)。最大的莫过于计算框架了，MapReduce进入了2.0时代，MR2.0或者叫YARN（其实YARN和MapReduce没什么关系了），这篇博客就简要的说说Apache Hadoop MapReduce的前世今生吧。主要是翻译了这篇博客：[地址](http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/)，也加上了自己的一些见解，后续再继续添加对YARN的认识。\n\n<!--more-->\n\n### Apache Hadoop MapReduce\n\nApache Hadoop MapReduce是一个Google MapReduce编程模型的开源版本，由Apache基金会维护。现在，已经有人花了超过6年的时间在Hadoop上。但是，基本上MapReduce基本上可以分为三个主要部分：\n\n1. MapReduce API：提供给终端用户（程序猿）开发MR程序的接口；\n2. MapReduce 框架：MR各个过程（phrase）的实现，如：map phrase、reduce phrase、sort/shuffle/merge phrase等；\n3. MapReduce 系统：运行用户MR程序的后端基础设施，用以管理资源、调度任务等。  \n\n将MR分成以上三个概念非常的重要，特别是对终端用户，他们可以完全专注于MR逻辑代码的编写，只需要通过API既可，由MR系统来解决资源管理、容错、调度的问题，而不需要用户考虑后端框架和系统的细节。\n\n现在工业界大部分还是用的0.23之前的版本（至少我待的公司还是0.20.2），老版本的MapReduce系统是简易的Master-Slaves结构，具体名字叫JobTracker-TaskTracker。\n\nJobTracker负责资源的管理（结点资源、计算资源等）以及任务生命周期管理（任务调度、进度查看、容错等）。而TaskTracker职责非常简单，开启/销毁任务，向JobTracker汇报任务状态。\n\n旧版的架构其实挺清晰的，不过也有很多不足的地方，业界一直嚷着要给MR一次大整修（Overhaul），JobTracker的可靠性是一直被诟病的一点（虽然我没见它挂过，但是风险一直存在着），但是除了JobTracker的单点问题，其它的问题也需要一一列出来。\n\n#### 不支持其它编程模型\n\nMapReduce对大多数应用（尤其是大数据统计分析）来说，都非常合适。但是有的时候，可能现实生活也有其它的编程模型，如图算法([Google Pregel](http://www.csdn.net/article/2012-08-20/2808870)/[Apache Giapah](http://giraph.apache.org/))或者是迭代式模型([MPI](http://en.wikipedia.org/wiki/Message_Passing_Interface))。当企业的所有数据在放在了HDFS上，有多种处理数据的方式就很重要了。\n\n而且，MR本质上是面向批处理的，并不支持实时或接近实时的处理请求，但是业界也希望Hadoop能支持实时计算。（我也一直希望可以支持实时计算，但是有时候觉得有点贪心，专注做一项不就好了么？但是好像人的贪欲是无穷的）\n\n有了以上的需求，为了降低了管理者使用成本，减少数据在HDFS和其它存储设备的迁移，Hadoop开发组织重新投入了Hadoop设计。\n\n#### 低可扩展性\n\n摩尔定律一直在生效，也让商用服务器的性能一直提高，以下就是一台商用服务器在不同时间的配置：\n\n* 2009 - 8 cores, 16GB of RAM, 4*1TB disk\n* 2012 - 16+ cores, 48-96GB of RAM, 12*2TB or 12*3TB of disk\n\n按照上面的配置，大约2-3年，服务器的配置就可以翻翻。而现在的Hadoop集群就只能支持10,000个节点和200,000个核。Hadoop软件需要赶上硬件的速度是非常重要的。顺带说句，我们公司的计算型服务器就是16cores 64GB of RAM。\n\n#### 服务器的低利用率\n\n在现在的系统中，JobTracker将管理集群视为很多的Map/Reduce槽（slot），然而在MR用运行的时候，大多数时候都是reduce槽在等待map槽完成（map 100% reduce 0%）。如果能优化这个的话，服务器就可以得到最大的利用。\n\n#### 使用的灵活性\n\n在现实生产环境中，Hadoop常常被部署成一个共享的、多用户的系统。这样就会导致一种情况，完全Hadoop软件可能会影响到整个部门。用户希望能够控制hadoop软件栈升级，因此，允许多版本的MapReduce框架并存对Hadoop来说就是很重要的了。\n\n### Apache Hadoop YARN\n\nYARN的基本思想是将JobTracker的两个主要职责给解耦：资源管理和任务管理（监控/调度），YARN将其分成了两个部分：全局的ResourceManager(RM)和给每个应用分配的ApplicationMaster(AM)。ResourceManager和它每个节点的slave——NodeManager(NM)，形成了一个全新的、用以管理应用的分布式系统。\n\nRM是系统资源的终极管理者，而AM则是一个特定应用框架的实体（每次提交任务的时候，需要编写相应的应用框架，现在只支持MapReduce），需要与RM索要应用资源，和NM一起执行和监控任务。\n\nRM中有调度器，而调度器内嵌有策略可插拔的插件，主要负责将集群中得资源分配给多个队列和应用。当前MapReduce的调度器，如Capacity Scheduler和Fair Scheduler，均可作为该插件。但是调度器的职责仅限于调度任务，并不保证任务的容错性。\n\nNodeManager有点类似于TaskTracker，它负责启动应用程序Container（类似于JVM），并监控container的资源（CPU、内存、磁盘、网络等），并将信息上报给ResouceManager。需要注意的是，调度器就是根据应用程序的Container进行调度的。\n\nApplicationMaster负责向调度器请求合适的container，并监控container的状态以及任务进程。\n\n下图是YARN的架构图：\n\n![YARNArch](/images/2012/09/YARNArch.png)\n\n新YARN系统比较重要的一条就是复用了原有的MapReduce框架，而并不需要大的改动，这对现有的MR应用以及用户来说，是非常重要的，具体是怎么复用的，以后再细说。\n\n接下来，Hadoop开发者会深入架构细节，继续提高系统的可扩展性，并让其支持更多的数据处理框架（graph, MPI）并提高集群可用性。\n\n以Hortonworks' Arun Murthy（YARN开发者）的一段话做结尾吧：\n\n> “People are not going to be comfortable buying a $5 million Hadoop cluster just to do MapReduce and a $2 million cluster to do something else. If you can allow them to run both apps in the same cluster, its not only easier for you in terms of a CapEx perspective … it’s also easier from an operational perspective because you don’t have to have two separate sets of people managing your clusters or two sets of tools for managing your clusters.”\n","source":"_posts/2012/09/apache-hadoop-yarn-background-and-an-overview.md","raw":"title: Apache Hadoop YARN - 背景及概述\ntags:\n  - Hadoop\n  - MapReduce\n  - YARN\nid: 578\ncategories:\n  - 技术分享\ndate: 2012-09-06 22:41:59\n---\n\n虽然yahoo!关于YARN作为下一代（Next-gen）MapReduce框架的文章（[点这里](http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/)）去年就看过了，但是那个看到是“下一代”，竟然以为只是一个设想，没想到早就发布了版本，导致对于Hadoop的认识还停留在0.20×版本上，真是罪过罪过。由于最近比较忙，闲暇时间扫了扫国内外博客，发现0.23、1.×，以及最近发布的2.×，hadoop的变化非常之大。比如说HDFS Federation（联邦）支持多NameNode并存，也有HA的BackupNode，想多了解的可以看[这里](http://ai-longyu.iteye.com/blog/1566619)以及[官方文档](http://hadoop.apache.org/common/docs/r0.23.0/hadoop-yarn/hadoop-yarn-site/Federation.html)。最大的莫过于计算框架了，MapReduce进入了2.0时代，MR2.0或者叫YARN（其实YARN和MapReduce没什么关系了），这篇博客就简要的说说Apache Hadoop MapReduce的前世今生吧。主要是翻译了这篇博客：[地址](http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/)，也加上了自己的一些见解，后续再继续添加对YARN的认识。\n\n<!--more-->\n\n### Apache Hadoop MapReduce\n\nApache Hadoop MapReduce是一个Google MapReduce编程模型的开源版本，由Apache基金会维护。现在，已经有人花了超过6年的时间在Hadoop上。但是，基本上MapReduce基本上可以分为三个主要部分：\n\n1. MapReduce API：提供给终端用户（程序猿）开发MR程序的接口；\n2. MapReduce 框架：MR各个过程（phrase）的实现，如：map phrase、reduce phrase、sort/shuffle/merge phrase等；\n3. MapReduce 系统：运行用户MR程序的后端基础设施，用以管理资源、调度任务等。  \n\n将MR分成以上三个概念非常的重要，特别是对终端用户，他们可以完全专注于MR逻辑代码的编写，只需要通过API既可，由MR系统来解决资源管理、容错、调度的问题，而不需要用户考虑后端框架和系统的细节。\n\n现在工业界大部分还是用的0.23之前的版本（至少我待的公司还是0.20.2），老版本的MapReduce系统是简易的Master-Slaves结构，具体名字叫JobTracker-TaskTracker。\n\nJobTracker负责资源的管理（结点资源、计算资源等）以及任务生命周期管理（任务调度、进度查看、容错等）。而TaskTracker职责非常简单，开启/销毁任务，向JobTracker汇报任务状态。\n\n旧版的架构其实挺清晰的，不过也有很多不足的地方，业界一直嚷着要给MR一次大整修（Overhaul），JobTracker的可靠性是一直被诟病的一点（虽然我没见它挂过，但是风险一直存在着），但是除了JobTracker的单点问题，其它的问题也需要一一列出来。\n\n#### 不支持其它编程模型\n\nMapReduce对大多数应用（尤其是大数据统计分析）来说，都非常合适。但是有的时候，可能现实生活也有其它的编程模型，如图算法([Google Pregel](http://www.csdn.net/article/2012-08-20/2808870)/[Apache Giapah](http://giraph.apache.org/))或者是迭代式模型([MPI](http://en.wikipedia.org/wiki/Message_Passing_Interface))。当企业的所有数据在放在了HDFS上，有多种处理数据的方式就很重要了。\n\n而且，MR本质上是面向批处理的，并不支持实时或接近实时的处理请求，但是业界也希望Hadoop能支持实时计算。（我也一直希望可以支持实时计算，但是有时候觉得有点贪心，专注做一项不就好了么？但是好像人的贪欲是无穷的）\n\n有了以上的需求，为了降低了管理者使用成本，减少数据在HDFS和其它存储设备的迁移，Hadoop开发组织重新投入了Hadoop设计。\n\n#### 低可扩展性\n\n摩尔定律一直在生效，也让商用服务器的性能一直提高，以下就是一台商用服务器在不同时间的配置：\n\n* 2009 - 8 cores, 16GB of RAM, 4*1TB disk\n* 2012 - 16+ cores, 48-96GB of RAM, 12*2TB or 12*3TB of disk\n\n按照上面的配置，大约2-3年，服务器的配置就可以翻翻。而现在的Hadoop集群就只能支持10,000个节点和200,000个核。Hadoop软件需要赶上硬件的速度是非常重要的。顺带说句，我们公司的计算型服务器就是16cores 64GB of RAM。\n\n#### 服务器的低利用率\n\n在现在的系统中，JobTracker将管理集群视为很多的Map/Reduce槽（slot），然而在MR用运行的时候，大多数时候都是reduce槽在等待map槽完成（map 100% reduce 0%）。如果能优化这个的话，服务器就可以得到最大的利用。\n\n#### 使用的灵活性\n\n在现实生产环境中，Hadoop常常被部署成一个共享的、多用户的系统。这样就会导致一种情况，完全Hadoop软件可能会影响到整个部门。用户希望能够控制hadoop软件栈升级，因此，允许多版本的MapReduce框架并存对Hadoop来说就是很重要的了。\n\n### Apache Hadoop YARN\n\nYARN的基本思想是将JobTracker的两个主要职责给解耦：资源管理和任务管理（监控/调度），YARN将其分成了两个部分：全局的ResourceManager(RM)和给每个应用分配的ApplicationMaster(AM)。ResourceManager和它每个节点的slave——NodeManager(NM)，形成了一个全新的、用以管理应用的分布式系统。\n\nRM是系统资源的终极管理者，而AM则是一个特定应用框架的实体（每次提交任务的时候，需要编写相应的应用框架，现在只支持MapReduce），需要与RM索要应用资源，和NM一起执行和监控任务。\n\nRM中有调度器，而调度器内嵌有策略可插拔的插件，主要负责将集群中得资源分配给多个队列和应用。当前MapReduce的调度器，如Capacity Scheduler和Fair Scheduler，均可作为该插件。但是调度器的职责仅限于调度任务，并不保证任务的容错性。\n\nNodeManager有点类似于TaskTracker，它负责启动应用程序Container（类似于JVM），并监控container的资源（CPU、内存、磁盘、网络等），并将信息上报给ResouceManager。需要注意的是，调度器就是根据应用程序的Container进行调度的。\n\nApplicationMaster负责向调度器请求合适的container，并监控container的状态以及任务进程。\n\n下图是YARN的架构图：\n\n![YARNArch](/images/2012/09/YARNArch.png)\n\n新YARN系统比较重要的一条就是复用了原有的MapReduce框架，而并不需要大的改动，这对现有的MR应用以及用户来说，是非常重要的，具体是怎么复用的，以后再细说。\n\n接下来，Hadoop开发者会深入架构细节，继续提高系统的可扩展性，并让其支持更多的数据处理框架（graph, MPI）并提高集群可用性。\n\n以Hortonworks' Arun Murthy（YARN开发者）的一段话做结尾吧：\n\n> “People are not going to be comfortable buying a $5 million Hadoop cluster just to do MapReduce and a $2 million cluster to do something else. If you can allow them to run both apps in the same cluster, its not only easier for you in terms of a CapEx perspective … it’s also easier from an operational perspective because you don’t have to have two separate sets of people managing your clusters or two sets of tools for managing your clusters.”\n","slug":"2012/09/apache-hadoop-yarn-background-and-an-overview","published":1,"updated":"2015-12-31T12:15:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba932004x3x8fq7entvla"},{"title":"Google Doodle for Turing","id":"564","date":"2012-06-23T13:29:27.000Z","_content":"\n![clip_image001](/images/2012/06/clip_image001.jpg)\n\n今天是图灵的诞辰100周年，Google又推出了有意思的doodle。今天的doodle主要是围绕着图灵机来的，需要完成多个图灵机规则表的选择，共有6关，每完成一关Google就亮一个字母，直到所有字母都亮起来为止。我就直接贴结果了，想知道原理的可以Google一下“图灵 doodle”或者“图灵机”。游戏截图参见文尾。\n\n<!--more-->  \n\n> 1. 6盘通关后，还会出现更进阶的版本，这篇并没提到。\n> 2. 谷奥的这篇介绍得不错。[猛击](http://www.guao.hk/posts/alan-mathison-turings-birthday-2012.html)\n\n玩到最后，让我最感兴趣不是图灵机，而是Google所对应的6个二进制串是怎么来的？{01011, 00011, 00011, 01011, 01001, 10000}\n\n看到G/g与o/o对应的串是相同的，可以猜想这六个串不是随机得出的。对二进制排序、取反、求十进制、比较ascii码，均没发现任何规律。还是得Google一下，最后竟然发现这几个串对应的是国际电报2号码：ITA2（International Telegraph Alphabet Number 2，国际电报2号码）又称博多码（Baudot code）。有一种被玩了的感觉 `>_<`\n\n在后面游戏replay的时候，点击图下的红圈处，会有一个彩蛋。彩蛋所出来的图看着我头疼了，搜了一下说是Rabbit Sequence，应该就是斐波拉奇数列。有兴趣的童鞋可以仔细地研究研究最后的Rabbit图灵机。\n\n![clip_image003](/images/2012/06/clip_image003.jpg)\n\n![clip_image005](/images/2012/06/clip_image005.jpg)\n\nRabbit Sequence\n\n第一轮游戏截图：\n\n![clip_image007](/images/2012/06/clip_image007.jpg)\n\n![clip_image009](/images/2012/06/clip_image009.jpg)\n\n![clip_image011](/images/2012/06/clip_image011.jpg)\n\n![clip_image013](/images/2012/06/clip_image013.jpg)\n\n![clip_image015](/images/2012/06/clip_image015.jpg)\n\n[![clip_image017](/images/2012/06/clip_image017.jpg)\n","source":"_posts/2012/06/google-doodle-for-turing.md","raw":"title: Google Doodle for Turing\ntags:\n  - Google\nid: 564\ncategories:\n  - 生活分享\ndate: 2012-06-23 21:29:27\n---\n\n![clip_image001](/images/2012/06/clip_image001.jpg)\n\n今天是图灵的诞辰100周年，Google又推出了有意思的doodle。今天的doodle主要是围绕着图灵机来的，需要完成多个图灵机规则表的选择，共有6关，每完成一关Google就亮一个字母，直到所有字母都亮起来为止。我就直接贴结果了，想知道原理的可以Google一下“图灵 doodle”或者“图灵机”。游戏截图参见文尾。\n\n<!--more-->  \n\n> 1. 6盘通关后，还会出现更进阶的版本，这篇并没提到。\n> 2. 谷奥的这篇介绍得不错。[猛击](http://www.guao.hk/posts/alan-mathison-turings-birthday-2012.html)\n\n玩到最后，让我最感兴趣不是图灵机，而是Google所对应的6个二进制串是怎么来的？{01011, 00011, 00011, 01011, 01001, 10000}\n\n看到G/g与o/o对应的串是相同的，可以猜想这六个串不是随机得出的。对二进制排序、取反、求十进制、比较ascii码，均没发现任何规律。还是得Google一下，最后竟然发现这几个串对应的是国际电报2号码：ITA2（International Telegraph Alphabet Number 2，国际电报2号码）又称博多码（Baudot code）。有一种被玩了的感觉 `>_<`\n\n在后面游戏replay的时候，点击图下的红圈处，会有一个彩蛋。彩蛋所出来的图看着我头疼了，搜了一下说是Rabbit Sequence，应该就是斐波拉奇数列。有兴趣的童鞋可以仔细地研究研究最后的Rabbit图灵机。\n\n![clip_image003](/images/2012/06/clip_image003.jpg)\n\n![clip_image005](/images/2012/06/clip_image005.jpg)\n\nRabbit Sequence\n\n第一轮游戏截图：\n\n![clip_image007](/images/2012/06/clip_image007.jpg)\n\n![clip_image009](/images/2012/06/clip_image009.jpg)\n\n![clip_image011](/images/2012/06/clip_image011.jpg)\n\n![clip_image013](/images/2012/06/clip_image013.jpg)\n\n![clip_image015](/images/2012/06/clip_image015.jpg)\n\n[![clip_image017](/images/2012/06/clip_image017.jpg)\n","slug":"2012/06/google-doodle-for-turing","published":1,"updated":"2015-12-31T12:16:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93400533x8fvzmqtrc2"},{"title":"Hadoop Pipes编程","id":"533","date":"2012-05-12T15:04:42.000Z","_content":"\n### 1、Hadoop Pipes简介\n\nHadoop Pipes是Hadoop MapReduce的C++接口代称。不同于使用标准输入和输出来实现的map代码和reduce代码之间的Streaming编程，Pipes使用Socket作为TaskTracker与C++进程之间数据传输的通道，数据传输为字节流。\n\n<!--more-->\n\n### 2、Hadoop Pipes编程初探\n\nHadoop Pipes可供开发者编写RecordReader、Mapper、Partitioner、Reducer、RecordWriter五个组件，当然，也可以自定义Combiner。\n\n网上有一大堆Hadoop Pipes的WordCount，个人觉得最好的WordCount还是Hadoop自带的，可以参见目录：$HADOOP_HOME/src/examples/pipes/impl\n\n与Pipes相关的头文件放在了目录：\n  > $HADOOP_HOME/c++/Linux-i386oramd64-32/include/hadoop/  \n\n主要的文件为Pipes.hh，该头文件定义了一些抽象类，除去开发者需要编写的五大组件之外，还有JobConf、TaskContext、Closeable、Factory四个。\n\nTaskContext：开发者可以从context中获取当前的key，value，progress和inputSplit等数据信息，当然，比较重要的就是调用emit将结果回传给Hadoop Framework。除了TaskContext，还有MapContext与ReduceContext，代码见下：\n\n``` c\nclass TaskContext {  \npublic:  \n  class Counter {  \n  private:  \n    int id;  \n  public:  \n    Counter(int counterId) : id(counterId) {}  \n    Counter(const Counter& counter) : id(counter.id) {}  \n    int getId() const { return id; }  \n  };  \n\n  virtual const JobConf* getJobConf() = 0;  \n  virtual const std::string& getInputKey() = 0;   \n  virtual const std::string& getInputValue() = 0;    \n  virtual void emit(const std::string& key, const std::string& value) = 0;    \n  virtual void progress() = 0;    \n  virtual void setStatus(const std::string& status) = 0;  \n  virtual Counter*   \ngetCounter(const std::string& group, const std::string& name) = 0;  \n  virtual void incrementCounter(const Counter* counter, uint64_t amount) = 0;  \n  virtual ~TaskContext() {}  \n};  \n\nclass MapContext: public TaskContext {  \npublic:  \n  virtual const std::string& getInputSplit() = 0;  \n  virtual const std::string& getInputKeyClass() = 0;  \n  virtual const std::string& getInputValueClass() = 0;  \n};  \n\nclass ReduceContext: public TaskContext {  \npublic:  \n  virtual bool nextValue() = 0;  \n};  \n```\n\nJobConf：开发者可以通过获得任务的属性\n\n``` c\nclass JobConf {  \npublic:  \n  virtual bool hasKey(const std::string& key) const = 0;  \n  virtual const std::string& get(const std::string& key) const = 0;  \n  virtual int getInt(const std::string& key) const = 0;  \n  virtual float getFloat(const std::string& key) const = 0;  \n  virtual bool getBoolean(const std::string&key) const = 0;  \n  virtual ~JobConf() {}  \n};  \n```\n\nCloseable：这个抽象类是五大组件的基类，只有两个方法，一个close()，一个析构函数。这个设计还是挺有Java风格的。\n\nFactory：一个抽象工厂，用来创建五大组件的类，是模版工厂的基类。具体的可以参见TemplateFactory.hh。开发者在调用runTask时，创建相应的Factory传入即可。\n\n### 3、Hadoop Pipes编程\n\n有了以上的基础知识，就可以开始编写MapReduce任务了。我们可以直接从examples着手，先来看看wordcount-simple.cc。\n\n\n``` c\n// wordcount-simple.cc -> Mapper & Reducer\nclass WordCountMap: public HadoopPipes::Mapper {  \npublic:  \n  HadoopPipes::TaskContext::Counter* inputWords;  \n\n  WordCountMap(HadoopPipes::TaskContext& context) {  \n    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);  \n  }  \n\n  void map(HadoopPipes::MapContext& context) {  \n    std::vector<std::string> words =   \n      HadoopUtils::splitString(context.getInputValue(), \" \");  \n    for(unsigned int i=0; i < words.size(); ++i) {  \n      context.emit(words[i], \"1\");  \n    }  \n    context.incrementCounter(inputWords, words.size());  \n  }  \n};  \n\nclass WordCountReduce: public HadoopPipes::Reducer {  \npublic:  \n  HadoopPipes::TaskContext::Counter* outputWords;  \n\n  WordCountReduce(HadoopPipes::TaskContext& context) {  \n    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);  \n  }  \n\n  void reduce(HadoopPipes::ReduceContext& context) {  \n    int sum = 0;  \n    while (context.nextValue()) {  \n      sum += HadoopUtils::toInt(context.getInputValue());  \n    }  \n    context.emit(context.getInputKey(), HadoopUtils::toString(sum));  \n    context.incrementCounter(outputWords, 1);   \n  }  \n};\n```  \n\n该任务编写了两个主要组件，mapper与reducer。要实现这两个组件需要继承相应的基类。基类声明如下：\n\n``` c\nclass Mapper: public Closable {  \npublic:  \n  virtual void map(MapContext& context) = 0;  \n};  \n\nclass Reducer: public Closable {  \npublic:  \n  virtual void reduce(ReduceContext& context) = 0;  \n};  \n```\n\n继承了相应的基类，就可以大胆的通过context获得key/value实现自己的逻辑了，结果处理完毕后，需要通过context.emit(key, value)将结果发送到下一阶段。\n\n注：\n\n1. 由于Factory创建对象需要传入Context对象，所以还需要实现一个构造函数，参数为TaskContext。\n2. Hadoop Pipes内部规定，map与reduce的key/value均为Text类型，在C++中表现为string类型。不过，Hadoop还是做得比较贴心，有专门的方法负责处理string，具体可以参见StringUtils.hh。\n3. Counter可以称之为统计器，可供开发者统计一些需要的数据，如读入行数、处理字节数等。任务完毕后，可以在web控制参看结果。\n\n```\n// wordcount-part.cc -> Partitioner\nclass WordCountPartitioner: public HadoopPipes::Partitioner {\n    public:\n    WordCountPartitioner(HadoopPipes::TaskContext& context){}\n         virtual int partition(const std::string& key, int numOfReduces) {   \n           return 0;\n        }\n};  \n```\n\n该实例在提供简单Mapper与Reducer方法的同时，还提供了Partitioner，实例实现较为简单，直接返回了第一个reduce位置。开发者自定义的Partitioner同mapper/reducer一致，需要继承其基类HadoopPipes:: RecordWriter，也需要提供一个传入TaskContext的构造函数，它的声明如下：\n\n``` c\nclass Partitioner {   \n   public:   \n    virtual int partition(const std::string& key, int numOfReduces) = 0;   \n    virtual ~Partitioner() {}   \n};\n```\n\nPartitioner编写方法与Java的一致，对于partition方法，框架会自动为它传入两个参数，分别为key值和reduce task的个数numOfReduces，用户只需返回一个0~ numOfReduces-1的值即可。\n\nwordcount-nopipe.cc -> RecordReader & RecordWriter\n\n这个实例的命名让我思考了很久，是nopipe还是nopart呢？该实例没有实现Partitioner，实现了RecordReader与RecordWriter。框架在运行之初，检查到开发者没有使用Java内置的RecordWriter，所以就只将InputSplit信息通过Pipes发送给C++ Task，由Task实现自身的Record读方法。同样，在Record写数据时任务也没走Pipes，直接将数据写到了相应的位置，写临时文件会直接写到磁盘，写HDFS则需要通过libhdfs进行写操作。具体Pipes运行流程，请参见下篇博文。\n\nRecordReader/RecordWriter实现较长，这里就不贴了，贴一下这俩的基类：\n\n```\nclass RecordReader: public Closable {   \n   public:   \n   virtual bool next(std::string& key, std::string& value) = 0;   4.   // 读进度   \n      virtual float getProgress() = 0;   \n};   \nclass RecordWriter: public Closable {   \n   public:   \n   virtual void emit(const std::string& key,\n                     const std::string& value) = 0;\n};\n```    \n\n对于RecordReader，用户自定义的构造函数需携带类型为HadoopPipes::MapContext的参数（而不能是TaskContext），通过该参数的getInputSplit()的方法，用户可以获取经过序列化的InpuSplit对象，Java端采用不同的InputFormat可导致InputSplit对象格式不同，但对于大多数InpuSplit对象，它们可以提供至少三个信息：当前要处理的InputSplit所在的文件名，所在文件中的偏移量，它的长度。用户获取这三个信息后，可使用libhdfs库读取文件，以实现next方法。\n\n用户自定的RecordWriter的构造函数需携带参数TaskContext，通过该参数的getJobConf()可获取一个HadoopPipes::JobConf的对象，用户可从该对象中获取该reduce task的各种参数，如：该reduce task的编号（这对于确定输出文件名有用），reduce task的输出目录等。同时实现emit方法，将数据写入文件。\n\n### 4、Hadoop Pipes任务提交\n\nHadoop Pipes任务提交命令根据Hadoop版本而不一，主体的命令有如下：\n\n```\nhadoop pipes [-conf <path>] [-D <key=value>, <key=value>, …] [-input <path>] [-output <path>] [-jar <jar file>] [-inputformat <class>] [-map <class>] [-partitioner <class>] [-reduce <class>] [-writer <class>] [-program <executable>]\n```\n\n想使用其它静态数据的话，还可以使用-files命令，该命令就是DistributedCache，直接将静态数据分发到所有datanode上。具体机制参见：[DistributedCache](http://www.hongweiyi.com/2012/02/iterative-mapred-distcache/)。使用如下：\n\n> shell: bin/hadoop pipes … -files dict.txt\n>\n> c: file = fopen(“dict.txt”, “r”); // 直接根据文件名读取  \n\n### 5、小结\n\n本篇博文简要了说了一下Hadoop Pipes的使用方法，下篇博文会对Hadoop Pipes的运行机制进行一个深入的讲解。\n\n在这里贴一下董的优化意见：为了提高系能，RecordReader和RecordWriter最好采用Java代码实现（或者重用Hadoop中自带的），这是因为Hadoop自带的C++库libhdfs采用JNI实现，底层还是要调用Java相关接口，效率很低，此外，如果要处理的文件为二进制文件或者其他非文本文件，libhdfs可能不好处理。\n\n> 参考资料：\n>\n> 董的博客: [Hadoop pipes编程](http://dongxicheng.org/mapreduce/hadoop-pipes-programming/)\n>\n> 《Hadoop权威指南》\n","source":"_posts/2012/05/hadoop-pipes.md","raw":"title: Hadoop Pipes编程\ntags:\n  - Hadoop\n  - Hadoop Pipes\n  - MapReduce\nid: 533\ncategories:\n  - 技术分享\ndate: 2012-05-12 23:04:42\n---\n\n### 1、Hadoop Pipes简介\n\nHadoop Pipes是Hadoop MapReduce的C++接口代称。不同于使用标准输入和输出来实现的map代码和reduce代码之间的Streaming编程，Pipes使用Socket作为TaskTracker与C++进程之间数据传输的通道，数据传输为字节流。\n\n<!--more-->\n\n### 2、Hadoop Pipes编程初探\n\nHadoop Pipes可供开发者编写RecordReader、Mapper、Partitioner、Reducer、RecordWriter五个组件，当然，也可以自定义Combiner。\n\n网上有一大堆Hadoop Pipes的WordCount，个人觉得最好的WordCount还是Hadoop自带的，可以参见目录：$HADOOP_HOME/src/examples/pipes/impl\n\n与Pipes相关的头文件放在了目录：\n  > $HADOOP_HOME/c++/Linux-i386oramd64-32/include/hadoop/  \n\n主要的文件为Pipes.hh，该头文件定义了一些抽象类，除去开发者需要编写的五大组件之外，还有JobConf、TaskContext、Closeable、Factory四个。\n\nTaskContext：开发者可以从context中获取当前的key，value，progress和inputSplit等数据信息，当然，比较重要的就是调用emit将结果回传给Hadoop Framework。除了TaskContext，还有MapContext与ReduceContext，代码见下：\n\n``` c\nclass TaskContext {  \npublic:  \n  class Counter {  \n  private:  \n    int id;  \n  public:  \n    Counter(int counterId) : id(counterId) {}  \n    Counter(const Counter& counter) : id(counter.id) {}  \n    int getId() const { return id; }  \n  };  \n\n  virtual const JobConf* getJobConf() = 0;  \n  virtual const std::string& getInputKey() = 0;   \n  virtual const std::string& getInputValue() = 0;    \n  virtual void emit(const std::string& key, const std::string& value) = 0;    \n  virtual void progress() = 0;    \n  virtual void setStatus(const std::string& status) = 0;  \n  virtual Counter*   \ngetCounter(const std::string& group, const std::string& name) = 0;  \n  virtual void incrementCounter(const Counter* counter, uint64_t amount) = 0;  \n  virtual ~TaskContext() {}  \n};  \n\nclass MapContext: public TaskContext {  \npublic:  \n  virtual const std::string& getInputSplit() = 0;  \n  virtual const std::string& getInputKeyClass() = 0;  \n  virtual const std::string& getInputValueClass() = 0;  \n};  \n\nclass ReduceContext: public TaskContext {  \npublic:  \n  virtual bool nextValue() = 0;  \n};  \n```\n\nJobConf：开发者可以通过获得任务的属性\n\n``` c\nclass JobConf {  \npublic:  \n  virtual bool hasKey(const std::string& key) const = 0;  \n  virtual const std::string& get(const std::string& key) const = 0;  \n  virtual int getInt(const std::string& key) const = 0;  \n  virtual float getFloat(const std::string& key) const = 0;  \n  virtual bool getBoolean(const std::string&key) const = 0;  \n  virtual ~JobConf() {}  \n};  \n```\n\nCloseable：这个抽象类是五大组件的基类，只有两个方法，一个close()，一个析构函数。这个设计还是挺有Java风格的。\n\nFactory：一个抽象工厂，用来创建五大组件的类，是模版工厂的基类。具体的可以参见TemplateFactory.hh。开发者在调用runTask时，创建相应的Factory传入即可。\n\n### 3、Hadoop Pipes编程\n\n有了以上的基础知识，就可以开始编写MapReduce任务了。我们可以直接从examples着手，先来看看wordcount-simple.cc。\n\n\n``` c\n// wordcount-simple.cc -> Mapper & Reducer\nclass WordCountMap: public HadoopPipes::Mapper {  \npublic:  \n  HadoopPipes::TaskContext::Counter* inputWords;  \n\n  WordCountMap(HadoopPipes::TaskContext& context) {  \n    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);  \n  }  \n\n  void map(HadoopPipes::MapContext& context) {  \n    std::vector<std::string> words =   \n      HadoopUtils::splitString(context.getInputValue(), \" \");  \n    for(unsigned int i=0; i < words.size(); ++i) {  \n      context.emit(words[i], \"1\");  \n    }  \n    context.incrementCounter(inputWords, words.size());  \n  }  \n};  \n\nclass WordCountReduce: public HadoopPipes::Reducer {  \npublic:  \n  HadoopPipes::TaskContext::Counter* outputWords;  \n\n  WordCountReduce(HadoopPipes::TaskContext& context) {  \n    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);  \n  }  \n\n  void reduce(HadoopPipes::ReduceContext& context) {  \n    int sum = 0;  \n    while (context.nextValue()) {  \n      sum += HadoopUtils::toInt(context.getInputValue());  \n    }  \n    context.emit(context.getInputKey(), HadoopUtils::toString(sum));  \n    context.incrementCounter(outputWords, 1);   \n  }  \n};\n```  \n\n该任务编写了两个主要组件，mapper与reducer。要实现这两个组件需要继承相应的基类。基类声明如下：\n\n``` c\nclass Mapper: public Closable {  \npublic:  \n  virtual void map(MapContext& context) = 0;  \n};  \n\nclass Reducer: public Closable {  \npublic:  \n  virtual void reduce(ReduceContext& context) = 0;  \n};  \n```\n\n继承了相应的基类，就可以大胆的通过context获得key/value实现自己的逻辑了，结果处理完毕后，需要通过context.emit(key, value)将结果发送到下一阶段。\n\n注：\n\n1. 由于Factory创建对象需要传入Context对象，所以还需要实现一个构造函数，参数为TaskContext。\n2. Hadoop Pipes内部规定，map与reduce的key/value均为Text类型，在C++中表现为string类型。不过，Hadoop还是做得比较贴心，有专门的方法负责处理string，具体可以参见StringUtils.hh。\n3. Counter可以称之为统计器，可供开发者统计一些需要的数据，如读入行数、处理字节数等。任务完毕后，可以在web控制参看结果。\n\n```\n// wordcount-part.cc -> Partitioner\nclass WordCountPartitioner: public HadoopPipes::Partitioner {\n    public:\n    WordCountPartitioner(HadoopPipes::TaskContext& context){}\n         virtual int partition(const std::string& key, int numOfReduces) {   \n           return 0;\n        }\n};  \n```\n\n该实例在提供简单Mapper与Reducer方法的同时，还提供了Partitioner，实例实现较为简单，直接返回了第一个reduce位置。开发者自定义的Partitioner同mapper/reducer一致，需要继承其基类HadoopPipes:: RecordWriter，也需要提供一个传入TaskContext的构造函数，它的声明如下：\n\n``` c\nclass Partitioner {   \n   public:   \n    virtual int partition(const std::string& key, int numOfReduces) = 0;   \n    virtual ~Partitioner() {}   \n};\n```\n\nPartitioner编写方法与Java的一致，对于partition方法，框架会自动为它传入两个参数，分别为key值和reduce task的个数numOfReduces，用户只需返回一个0~ numOfReduces-1的值即可。\n\nwordcount-nopipe.cc -> RecordReader & RecordWriter\n\n这个实例的命名让我思考了很久，是nopipe还是nopart呢？该实例没有实现Partitioner，实现了RecordReader与RecordWriter。框架在运行之初，检查到开发者没有使用Java内置的RecordWriter，所以就只将InputSplit信息通过Pipes发送给C++ Task，由Task实现自身的Record读方法。同样，在Record写数据时任务也没走Pipes，直接将数据写到了相应的位置，写临时文件会直接写到磁盘，写HDFS则需要通过libhdfs进行写操作。具体Pipes运行流程，请参见下篇博文。\n\nRecordReader/RecordWriter实现较长，这里就不贴了，贴一下这俩的基类：\n\n```\nclass RecordReader: public Closable {   \n   public:   \n   virtual bool next(std::string& key, std::string& value) = 0;   4.   // 读进度   \n      virtual float getProgress() = 0;   \n};   \nclass RecordWriter: public Closable {   \n   public:   \n   virtual void emit(const std::string& key,\n                     const std::string& value) = 0;\n};\n```    \n\n对于RecordReader，用户自定义的构造函数需携带类型为HadoopPipes::MapContext的参数（而不能是TaskContext），通过该参数的getInputSplit()的方法，用户可以获取经过序列化的InpuSplit对象，Java端采用不同的InputFormat可导致InputSplit对象格式不同，但对于大多数InpuSplit对象，它们可以提供至少三个信息：当前要处理的InputSplit所在的文件名，所在文件中的偏移量，它的长度。用户获取这三个信息后，可使用libhdfs库读取文件，以实现next方法。\n\n用户自定的RecordWriter的构造函数需携带参数TaskContext，通过该参数的getJobConf()可获取一个HadoopPipes::JobConf的对象，用户可从该对象中获取该reduce task的各种参数，如：该reduce task的编号（这对于确定输出文件名有用），reduce task的输出目录等。同时实现emit方法，将数据写入文件。\n\n### 4、Hadoop Pipes任务提交\n\nHadoop Pipes任务提交命令根据Hadoop版本而不一，主体的命令有如下：\n\n```\nhadoop pipes [-conf <path>] [-D <key=value>, <key=value>, …] [-input <path>] [-output <path>] [-jar <jar file>] [-inputformat <class>] [-map <class>] [-partitioner <class>] [-reduce <class>] [-writer <class>] [-program <executable>]\n```\n\n想使用其它静态数据的话，还可以使用-files命令，该命令就是DistributedCache，直接将静态数据分发到所有datanode上。具体机制参见：[DistributedCache](http://www.hongweiyi.com/2012/02/iterative-mapred-distcache/)。使用如下：\n\n> shell: bin/hadoop pipes … -files dict.txt\n>\n> c: file = fopen(“dict.txt”, “r”); // 直接根据文件名读取  \n\n### 5、小结\n\n本篇博文简要了说了一下Hadoop Pipes的使用方法，下篇博文会对Hadoop Pipes的运行机制进行一个深入的讲解。\n\n在这里贴一下董的优化意见：为了提高系能，RecordReader和RecordWriter最好采用Java代码实现（或者重用Hadoop中自带的），这是因为Hadoop自带的C++库libhdfs采用JNI实现，底层还是要调用Java相关接口，效率很低，此外，如果要处理的文件为二进制文件或者其他非文本文件，libhdfs可能不好处理。\n\n> 参考资料：\n>\n> 董的博客: [Hadoop pipes编程](http://dongxicheng.org/mapreduce/hadoop-pipes-programming/)\n>\n> 《Hadoop权威指南》\n","slug":"2012/05/hadoop-pipes","published":1,"updated":"2016-01-02T16:15:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93500573x8flv87dcot"},{"title":"Hadoop Pipes运行机制","id":"541","date":"2012-05-13T06:20:19.000Z","_content":"\n### 1、前言\n\nHadoop Pipes可供C++开发者开发MapReduce任务。文献与书籍上也写了，C++与Java是通过Socket通信，但是具体的运行机制是什么还是得参考源码。\n\n这篇博文主要从源码角度来讲解Hadoop Pipes运行机制以及设计原理，实际的Hadoop Pipes编程请参见：[Hadoop Pipes编程](http://www.hongweiyi.com/2012/05/hadoop-pipes/)\n\n<!--more-->\n\n### 2、Hadoop Pipes运行图解\n\n![image](/images/2012/05/image.png)\n\n### 3、Hadoop运行机制\n\nHadoop端主要类均在org.apache.hadoop.mapred.pipes包下，见下图。\n\n其中，Application是JVM中主要运行程序，PipesMapRunner、PipesReducer、PipesPartitioner、PipesNonJavaInputFormat分别对应C++版的Mapper、Reducer、Partitioner、RecordReader，由于重写RecordWriter后，C++会直接写文件，这里就没有对应的类了。DownwardProtocol/BinaryProtocol、UpwardProtocol/OutputProtocol是Java与C++交互的接口代理类。\n\n![image](/images/2012/05/image1.png)\n\n开发者通过$HADOOP_HOME/bin/hadoop pipes将作业提交到了包下的Submitter类。运行过程就直接贴文字了，可以结合代码一起看：    \n```\n1 解析命令行参数\n2 setupPipes(job)\n2.1 设置Mapper，Partitioner，Reducer，RecordWriter，如果不是java编写的，则用PipesMapRunner，PipesPartitioner，PipesReducer，NullOutputFormat（所有输出均输出到/dev/null中）；\n2.2 设置map/reduce的key/value class，均为Text.class；\n2.3 设置RecordReader，如果不是java编写的，则用PipesNonJavaInputFormat；\n2.4 获得运行程序，debug脚本以及缓存文件；\n3 JobClient.submitJob(job);\n```\n\nJobClient提交任务和非Pipes编程提交过程一致，进行Task调度分配之后，就会在分配的TaskTracker上开启JVM进程，运行Runner。这里解析一下PipesMapRunner的运行机制：    \n\n```\n1 创建Application\n1.1 创建ServerSocket\n1.2 设置环境（临时文件位置、命令端口等）\n1.3 获得执行文件，并设置执行权限（chmod +x）\n1.4 执行任务，通过java.lang.ProcessBuilder\n1.5 创建任务交互代理，DownwardProcotol对象。\n1.5.1 创建接收交互代理，UplinkReaderThread对象\n1.5.2 循环接受客户端的请求\n1.6 downlink.start()，发送消息，客户端可以开始运行\n2 如果不是Java编写的RecordReader，直接发送一个InputSplit（注：只是Split的信息，不包括文件数据）给客户端；反之，发送InputSplit之后，再循环读取split，将record格式化之后，将KVP发给客户端。\n```\n\n2 如果不是Java编写的RecordReader，直接发送一个InputSplit（注：只是Split的信息，不包括文件数据）给客户端；反之，发送InputSplit之后，再循环读取split，将record格式化之后，将KVP发给客户端。\n\n```\n1 创建Application，与Map一致；\n2 向客户端发送key，之后循环发送value。\n```\n\n以上是Hadoop端的运行机制，C++端的与Java的也基本一致，源文件在$HADOOP_HOME/src/c++/pipes/impl/HadoopPipes.cc\n\n在组件运行时，会用ProcessBuilder运行C++可执行文件，可执行文件的main程序基本上都是这样写的：\n\n```\nint main(int argc, char *argv[]) {\n    return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap,                                   WordCountReduce>());   \n}  \n```\n\n调用了HadoopUtils::runTask(factory)方法，运行机制如下：\n\n```\n1 创建运行环境；\n2 获得socket端口，如果有端口，则创建socket，并获得其输入输出流。如果没有端口，则获得文件输出输入流；\n3 创建ping线程，该线程每隔5s发送一次心跳信息；\n4 等待接受任务；\n5 循环获得任务消息，直到结束（done）；\n6 通知Hadoop完成任务，关闭流\n```\n\n### 4、Hadoop Pipes浅析\n\nHadoop Pipes采用类RPC机制，封装了Hadoop端与C++端的调用接口。Hadoop调用C++的协议为DownwardProtocol，C++调用Hadoop的为UpwardProtocol。同时也封装了传输数据序列化的接口（SerialUtils.cc），代码结构十分清晰。\n\n但是实际使用中也有一定缺陷，调试起来十分麻烦。C++端挂了之后，Hadoop也就接受不到的心跳消息，所以错误一律为：Pipes Broken。Apache的维基上有一个条目：[howToDebugMapReducePrograms](http://wiki.apache.org/hadoop/HowToDebugMapReducePrograms)，改天得好好研究一下。\n\n> 参考资料：\n>\n> [董的博客](http://dongxicheng.org/mapreduce/hadoop-pipes-architecture/)\n>\n> Hadoop源码\n","source":"_posts/2012/05/hadoop-pipes-src.md","raw":"title: Hadoop Pipes运行机制\ntags:\n  - Hadoop\n  - Hadoop Pipes\n  - MapReduce\nid: 541\ncategories:\n  - 技术分享\ndate: 2012-05-13 14:20:19\n---\n\n### 1、前言\n\nHadoop Pipes可供C++开发者开发MapReduce任务。文献与书籍上也写了，C++与Java是通过Socket通信，但是具体的运行机制是什么还是得参考源码。\n\n这篇博文主要从源码角度来讲解Hadoop Pipes运行机制以及设计原理，实际的Hadoop Pipes编程请参见：[Hadoop Pipes编程](http://www.hongweiyi.com/2012/05/hadoop-pipes/)\n\n<!--more-->\n\n### 2、Hadoop Pipes运行图解\n\n![image](/images/2012/05/image.png)\n\n### 3、Hadoop运行机制\n\nHadoop端主要类均在org.apache.hadoop.mapred.pipes包下，见下图。\n\n其中，Application是JVM中主要运行程序，PipesMapRunner、PipesReducer、PipesPartitioner、PipesNonJavaInputFormat分别对应C++版的Mapper、Reducer、Partitioner、RecordReader，由于重写RecordWriter后，C++会直接写文件，这里就没有对应的类了。DownwardProtocol/BinaryProtocol、UpwardProtocol/OutputProtocol是Java与C++交互的接口代理类。\n\n![image](/images/2012/05/image1.png)\n\n开发者通过$HADOOP_HOME/bin/hadoop pipes将作业提交到了包下的Submitter类。运行过程就直接贴文字了，可以结合代码一起看：    \n```\n1 解析命令行参数\n2 setupPipes(job)\n2.1 设置Mapper，Partitioner，Reducer，RecordWriter，如果不是java编写的，则用PipesMapRunner，PipesPartitioner，PipesReducer，NullOutputFormat（所有输出均输出到/dev/null中）；\n2.2 设置map/reduce的key/value class，均为Text.class；\n2.3 设置RecordReader，如果不是java编写的，则用PipesNonJavaInputFormat；\n2.4 获得运行程序，debug脚本以及缓存文件；\n3 JobClient.submitJob(job);\n```\n\nJobClient提交任务和非Pipes编程提交过程一致，进行Task调度分配之后，就会在分配的TaskTracker上开启JVM进程，运行Runner。这里解析一下PipesMapRunner的运行机制：    \n\n```\n1 创建Application\n1.1 创建ServerSocket\n1.2 设置环境（临时文件位置、命令端口等）\n1.3 获得执行文件，并设置执行权限（chmod +x）\n1.4 执行任务，通过java.lang.ProcessBuilder\n1.5 创建任务交互代理，DownwardProcotol对象。\n1.5.1 创建接收交互代理，UplinkReaderThread对象\n1.5.2 循环接受客户端的请求\n1.6 downlink.start()，发送消息，客户端可以开始运行\n2 如果不是Java编写的RecordReader，直接发送一个InputSplit（注：只是Split的信息，不包括文件数据）给客户端；反之，发送InputSplit之后，再循环读取split，将record格式化之后，将KVP发给客户端。\n```\n\n2 如果不是Java编写的RecordReader，直接发送一个InputSplit（注：只是Split的信息，不包括文件数据）给客户端；反之，发送InputSplit之后，再循环读取split，将record格式化之后，将KVP发给客户端。\n\n```\n1 创建Application，与Map一致；\n2 向客户端发送key，之后循环发送value。\n```\n\n以上是Hadoop端的运行机制，C++端的与Java的也基本一致，源文件在$HADOOP_HOME/src/c++/pipes/impl/HadoopPipes.cc\n\n在组件运行时，会用ProcessBuilder运行C++可执行文件，可执行文件的main程序基本上都是这样写的：\n\n```\nint main(int argc, char *argv[]) {\n    return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap,                                   WordCountReduce>());   \n}  \n```\n\n调用了HadoopUtils::runTask(factory)方法，运行机制如下：\n\n```\n1 创建运行环境；\n2 获得socket端口，如果有端口，则创建socket，并获得其输入输出流。如果没有端口，则获得文件输出输入流；\n3 创建ping线程，该线程每隔5s发送一次心跳信息；\n4 等待接受任务；\n5 循环获得任务消息，直到结束（done）；\n6 通知Hadoop完成任务，关闭流\n```\n\n### 4、Hadoop Pipes浅析\n\nHadoop Pipes采用类RPC机制，封装了Hadoop端与C++端的调用接口。Hadoop调用C++的协议为DownwardProtocol，C++调用Hadoop的为UpwardProtocol。同时也封装了传输数据序列化的接口（SerialUtils.cc），代码结构十分清晰。\n\n但是实际使用中也有一定缺陷，调试起来十分麻烦。C++端挂了之后，Hadoop也就接受不到的心跳消息，所以错误一律为：Pipes Broken。Apache的维基上有一个条目：[howToDebugMapReducePrograms](http://wiki.apache.org/hadoop/HowToDebugMapReducePrograms)，改天得好好研究一下。\n\n> 参考资料：\n>\n> [董的博客](http://dongxicheng.org/mapreduce/hadoop-pipes-architecture/)\n>\n> Hadoop源码\n","slug":"2012/05/hadoop-pipes-src","published":1,"updated":"2016-01-02T16:15:30.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba937005d3x8f3jdsygqk"},{"title":"新手说自然语言处理","id":"504","date":"2012-04-09T07:41:27.000Z","_content":"\n### 1、自然语言处理（Natural Language Processing）\n\n看自然语言处理的材料、书籍也有一段时间了，最近好像快看出点门道了，今天就以一个新手的角度来说说我所理解的自然语言处理。\n\n<!--more-->\n\n自然语言处理是研究计算机如何处理人类语言的学科，我一般大白话解释就是：让计算机能懂我的话，这就叫自然语言处理。不过维基百科将“懂”人类语言称之为“自然语言认知”，我认为无论对自然语言做何种处理，计算机都需要“懂”一点这个语言，无论以何种方式。至于何谓计算机的“懂”，我觉得可以参考一下[图灵测试（Turing Test）](http://en.wikipedia.org/wiki/Turing_test)，以及前两篇转载的Matrix67的博客，你应该会对计算机的“懂”有所理解。\n\n平常和朋友聊NLP，或者他们翻我的书时，一般都会问一句：“怎么这么像编译原理呢？”。从我看来，基本思想差不太多，所不同的是编译原理所处理的是人工语言，而NLP则是处理分析的自然语言。自然语言处理起来会有其特殊性，最常见也是最难的就是处理歧义了，在人工语言中决不允许出现歧义，因为规矩定死了，自然语言就不一样了，拿最近很火的小明来说：\n\n> “小明，昨天下午你抱着的是谁啊？你女朋友吧？”“你妹！！！我妹！！”  \n\n你们觉得计算机能知道小明抱着的是谁么？我相信，以后能！！！\n\n### 2、读书有感\n\n以上就是我对自然语言处理的大致理解，现在我来梳理梳理我看书的知识。\n\n#### 2.1 NLP研究方向\n\nNLP研究主要有两个方法：理性（规则）主义方法以及基于经验（统计）主义方法。现在主流的好像是理性与经验相结合。\n\n* 理性（规则）主义方法\n\n就是认为计算机必须按照人的思维方式来思考语言，按照严格的规则来处理自然语言。语言学家乔姆斯基（Noam Chomsky）曾经把语言定义为：按照一定规律构成的句子和符号串的有限或无限的集合。我国学者吴蔚天则认为，可以将语言看成一个抽象的数学系统。语言无论是集合还是数学系统，都可以用数学的方法（规则）来刻画与描述。这个规则一般来说应该是语法，但是NLP中被定义为形式语言，是用来精确地描述语言及其结构的手段。\n\n形式语言的语法是一个四元组，G=（N，Σ，P，S）。N为非终结符（non-terminal symbol）的有限集合，Σ为终结符（terminal-symbol）的有限集合，N与Σ无交集，N并Σ就是称之为总词汇表。P是一组重写规则的有限集合：P = {a -&gt; b}，a，b均是词汇表中的，但是a中必须包含一个非终结符。S是初始符，也属于N。\n\n例如，有如下的形式语法：\n\n> G=（N，Σ，P，S）， N={S，B，N，A }\n>\n> Σ ={wikie，is，a，handsome，boy}  \n\n规则P如下：\n\n> S -> NBN， N -> AN，B -> BB\n>\n> B -> is | a，N ->wikie | boy，A -> handsome\n\n所以根据语法简单推导就是：\n\n> S -> NBN\n>\n> Wikie is boy或者Boy is wikie或者wikie a boy或者 boy a wikie  \n\n当然，最后的推导应该是这样：\n\n> S -> NBN -> NBBN -> NBBAN\n>\n> Wikie is a handsome boy.  \n\n觉得怎么样，以上语法是不是清晰明了？但这只是简单的情况，真实的情况下远比这复杂。\n\n* 基于经验（统计）主义方法\n\n以上形式语法虽然清晰明了，但是应对真正的千变万化的语言来说，总有无法形式表达的情况。而且很多排斥理性主义方法的人都有这样的一个想法：文盲从来没有学过语法，但TA不仅能够理解语言，而且说不定还能说得很溜很好。那么文盲为什么能听懂且说出话来呢？\n\n语言模型在自然语言处理中占有重要的地位，一个语言模型通常构建为字符串s的概率分布p（s），这里的p（s）试图反映的是字符串s作为一个句子出现的频率。\n\n例如，Yes这个词出现在我们的英语课本中的频率非常高，所以同学们和老外交流的时候，大部分时间在听，听完之后不管啥内容都会说一句：Yes，Yes！还有Oh这个语气词无论在电影还是书本中，都很多，同学们在回答了一个Oh的时候，突然会很习惯性的接上一个词Yeah，即：Oh, yeah yeah，高兴的话，还有可能是：Oh, yeah, yeah, you’re right！\n\n这个频率和习惯都可以反映成语言模型，即某个句子出现的概率，概率高出现的可能性就大，低则反之。对于一个句子s=w1w2w3w4…wl来说，其概率计算公式可以表示为：\n\n> p(s) = p(w1)p(w2|w1)p(w3|w1w2)…p(wl|w1…w(l-1)) // | 表示条件概率  \n\n需要注意的是，如果以上计算中，有一个值为0，p(s)则为0。显然，这个是不够准确的，所以需要进行平滑操作。顾名思义，线平滑即将折线平滑成曲线，语言模型即提高低概率（如0概率），降低高概率，尽量使概率分布趋于均匀。\n\n有了语言模型，文盲还不能够听与说，因为TA不知道出现的那句话对应的意思是什么。所以文盲还会在脑子存一个音与意的映射，在NLP中就是语料库了。听到一个词，就去查一下，听到一个词，就去查一下。\n\n顺带简单说一下机器学习，以前我怎么都不理解机器到底是怎么学习的，可能是我也仔细分析过我是怎么学习的。机器学习和人其实差不多，归纳法加演绎法。如有一段材料：鸟会飞，鹦鹉是鸟。计算机就能学会：鹦鹉会飞。不过这仅是我粗浅的理解，还有待继续深入。\n\n#### 2.2 NLP理论基础\n\n上面介绍了两个大方向，理论基础也就是大方向的理论基础：语言学、概率论、信息论。在这里就不一一说了，不过我还是得单独都搞本书看看咯。\n\n#### 2.3 NLP分析步骤\n\n觉着还是和编译原理差不多，不过多了一个分词的步骤，具体见下：\n\n* 分词\n\n分词是每个语言都要碰到的问题，很多地方说英语没有这个问题，因为有空格作为分隔符。但是我觉得英语也有词组，分词组应该也不是一件容易的事情。具体分词可以参加：[漫话中文自动分词和语义识别（上）：中文分词算法](http://www.hongweiyi.com/2012/04/nlp-repost-segmentation/)。\n\n* 语法分析\n\n分完词后，就需要进行语法分析了，即分析这句话是否通顺合理。语法分析又分为词法分析与句法分析。词法分析就是分析诸如名词性短语、动词性短语，句法分析就是分析诸如主谓宾、从句等结构，但这都是从理性主义方法出发，关于这个可以参加：[漫话中文自动分词和语义识别（下）：句法结构和语义结构](http://www.hongweiyi.com/2012/04/nlp-repost-semantic/)。目前更实用的，则是对大规模真实语料的概率统计分析与机器学习算法。简单理解就是，一个词一个句，出现的概率大就是正确的。\n\n* 语义分析\n\n到了我觉得最难的地方了，句子的语义分析。高中考试的时候，句子出现歧义了，我这个大活人有时候都会理解错误，能让计算机理解更是难上加难。再加上国人说话隐晦，一语双关。说个有趣的HSK（汉语水平考试）考试题目：\n\n张三找了个女朋友，李四问：“你女朋友长得怎么样？”张三答：“她人还不错。”问：张三的女朋友长得好么？\n\nA、长得好 B、长得不好 C、她人不错 D、不知道\n\n套用书本的话说就是：自然语言的语义计算问题十分困难，如何模拟人脑思维的过程，建立语言、知识与客观世界之间可计算的逻辑关系，并实现具有高区分能力的语义计算模型，至今任是个未能解决的难题。\n\n#### 2.4 NLP应用领域\n\n* 机器翻译类\n\n这个应该是常人能看到体会到的，从文曲星到金山词霸再到Google翻译，机器翻译伴随着我们80后一起成长。Google翻译更是论文翻译的必须物，尽管得到的结果有时候会惨不忍睹。如：今天天气好好啊 -> Today the weather good ah。多么直白的翻译。\n\n机器翻译的主要方法和NLP的方法差不多，不过过程有些不一样，有两种：基于中间语言与不基于中间语言。\n\n基于中间语言，就是相当于将所有语言互译成一种语言（如世界语），所有全世界N种语言翻译就只需要与同一种语言互译N次即可。但是是否能够构造出表示各种不同的自然语言语法、语义的中间语言，至少目前还是个未知数。此外，由于翻译都是误差，误差传递两次会有更大的误差，无法很好的生成对应的各种语言。\n\n不基于中间语言的又有：基于统计和基于实例的，统计就和前面提到的语言模型类似，基于实例的就是将不断累积的已经译好的文本作为机器翻译的样本，翻译的时候直接查看是否有类似的翻译。\n\n话说，以前老师说做同声传译一小时有5W，等到机器翻译完美解决那一刻，同声传译一小时多少捏？哇哈哈，很是邪恶啊！当然，我们的路还有很长。\n\n* 阅读理解类\n\n阅读理解无外乎就是读完文章，让我们说出文章的中心思想，文章内容以及文章类型。说出文章的中心思想就是自动文摘生成技术，说出文章内容的就是信息抽取技术，说出文章类型就是文本分类技术。不过书还没看到这里来，就不继续写了。\n\n* 问答类\n\nSiri横空出世，让业内业外人士都开了眼界，52nlp中有一篇关于siri的文章，见：[这里](http://www.52nlp.cn/sir)，不知道作者是褒还是贬。同时，移动10086在siri之前也有小机器人可以相互扯淡，这里也有一个获奖的开源自然语言的人工智能的聊天机器人：[A.L.I.C.E](http://www.alicebot.org/)，我一朋友还和AliceBot聊了好一会儿天。\n\n问答类我觉得是NLP与AI关系最为密切的一个环节，也是图灵测试直接使用的工具，现在一个叫[CleverBot](http://cleverbot.com/)的机器人号称通过了图灵测试，成功欺骗了800位观众。当然，现在问答类的系统并不是仅仅用于扯淡的，主要的还是类似与10086那样的客服系统，基于某个领域的问答，难度相较而言没那么高。\n\n### 3、小结\n\n以上很多仅仅是一些肤浅的理解，对NLP内部实现有些许了解，但是不深入。还需要继续阅读书籍以及相关文献，而且还得再恶补数学相关的知识。NLP刚入门，路还长着……\n\n\n> 参考资料：\n>\n> 1. [维基百科](http://en.wikipedia.org/wiki/Wiki)\n> 2. [52NLP](http://www.52nlp.cn/)\n> 3. 《统计自然语言处理》 宗成庆\n> 4. 《自然语言处理的原理及其应用》 杨宪泽\n> 5. 《自然语言处理》 江铭虎\n","source":"_posts/2012/04/nlp-say-hi.md","raw":"title: 新手说自然语言处理\ntags:\n  - 自然语言处理\nid: 504\ncategories:\n  - 技术分享\ndate: 2012-04-09 15:41:27\n---\n\n### 1、自然语言处理（Natural Language Processing）\n\n看自然语言处理的材料、书籍也有一段时间了，最近好像快看出点门道了，今天就以一个新手的角度来说说我所理解的自然语言处理。\n\n<!--more-->\n\n自然语言处理是研究计算机如何处理人类语言的学科，我一般大白话解释就是：让计算机能懂我的话，这就叫自然语言处理。不过维基百科将“懂”人类语言称之为“自然语言认知”，我认为无论对自然语言做何种处理，计算机都需要“懂”一点这个语言，无论以何种方式。至于何谓计算机的“懂”，我觉得可以参考一下[图灵测试（Turing Test）](http://en.wikipedia.org/wiki/Turing_test)，以及前两篇转载的Matrix67的博客，你应该会对计算机的“懂”有所理解。\n\n平常和朋友聊NLP，或者他们翻我的书时，一般都会问一句：“怎么这么像编译原理呢？”。从我看来，基本思想差不太多，所不同的是编译原理所处理的是人工语言，而NLP则是处理分析的自然语言。自然语言处理起来会有其特殊性，最常见也是最难的就是处理歧义了，在人工语言中决不允许出现歧义，因为规矩定死了，自然语言就不一样了，拿最近很火的小明来说：\n\n> “小明，昨天下午你抱着的是谁啊？你女朋友吧？”“你妹！！！我妹！！”  \n\n你们觉得计算机能知道小明抱着的是谁么？我相信，以后能！！！\n\n### 2、读书有感\n\n以上就是我对自然语言处理的大致理解，现在我来梳理梳理我看书的知识。\n\n#### 2.1 NLP研究方向\n\nNLP研究主要有两个方法：理性（规则）主义方法以及基于经验（统计）主义方法。现在主流的好像是理性与经验相结合。\n\n* 理性（规则）主义方法\n\n就是认为计算机必须按照人的思维方式来思考语言，按照严格的规则来处理自然语言。语言学家乔姆斯基（Noam Chomsky）曾经把语言定义为：按照一定规律构成的句子和符号串的有限或无限的集合。我国学者吴蔚天则认为，可以将语言看成一个抽象的数学系统。语言无论是集合还是数学系统，都可以用数学的方法（规则）来刻画与描述。这个规则一般来说应该是语法，但是NLP中被定义为形式语言，是用来精确地描述语言及其结构的手段。\n\n形式语言的语法是一个四元组，G=（N，Σ，P，S）。N为非终结符（non-terminal symbol）的有限集合，Σ为终结符（terminal-symbol）的有限集合，N与Σ无交集，N并Σ就是称之为总词汇表。P是一组重写规则的有限集合：P = {a -&gt; b}，a，b均是词汇表中的，但是a中必须包含一个非终结符。S是初始符，也属于N。\n\n例如，有如下的形式语法：\n\n> G=（N，Σ，P，S）， N={S，B，N，A }\n>\n> Σ ={wikie，is，a，handsome，boy}  \n\n规则P如下：\n\n> S -> NBN， N -> AN，B -> BB\n>\n> B -> is | a，N ->wikie | boy，A -> handsome\n\n所以根据语法简单推导就是：\n\n> S -> NBN\n>\n> Wikie is boy或者Boy is wikie或者wikie a boy或者 boy a wikie  \n\n当然，最后的推导应该是这样：\n\n> S -> NBN -> NBBN -> NBBAN\n>\n> Wikie is a handsome boy.  \n\n觉得怎么样，以上语法是不是清晰明了？但这只是简单的情况，真实的情况下远比这复杂。\n\n* 基于经验（统计）主义方法\n\n以上形式语法虽然清晰明了，但是应对真正的千变万化的语言来说，总有无法形式表达的情况。而且很多排斥理性主义方法的人都有这样的一个想法：文盲从来没有学过语法，但TA不仅能够理解语言，而且说不定还能说得很溜很好。那么文盲为什么能听懂且说出话来呢？\n\n语言模型在自然语言处理中占有重要的地位，一个语言模型通常构建为字符串s的概率分布p（s），这里的p（s）试图反映的是字符串s作为一个句子出现的频率。\n\n例如，Yes这个词出现在我们的英语课本中的频率非常高，所以同学们和老外交流的时候，大部分时间在听，听完之后不管啥内容都会说一句：Yes，Yes！还有Oh这个语气词无论在电影还是书本中，都很多，同学们在回答了一个Oh的时候，突然会很习惯性的接上一个词Yeah，即：Oh, yeah yeah，高兴的话，还有可能是：Oh, yeah, yeah, you’re right！\n\n这个频率和习惯都可以反映成语言模型，即某个句子出现的概率，概率高出现的可能性就大，低则反之。对于一个句子s=w1w2w3w4…wl来说，其概率计算公式可以表示为：\n\n> p(s) = p(w1)p(w2|w1)p(w3|w1w2)…p(wl|w1…w(l-1)) // | 表示条件概率  \n\n需要注意的是，如果以上计算中，有一个值为0，p(s)则为0。显然，这个是不够准确的，所以需要进行平滑操作。顾名思义，线平滑即将折线平滑成曲线，语言模型即提高低概率（如0概率），降低高概率，尽量使概率分布趋于均匀。\n\n有了语言模型，文盲还不能够听与说，因为TA不知道出现的那句话对应的意思是什么。所以文盲还会在脑子存一个音与意的映射，在NLP中就是语料库了。听到一个词，就去查一下，听到一个词，就去查一下。\n\n顺带简单说一下机器学习，以前我怎么都不理解机器到底是怎么学习的，可能是我也仔细分析过我是怎么学习的。机器学习和人其实差不多，归纳法加演绎法。如有一段材料：鸟会飞，鹦鹉是鸟。计算机就能学会：鹦鹉会飞。不过这仅是我粗浅的理解，还有待继续深入。\n\n#### 2.2 NLP理论基础\n\n上面介绍了两个大方向，理论基础也就是大方向的理论基础：语言学、概率论、信息论。在这里就不一一说了，不过我还是得单独都搞本书看看咯。\n\n#### 2.3 NLP分析步骤\n\n觉着还是和编译原理差不多，不过多了一个分词的步骤，具体见下：\n\n* 分词\n\n分词是每个语言都要碰到的问题，很多地方说英语没有这个问题，因为有空格作为分隔符。但是我觉得英语也有词组，分词组应该也不是一件容易的事情。具体分词可以参加：[漫话中文自动分词和语义识别（上）：中文分词算法](http://www.hongweiyi.com/2012/04/nlp-repost-segmentation/)。\n\n* 语法分析\n\n分完词后，就需要进行语法分析了，即分析这句话是否通顺合理。语法分析又分为词法分析与句法分析。词法分析就是分析诸如名词性短语、动词性短语，句法分析就是分析诸如主谓宾、从句等结构，但这都是从理性主义方法出发，关于这个可以参加：[漫话中文自动分词和语义识别（下）：句法结构和语义结构](http://www.hongweiyi.com/2012/04/nlp-repost-semantic/)。目前更实用的，则是对大规模真实语料的概率统计分析与机器学习算法。简单理解就是，一个词一个句，出现的概率大就是正确的。\n\n* 语义分析\n\n到了我觉得最难的地方了，句子的语义分析。高中考试的时候，句子出现歧义了，我这个大活人有时候都会理解错误，能让计算机理解更是难上加难。再加上国人说话隐晦，一语双关。说个有趣的HSK（汉语水平考试）考试题目：\n\n张三找了个女朋友，李四问：“你女朋友长得怎么样？”张三答：“她人还不错。”问：张三的女朋友长得好么？\n\nA、长得好 B、长得不好 C、她人不错 D、不知道\n\n套用书本的话说就是：自然语言的语义计算问题十分困难，如何模拟人脑思维的过程，建立语言、知识与客观世界之间可计算的逻辑关系，并实现具有高区分能力的语义计算模型，至今任是个未能解决的难题。\n\n#### 2.4 NLP应用领域\n\n* 机器翻译类\n\n这个应该是常人能看到体会到的，从文曲星到金山词霸再到Google翻译，机器翻译伴随着我们80后一起成长。Google翻译更是论文翻译的必须物，尽管得到的结果有时候会惨不忍睹。如：今天天气好好啊 -> Today the weather good ah。多么直白的翻译。\n\n机器翻译的主要方法和NLP的方法差不多，不过过程有些不一样，有两种：基于中间语言与不基于中间语言。\n\n基于中间语言，就是相当于将所有语言互译成一种语言（如世界语），所有全世界N种语言翻译就只需要与同一种语言互译N次即可。但是是否能够构造出表示各种不同的自然语言语法、语义的中间语言，至少目前还是个未知数。此外，由于翻译都是误差，误差传递两次会有更大的误差，无法很好的生成对应的各种语言。\n\n不基于中间语言的又有：基于统计和基于实例的，统计就和前面提到的语言模型类似，基于实例的就是将不断累积的已经译好的文本作为机器翻译的样本，翻译的时候直接查看是否有类似的翻译。\n\n话说，以前老师说做同声传译一小时有5W，等到机器翻译完美解决那一刻，同声传译一小时多少捏？哇哈哈，很是邪恶啊！当然，我们的路还有很长。\n\n* 阅读理解类\n\n阅读理解无外乎就是读完文章，让我们说出文章的中心思想，文章内容以及文章类型。说出文章的中心思想就是自动文摘生成技术，说出文章内容的就是信息抽取技术，说出文章类型就是文本分类技术。不过书还没看到这里来，就不继续写了。\n\n* 问答类\n\nSiri横空出世，让业内业外人士都开了眼界，52nlp中有一篇关于siri的文章，见：[这里](http://www.52nlp.cn/sir)，不知道作者是褒还是贬。同时，移动10086在siri之前也有小机器人可以相互扯淡，这里也有一个获奖的开源自然语言的人工智能的聊天机器人：[A.L.I.C.E](http://www.alicebot.org/)，我一朋友还和AliceBot聊了好一会儿天。\n\n问答类我觉得是NLP与AI关系最为密切的一个环节，也是图灵测试直接使用的工具，现在一个叫[CleverBot](http://cleverbot.com/)的机器人号称通过了图灵测试，成功欺骗了800位观众。当然，现在问答类的系统并不是仅仅用于扯淡的，主要的还是类似与10086那样的客服系统，基于某个领域的问答，难度相较而言没那么高。\n\n### 3、小结\n\n以上很多仅仅是一些肤浅的理解，对NLP内部实现有些许了解，但是不深入。还需要继续阅读书籍以及相关文献，而且还得再恶补数学相关的知识。NLP刚入门，路还长着……\n\n\n> 参考资料：\n>\n> 1. [维基百科](http://en.wikipedia.org/wiki/Wiki)\n> 2. [52NLP](http://www.52nlp.cn/)\n> 3. 《统计自然语言处理》 宗成庆\n> 4. 《自然语言处理的原理及其应用》 杨宪泽\n> 5. 《自然语言处理》 江铭虎\n","slug":"2012/04/nlp-say-hi","published":1,"updated":"2015-12-31T09:53:29.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba938005i3x8fsucb768a"},{"title":"转 - 漫话中文自动分词和语义识别（下）：句法结构和语义结构","id":"499","date":"2012-04-01T09:54:48.000Z","_content":"\n这篇文章是[漫话中文分词算法](http://www.matrix67.com/blog/archives/4212)的续篇。在这里，我们将紧接着上一篇文章的内容继续探讨下去：如果计算机可以对一句话进行自动分词，它还能进一步整理句子的结构，甚至理解句子的意思吗？这两篇文章的关系十分紧密，因此，我把前一篇文章改名为了《漫话中文自动分词和语义识别（上）》，这篇文章自然就是它的下篇。我已经在很多不同的地方做过与这个话题有关的演讲了，在这里我想把它们写下来，和更多的人一同分享。\n\n<!--more-->\n\n什么叫做句法结构呢？让我们来看一些例子。“白天鹅在水中游”，这句话是有歧义的，它可能指的是“白天有一只鹅在水中游”，也可能指的是“有一只白天鹅在水中游”。不同的分词方案，产生了不同的意义。有没有什么句子，它的分词方案是唯一的，但也会产生不同的意思呢？有。比如“门没有锁”，它可能是指的“门没有被锁上”，也有可能是指的“门上根本就没有挂锁”。这个句子虽然只能切分成“门／没有／锁”，但由于“锁”这个词既有可能是动词，也有可能是名词，因而让整句话产生了不同的意思。有没有什么句子，它的分词方案是唯一的，并且每个词的词义也都不再变化，但整个句子仍然有歧义呢？有可能。看看这句话：“咬死了猎人的狗”。这句话有可能指的是“把猎人的狗咬死了”，也有可能指的是“一只咬死了猎人的狗”。这个歧义是怎么产生的呢？仔细体会两种不同的意思后，你会发现，句子中最底层的成分可以以不同的顺序组合起来，歧义由此产生。\n\n在前一篇文章中，我们看到了，利用概率转移的方法，我们可以有效地给一句话分词。事实上，利用相同的模型，我们也能给每一个词标注词性。更好的做法则是，我们直接把同一个词不同词性的用法当作是不同的词，从而把分词和词性标注的工作统一起来。但是，所有这样的工作都是对句子进行从左至右线性的分析，而句子结构实际上比这要复杂多了，它是这些词有顺序有层次地组合在一起的。计算机要想正确地解析一个句子，在分词和标注词性后，接下来该做的就是分析句法结构的层次。\n\n在计算机中，怎样描述一个句子的句法结构呢？ 1957 年， Noam Chomsky 出版了《句法结构》一书，把这种语言的层次化结构用形式化的方式清晰地描述了出来，这也就是所谓的“生成语法”模型。这本书是 20 世纪为数不多的几本真正的著作之一，文字非常简练，思路非常明晰，震撼了包括语言学、计算机理论在内的多个领域。记得 Quora 上曾经有人问 [Who are the best minds of the world today](http://www.quora.com/Best-Of/Who-are-the-best-minds-of-the-world-today-and-why) ，投出来的答案就是 Noam Chomsky 。\n\n随便取一句很长很复杂的话，比如“汽车被开车的师傅修好了”，我们总能至顶向下地一层层分析出它的结构。这个句子最顶层的结构就是“汽车修好了”。汽车怎么修好了呢？汽车被师傅修好了。汽车被什么样的师傅修好了呢？哦，汽车被开车的师傅修好了。当然，我们还可以无限地扩展下去，继续把句子中的每一个最底层的成分替换成更详细更复杂的描述，就好像小学语文中的扩句练习那样。这就是生成语法的核心思想。\n\n熟悉编译原理的朋友们可能知道“上下文无关文法”。其实，上面提到的扩展规则本质上就是一种上下文无关文法。例如，一个句子可以是“什么怎么样”的形式，我们就把这条规则记作\n\n> 句子 → 名词性短语＋动词性短语  \n\n其中，“名词性短语”指的是一个具有名词功能的成分，它有可能就是一个名词，也有可能还有它自己的内部结构。例如，它有可能是一个形容词性短语加上“的”再加上另一个名词性短语构成的，比如“便宜的汽车”；它还有可能是由“动词性短语＋的＋名词性短语”构成的，比如“抛锚了的汽车”；它甚至可能是由“名词性短语＋的＋名词性短语”构成的，比如“老师的汽车”。我们把名词性短语的生成规则也都记下来：\n\n> 名词性短语 → 名词\n>\n> 名词性短语 → 形容词性短语＋的＋名词性短语\n>\n> 名词性短语 → 动词性短语＋的＋名词性短语\n>\n> 名词性短语 → 名词性短语＋的＋名词性短语\n>\n>  ⋯⋯  \n\n类似地，动词性短语也有诸多具体的形式：\n\n> 动词性短语 → 动词\n>\n> 动词性短语 → 动词性短语＋了\n>\n> 动词性短语 → 介词短语＋动词性短语\n>\n> ⋯⋯  \n\n上面我们涉及到了介词短语，它也有自己的生成规则：\n\n> 介词短语 → 介词＋名词性短语\n>\n> ⋯⋯  \n\n我们构造句子的任务，也就是从“句子”这个初始结点出发，不断调用规则，产生越来越复杂的句型框架，然后从词库中选择相应词性的单词，填进这个框架里：\n\n![](http://www.matrix67.com/blogimage_2012/201201051.png)\n\n而分析句法结构的任务，则是已知一个句子从左到右各词的词性，要反过来求出一棵满足要求的“句法结构树”。这可以用 [Earley parser](http://en.wikipedia.org/wiki/Earley_parser) 来实现。\n\n这样看来，句法结构的问题似乎就已经完美的解决了。其实，我们还差得很远。生成语法有两个大问题。首先，句法结构正确的句子不见得都是好句子。 Chomsky 本人给出了一个经典的例子： Colorless green ideas sleep furiously 。形容词加形容词加名词加动词加副词，这是一个完全符合句法要求的序列，但随便拼凑会闹出很多笑话——什么叫做“无色的绿色的想法在狂暴地睡觉”？顺便插播个广告，如果你还挺喜欢这句话的意境的，欢迎去我以前做的 [IdeaGenerator](http://www.matrix67.com/ideagen/) 玩玩。不过，如果我们不涉及句子的生成，只关心句子的结构分析，这个缺陷对我们来说影响似乎并不大。生成语法的第二个问题就比较麻烦了：从同一个词性序列出发，可能会构建出不同的句法结构树。比较下面两个例子：\n\n> 老师 被 迟到 的 学生 逗乐 了\n>\n> 电话 被 窃听 的 房间 找到 了  \n\n它们都是“名词＋介词＋动词＋的＋名词＋动词＋了”，但它们的结构并不一样，前者是老师被逗乐了，“迟到”是修饰“学生”的，后者是房间找到了，“电话被窃听”是一起来修饰房间的。但是，纯粹运用前面的模型，我们无法区分出哪句话应该是哪个句法结构树。如何强化句法分析的模型和算法，让计算机构建出一棵正确的句法树，这成了一个大问题。\n\n让我们来看一个更简单的例子吧。同样是“动词＋形容词＋名词”，我们有两种构建句法结构树的方案：\n\n![](http://www.matrix67.com/blogimage_2012/201201052.png)\n\n未经过汉语语法训练的朋友可能会问，“点亮蜡烛”和“踢新皮球”的句法结构真的不同吗？我们能证明，这里面真的存在不同。我们造一个句子“踢破皮球”，你会发现对于这个句子来说，两种句法结构都是成立的，于是出现了歧义：把皮球踢破了（结构和“点亮蜡烛”一致），或者是，踢一个破的皮球（结构和“踢新皮球”一致）。\n\n但为什么“点亮蜡烛”只有一种理解方式呢？这是因为我们通常不会把“亮”字直接放在名词前做定语，我们一般不说“一根亮蜡烛”、“一颗亮星星”等等。为什么“踢新皮球”也只有一种理解方式呢？这是因为我们通常不会把“新”直接放在动词后面作补语，不会说“皮球踢新了”，“衣服洗新了”等等。但是“破”既能作定语又能作补语，于是“踢破皮球”就产生了两种不同的意思。如果我们把每个形容词能否作定语，能否作补语都记下来，然后在生成规则中添加限制条件，不就能完美解决这个问题了吗？\n\n基于规则的句法分析器就是这么做的。汉语语言学家们已经列出了所有词的各种特征：\n\n亮：词性 = 形容词，能作补语 = True ，能作定语 = False ⋯⋯\n\n新：词性 = 形容词，能作补语 = False ，能作定语 = True ⋯⋯\n\n⋯⋯\n\n当然，每个动词也有一大堆属性：\n\n> 点：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯\n>\n> 踢：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯\n>\n> 污染：词性 = 动词，能带宾语 = True ，能带补语 = False ⋯⋯\n>\n> 排队：词性 = 动词，能带宾语 = False ，能带补语 = False ⋯⋯\n>\n> ⋯⋯  \n\n名词也不例外：\n\n> 蜡烛：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯\n>\n> 皮球：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯\n>\n> ⋯⋯  \n\n有人估计会觉得奇怪了：“能作主语”也是一个属性，莫非有些名词不能做主语？哈哈，这样的名词不但有，而且还真不少：剧毒、看头、厉害、正轨、存亡⋯⋯这些词都不放在动词前面。难道有些名词不能做宾语吗？这样的词也有不少：享年、芳龄、心术、浑身、家丑⋯⋯这些词都不放在动词后面。这样说来，存在不受数量词修饰的词也就不奇怪了，事实上上面这些怪异的名词前面基本上都不能加数量词。\n\n另外一个至关重要的就是，这些性质可以“向上传递”。比方说，我们规定，套用规则\n\n> 名词性短语 → 形容词性短语＋名词性短语  \n\n后，整个名词性短语能否作主语、能否作宾语、能否受数量词修饰，这将取决于它的第二个构成成分。通俗地讲就是，如果“皮球”能够作主语，那么“新皮球”也能够作主语。有了“词语知识库”，又确保了这些知识能够在更高层次得到保留，我们就能给语法生成规则添加限制条件了。例如，我们可以规定，套用规则\n\n> 动词性短语 → 动词性短语＋名词性短语  \n\n的前提条件就是，那个动词性短语的“能带宾语”属性为 True ，并且那个名词性短语“能作宾语”的属性为 True 。另外，我们规定\n\n> 动词性短语 → 动词性短语＋形容词性短语  \n\n必须满足动词性短语的“能带补语”属性为 True ，并且形容词性短语“能作补语”属性为 True 。这样便阻止了“踢新皮球”中的“踢”和“新”先结合起来，因为“新”不能作补语。\n\n最后我们规定，套用规则\n\n> 名词性短语 → 形容词性短语＋名词性短语  \n\n时，形容词性短语必须要能作定语。这就避免了“点亮蜡烛”中的“亮”和“蜡烛”先组合起来，因为“亮”通常不作定语。这样，我们便解决了“动词＋形容词＋名词”的结构分析问题。\n\n当然，这只是一个很简单的例子。在[这里](http://www.matrix67.com/blog/archives/508)的问题 6 、 7 、 8 中你可以看到，一条语法生成规则往往有很多限制条件，这些限制条件不光是简单的“功能相符”和“前后一致”，有些复杂的限制条件甚至需要用 IF … THEN … 的方式来描述。你可以在[这里](http://www.matrix67.com/blog/archives/4858)看到，汉语中词与词之间还有各种怪异的区别特征，并且哪个词拥有哪些性质纯粹是知识库的问题，完全没有规律可循。一个实用的句法结构分析系统，往往拥有上百种属性标签。北京大学计算语言所编写了《现代汉语语法信息词典》，它里面包含了 579 种属性。我们的理想目标就是，找到汉语中每一种可能会影响句法结构的因素，并据此为词库里的每一个词打上标签；再列出汉语语法中的每一条生成规则，找到每一条生成规则的应用条件，以及应用这条规则之后，整个成分将会以怎样的方式继承哪些子成分的哪些属性，又会在什么样的情况下产生哪些新的属性。按照生成语言学的观点，计算机就应该能正确解析所有的汉语句子了。\n\n那么，这样一来，计算机是否就已经能从句子中获取到理解语义需要的所有信息了呢？答案是否定的。还有这么一些句子，它从分词到词义到结构都没有两可的情况，但整个句子仍然有歧义。考虑这句话“鸡不吃了”，它有两种意思：鸡不吃东西了，或者我们不吃鸡了。但是，这种歧义并不是由分词或者词义或者结构导致的，两种意思所对应的语法结构完全相同，都是“鸡”加上“不吃了”。但为什么歧义仍然产生了呢？这是因为，在句法结构内部，还有更深层次的语义结构，两者并不相同。\n\n汉语就是这么奇怪，位于主语位置上的事物既有可能是动作的发出者，也有可能是动作的承受者。“我吃完了”可以说，“苹果吃完了”也能讲。然而，“鸡”这个东西既能吃，也能被吃，歧义由此产生。\n\n位于宾语位置上的事物也不一定就是动作的承受者，“来客人了”、“住了一个人”都是属于宾语反而是动作发出者的情况。记得某次数理逻辑课上老师感叹，汉语的谓词非常不规范，明明是太阳在晒我，为什么要说成是“我晒太阳”呢？事实上，汉语的动宾搭配范围极其广泛，还有很多更怪异的例子：“写字”是我们真正在写的东西，“写书”是写的结果，“写毛笔”是写的工具，“写楷体”是写的方式，“写地上”是写的场所，“写一只狗”，等等，什么叫做“写一只狗”啊？我们能说“写一只狗”吗？当然可以，这是写的内容嘛，“同学们这周作文写什么啊”，“我写一只狗”。大家可以想像，学中文的老外看了这个会是什么表情。虽然通过句法分析，我们能够判断出句子中的每样东西都和哪个动词相关联，但从语义层面上看这个关联是什么，我们还需要新的模型。\n\n汉语语言学家把事物与动词的语义关系分为了 17 种，叫做 17 种“语义角色”，它们是施事、感事、当事、动力、受事、结果、系事、工具、材料、方式、内容、与事、对象、场所、目标、起点、时间。你可以看到，语义角色的划分非常详细。同样是动作的发出者，施事指的是真正意义上的发出动作，比如“他吃饭”中的“他”；感事则是指某种感知活动的经验者，比如“他知道这件事了”中的“他”；当事则是指性质状态的主体，比如“他病了”中的“他”；动力则是自然力量的发出者，比如“洪水淹没了村庄”中的“洪水”。语义角色的具体划分以及 17 这个数目是有争议的，不过不管怎样，这个模型本身能够非常贴切地回答“什么是语义”这个问题。\n\n汉语有一种“投射理论”，即一个句子的结构是由这个句子中的谓语投射出来的。给定一个动词后，这个动词能够带多少个语义角色，这几个语义角色都是什么，基本上都已经确定了。因而，完整的句子所应有的结构实际上也就已经确定了。比如，说到“休息”这个动词，你就会觉得它缺少一个施事，而且也不缺别的了。我们只会说“老王休息”，不会说“老王休息手”或者“老王休息沙发”。因而我们认为，“休息”只有一个“论元”。它的“论元结构”是：\n\n> 休息 <施事>\n\n因此，一旦在句子中看到“休息”这个词，我们就需要在句内或者句外寻找“休息”所需要的施事。这个过程有一个很帅的名字，叫做“配价”。“休息”就是一个典型的“一价动词”。我们平时接触的比较多的则是二价动词。不过，它们具体的论元有可能不一样：\n\n> 吃 <施事，受事>\n>\n> 去 <施事，目标>\n>\n> 淹没 <动力，受事>\n\n三价动词也是有的，例如\n\n> 送 <施事，受事，与事>\n\n甚至还有零价动词，例如\n\n> 下雨 <Ф>\n\n下面我们要教计算机做的，就是怎样给动词配价。之前，我们已经给出了解析句法结构的方法，这样计算机便能判断出每个动词究竟在和哪些词发生关系。语义分析的实质，就是确定出它们具体是什么关系。因此，语义识别的问题，也就转化为了“语义角色标注”的问题。然而，语义角色出现的位置并不固定，施事也能出现在动词后面，受事也能出现在动词前面，怎样让计算机识别语义角色呢？在回答这个问题之前，我们不妨问问自己：我们是怎么知道，“我吃完了”中的“我”是“吃”的施事，“苹果吃完了”中的“苹果”是“吃”的受事的呢？大家肯定会说，废话，“我”当然只能是“吃”的施事，因为我显然不会“被吃”；“苹果”当然只能是“吃”的受事，因为苹果显然不能发出“吃”动作。也就是说，“吃”的两个论元都有语义类的要求。我们把“吃”的论元结构写得更详细一些：\n\n> 吃 <施事[语义类：人|动物]，受事[语义类：食物|药物]>\n\n而“淹没”一词的论元结构则可以补充为：\n\n> 淹没 <动力[语义类：自然事物]，受事[语义类：建筑物|空间]>\n\n所以，为了完成计算机自动标注语义角色的任务，我们需要人肉建立两个庞大的数据库：语义类词典和论元结构词典。这样的人肉工程早就已经做过了。北京语言大学 1990 年 5 月启动的“九〇￼五语义工程”就是人工构建的一棵规模相当大的语义树。它把词语分成了事物、运动、时空、属性四大类，其中事物类分为事类和物类，物类又分为具体物和抽象物，具体物则再分为生物和非生物，生物之下则分了人类、动物、植物、微生物、生物构件五类，非生物之下则分了天然物、人工物、遗弃物、几何图形和非生物构件五类，其中人工物之下又包括设施物、运载物、器具物、原材料、耗散物、信息物、钱财七类。整棵语义树有 414 个结点，其中叶子结点 309 个，深度最大的地方达到了 9 层。论元结构方面则有清华大学和人民大学共同完成的《现代汉语述语动词机器词典》，词典中包括了各种动词的拼音、释义、分类、论元数、论元的语义角色、论元的语义限制等语法和语义信息。\n\n说到语义工程，不得不提到董振东先生的[知网](http://www.keenage.com/html/c_index.html)。这是一个综合了语义分类和语义关系的知识库，不但通过语义树反映了词与词的共性，还通过语义关系反映了每个词的个性。它不但能告诉你“医生”和“病人”都是人，还告诉了你“医生”可以对“病人”发出一个“医治”的动作。知网的理念和 WordNet 工程很相似，后者是 Princeton 在 1985 年就已经开始构建的英文单词语义关系词典，背后也是一个语义关系网的概念，词与词的关系涉及同义词、反义词、上下位词、整体与部分、子集与超集、材料与成品等等。如果你装了 Mathematica，你可以通过 WordData 函数获取到 WordNet 的数据。至于前面说的那几个中文知识库嘛，别问我，我也不知道上哪儿取去。\n\n看到这里，想必大家会欢呼，啊，这下子，在中文信息处理领域，从语法到语义都已经漂亮的解决了吧。其实并没有。上面的论元语义角色的模型有很多问题。其中一个很容易想到的就是隐喻的问题，比如“信息淹没了我”、“悲伤淹没了我”。一旦出现动词的新用法，我们只能更新论元结构：\n\n> 淹没 <动力[语义类：自然事物|抽象事物]，受事[语义类：建筑物|空间|人类]>\n\n但更麻烦的则是下面这些违背语义规则的情况。一个是否定句，比如“张三不可能吃思想”。一个是疑问句，比如“张三怎么可能吃思想”。更麻烦的就是超常现象。随便在新闻网站上一搜，你就会发现各种不符合语义规则的情形。我搜了一个“吃金属”，立即看到某新闻标题《法国一位老人以吃金属为生》。要想解决这些问题，需要给配价模型打上不少补丁。\n\n然而，配价模型也仅仅解决了动词的语义问题。其他词呢？好在，我们也可以为名词发展一套类似的配价理论。我们通常认为“教师”是一个零价名词，而“老师”则是一个一价名词，因为说到“老师”时，我们通常会说“谁的老师”。“态度”则是一个二价的名词，因为我们通常要说“谁对谁的态度”才算完整。事实上，形容词也有配价，“优秀”就是一个一价形容词，“友好”则是一个二价形容词，原因也是类似的。配价理论还有很多更复杂的内容，这里我们就不再详说了。\n\n但还有很多配价理论完全无法解决的问题。比如，语义有指向的问题。“砍光了”、“砍累了”、“砍钝了”、“砍快了”，都是动词后面跟形容词作补语，但实际意义各不相同。“砍光了”指的是“树砍光了”，“砍累了”指的是“人砍累了”，“砍钝了”指的是“斧子砍钝了”，“砍快了”指的是“砍砍快了”。看来，一个动词的每个论元不但有语义类的限制，还有“评价方式”的限制。\n\n两个动词连用，也有语义关系的问题。“抓住不放”中，“抓住”和“不放”这两个动作构成一种反复的关系，抓住就等于不放。“说起来气人”中，“说起来”和“气人”这两个动作构成了一种条件关系，即每次发生了“说起来”这个事件后，都会产生“气人”这个结果。大家或许又会说，这两种情况真的有区别吗？是的，而且我能证明这一点。让我们造一个句子“留着没用”，你会发现它出现了歧义：既可以像“抓住不放”一样理解为反复关系，一直把它留着一直没有使用；又可以像“说起来气人”一样理解为条件关系，留着的话是不会有用的。因此，动词与动词连用确实会产生不同的语义关系，这需要另一套模型来处理。\n\n虚词的语义更麻烦。别以为“了”就是表示完成，“这本书看了三天”表示这本书看完了，“这本书看了三天了”反而表示这本书没看完。“了”到底有多少个义项，现在也没有一个定论。副词也算虚词，副词的语义同样捉摸不定。比较“张三和李四结婚了”与“张三和李四都结婚了”，你会发现描述“都”字的语义没那么简单。\n\n不过，在实际的产品应用中，前面所说的这些问题都不大。这篇文章中讲到的基本上都是基于规则的语言学处理方法。目前更实用的，则是对大规模真实语料的概率统计分析与机器学习算法，这条路子可以无视很多具体的语言学问题，并且效果也相当理想。最大熵模型和条件随机场都是目前非常常用的自然语言处理手段，感兴趣的朋友可以深入研究一下。但是，这些方法也有它们自己的缺点，就是它们的不可预测性。不管哪条路，似乎都离目标还有很远的一段距离。期待在未来的某一日，自然语言处理领域会迎来一套全新的语言模型，一举解决前面提到的所有难题。\n\n> **转载：**\n>\n> [Matrix67](http://www.matrix67.com/blog/archives/4870)\n","source":"_posts/2012/04/nlp-repost-semantic.md","raw":"title: 转 - 漫话中文自动分词和语义识别（下）：句法结构和语义结构\ntags:\n  - 算法\n  - 自然语言处理\n  - 语义\nid: 499\ncategories:\n  - 技术分享\ndate: 2012-04-01 17:54:48\n---\n\n这篇文章是[漫话中文分词算法](http://www.matrix67.com/blog/archives/4212)的续篇。在这里，我们将紧接着上一篇文章的内容继续探讨下去：如果计算机可以对一句话进行自动分词，它还能进一步整理句子的结构，甚至理解句子的意思吗？这两篇文章的关系十分紧密，因此，我把前一篇文章改名为了《漫话中文自动分词和语义识别（上）》，这篇文章自然就是它的下篇。我已经在很多不同的地方做过与这个话题有关的演讲了，在这里我想把它们写下来，和更多的人一同分享。\n\n<!--more-->\n\n什么叫做句法结构呢？让我们来看一些例子。“白天鹅在水中游”，这句话是有歧义的，它可能指的是“白天有一只鹅在水中游”，也可能指的是“有一只白天鹅在水中游”。不同的分词方案，产生了不同的意义。有没有什么句子，它的分词方案是唯一的，但也会产生不同的意思呢？有。比如“门没有锁”，它可能是指的“门没有被锁上”，也有可能是指的“门上根本就没有挂锁”。这个句子虽然只能切分成“门／没有／锁”，但由于“锁”这个词既有可能是动词，也有可能是名词，因而让整句话产生了不同的意思。有没有什么句子，它的分词方案是唯一的，并且每个词的词义也都不再变化，但整个句子仍然有歧义呢？有可能。看看这句话：“咬死了猎人的狗”。这句话有可能指的是“把猎人的狗咬死了”，也有可能指的是“一只咬死了猎人的狗”。这个歧义是怎么产生的呢？仔细体会两种不同的意思后，你会发现，句子中最底层的成分可以以不同的顺序组合起来，歧义由此产生。\n\n在前一篇文章中，我们看到了，利用概率转移的方法，我们可以有效地给一句话分词。事实上，利用相同的模型，我们也能给每一个词标注词性。更好的做法则是，我们直接把同一个词不同词性的用法当作是不同的词，从而把分词和词性标注的工作统一起来。但是，所有这样的工作都是对句子进行从左至右线性的分析，而句子结构实际上比这要复杂多了，它是这些词有顺序有层次地组合在一起的。计算机要想正确地解析一个句子，在分词和标注词性后，接下来该做的就是分析句法结构的层次。\n\n在计算机中，怎样描述一个句子的句法结构呢？ 1957 年， Noam Chomsky 出版了《句法结构》一书，把这种语言的层次化结构用形式化的方式清晰地描述了出来，这也就是所谓的“生成语法”模型。这本书是 20 世纪为数不多的几本真正的著作之一，文字非常简练，思路非常明晰，震撼了包括语言学、计算机理论在内的多个领域。记得 Quora 上曾经有人问 [Who are the best minds of the world today](http://www.quora.com/Best-Of/Who-are-the-best-minds-of-the-world-today-and-why) ，投出来的答案就是 Noam Chomsky 。\n\n随便取一句很长很复杂的话，比如“汽车被开车的师傅修好了”，我们总能至顶向下地一层层分析出它的结构。这个句子最顶层的结构就是“汽车修好了”。汽车怎么修好了呢？汽车被师傅修好了。汽车被什么样的师傅修好了呢？哦，汽车被开车的师傅修好了。当然，我们还可以无限地扩展下去，继续把句子中的每一个最底层的成分替换成更详细更复杂的描述，就好像小学语文中的扩句练习那样。这就是生成语法的核心思想。\n\n熟悉编译原理的朋友们可能知道“上下文无关文法”。其实，上面提到的扩展规则本质上就是一种上下文无关文法。例如，一个句子可以是“什么怎么样”的形式，我们就把这条规则记作\n\n> 句子 → 名词性短语＋动词性短语  \n\n其中，“名词性短语”指的是一个具有名词功能的成分，它有可能就是一个名词，也有可能还有它自己的内部结构。例如，它有可能是一个形容词性短语加上“的”再加上另一个名词性短语构成的，比如“便宜的汽车”；它还有可能是由“动词性短语＋的＋名词性短语”构成的，比如“抛锚了的汽车”；它甚至可能是由“名词性短语＋的＋名词性短语”构成的，比如“老师的汽车”。我们把名词性短语的生成规则也都记下来：\n\n> 名词性短语 → 名词\n>\n> 名词性短语 → 形容词性短语＋的＋名词性短语\n>\n> 名词性短语 → 动词性短语＋的＋名词性短语\n>\n> 名词性短语 → 名词性短语＋的＋名词性短语\n>\n>  ⋯⋯  \n\n类似地，动词性短语也有诸多具体的形式：\n\n> 动词性短语 → 动词\n>\n> 动词性短语 → 动词性短语＋了\n>\n> 动词性短语 → 介词短语＋动词性短语\n>\n> ⋯⋯  \n\n上面我们涉及到了介词短语，它也有自己的生成规则：\n\n> 介词短语 → 介词＋名词性短语\n>\n> ⋯⋯  \n\n我们构造句子的任务，也就是从“句子”这个初始结点出发，不断调用规则，产生越来越复杂的句型框架，然后从词库中选择相应词性的单词，填进这个框架里：\n\n![](http://www.matrix67.com/blogimage_2012/201201051.png)\n\n而分析句法结构的任务，则是已知一个句子从左到右各词的词性，要反过来求出一棵满足要求的“句法结构树”。这可以用 [Earley parser](http://en.wikipedia.org/wiki/Earley_parser) 来实现。\n\n这样看来，句法结构的问题似乎就已经完美的解决了。其实，我们还差得很远。生成语法有两个大问题。首先，句法结构正确的句子不见得都是好句子。 Chomsky 本人给出了一个经典的例子： Colorless green ideas sleep furiously 。形容词加形容词加名词加动词加副词，这是一个完全符合句法要求的序列，但随便拼凑会闹出很多笑话——什么叫做“无色的绿色的想法在狂暴地睡觉”？顺便插播个广告，如果你还挺喜欢这句话的意境的，欢迎去我以前做的 [IdeaGenerator](http://www.matrix67.com/ideagen/) 玩玩。不过，如果我们不涉及句子的生成，只关心句子的结构分析，这个缺陷对我们来说影响似乎并不大。生成语法的第二个问题就比较麻烦了：从同一个词性序列出发，可能会构建出不同的句法结构树。比较下面两个例子：\n\n> 老师 被 迟到 的 学生 逗乐 了\n>\n> 电话 被 窃听 的 房间 找到 了  \n\n它们都是“名词＋介词＋动词＋的＋名词＋动词＋了”，但它们的结构并不一样，前者是老师被逗乐了，“迟到”是修饰“学生”的，后者是房间找到了，“电话被窃听”是一起来修饰房间的。但是，纯粹运用前面的模型，我们无法区分出哪句话应该是哪个句法结构树。如何强化句法分析的模型和算法，让计算机构建出一棵正确的句法树，这成了一个大问题。\n\n让我们来看一个更简单的例子吧。同样是“动词＋形容词＋名词”，我们有两种构建句法结构树的方案：\n\n![](http://www.matrix67.com/blogimage_2012/201201052.png)\n\n未经过汉语语法训练的朋友可能会问，“点亮蜡烛”和“踢新皮球”的句法结构真的不同吗？我们能证明，这里面真的存在不同。我们造一个句子“踢破皮球”，你会发现对于这个句子来说，两种句法结构都是成立的，于是出现了歧义：把皮球踢破了（结构和“点亮蜡烛”一致），或者是，踢一个破的皮球（结构和“踢新皮球”一致）。\n\n但为什么“点亮蜡烛”只有一种理解方式呢？这是因为我们通常不会把“亮”字直接放在名词前做定语，我们一般不说“一根亮蜡烛”、“一颗亮星星”等等。为什么“踢新皮球”也只有一种理解方式呢？这是因为我们通常不会把“新”直接放在动词后面作补语，不会说“皮球踢新了”，“衣服洗新了”等等。但是“破”既能作定语又能作补语，于是“踢破皮球”就产生了两种不同的意思。如果我们把每个形容词能否作定语，能否作补语都记下来，然后在生成规则中添加限制条件，不就能完美解决这个问题了吗？\n\n基于规则的句法分析器就是这么做的。汉语语言学家们已经列出了所有词的各种特征：\n\n亮：词性 = 形容词，能作补语 = True ，能作定语 = False ⋯⋯\n\n新：词性 = 形容词，能作补语 = False ，能作定语 = True ⋯⋯\n\n⋯⋯\n\n当然，每个动词也有一大堆属性：\n\n> 点：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯\n>\n> 踢：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯\n>\n> 污染：词性 = 动词，能带宾语 = True ，能带补语 = False ⋯⋯\n>\n> 排队：词性 = 动词，能带宾语 = False ，能带补语 = False ⋯⋯\n>\n> ⋯⋯  \n\n名词也不例外：\n\n> 蜡烛：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯\n>\n> 皮球：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯\n>\n> ⋯⋯  \n\n有人估计会觉得奇怪了：“能作主语”也是一个属性，莫非有些名词不能做主语？哈哈，这样的名词不但有，而且还真不少：剧毒、看头、厉害、正轨、存亡⋯⋯这些词都不放在动词前面。难道有些名词不能做宾语吗？这样的词也有不少：享年、芳龄、心术、浑身、家丑⋯⋯这些词都不放在动词后面。这样说来，存在不受数量词修饰的词也就不奇怪了，事实上上面这些怪异的名词前面基本上都不能加数量词。\n\n另外一个至关重要的就是，这些性质可以“向上传递”。比方说，我们规定，套用规则\n\n> 名词性短语 → 形容词性短语＋名词性短语  \n\n后，整个名词性短语能否作主语、能否作宾语、能否受数量词修饰，这将取决于它的第二个构成成分。通俗地讲就是，如果“皮球”能够作主语，那么“新皮球”也能够作主语。有了“词语知识库”，又确保了这些知识能够在更高层次得到保留，我们就能给语法生成规则添加限制条件了。例如，我们可以规定，套用规则\n\n> 动词性短语 → 动词性短语＋名词性短语  \n\n的前提条件就是，那个动词性短语的“能带宾语”属性为 True ，并且那个名词性短语“能作宾语”的属性为 True 。另外，我们规定\n\n> 动词性短语 → 动词性短语＋形容词性短语  \n\n必须满足动词性短语的“能带补语”属性为 True ，并且形容词性短语“能作补语”属性为 True 。这样便阻止了“踢新皮球”中的“踢”和“新”先结合起来，因为“新”不能作补语。\n\n最后我们规定，套用规则\n\n> 名词性短语 → 形容词性短语＋名词性短语  \n\n时，形容词性短语必须要能作定语。这就避免了“点亮蜡烛”中的“亮”和“蜡烛”先组合起来，因为“亮”通常不作定语。这样，我们便解决了“动词＋形容词＋名词”的结构分析问题。\n\n当然，这只是一个很简单的例子。在[这里](http://www.matrix67.com/blog/archives/508)的问题 6 、 7 、 8 中你可以看到，一条语法生成规则往往有很多限制条件，这些限制条件不光是简单的“功能相符”和“前后一致”，有些复杂的限制条件甚至需要用 IF … THEN … 的方式来描述。你可以在[这里](http://www.matrix67.com/blog/archives/4858)看到，汉语中词与词之间还有各种怪异的区别特征，并且哪个词拥有哪些性质纯粹是知识库的问题，完全没有规律可循。一个实用的句法结构分析系统，往往拥有上百种属性标签。北京大学计算语言所编写了《现代汉语语法信息词典》，它里面包含了 579 种属性。我们的理想目标就是，找到汉语中每一种可能会影响句法结构的因素，并据此为词库里的每一个词打上标签；再列出汉语语法中的每一条生成规则，找到每一条生成规则的应用条件，以及应用这条规则之后，整个成分将会以怎样的方式继承哪些子成分的哪些属性，又会在什么样的情况下产生哪些新的属性。按照生成语言学的观点，计算机就应该能正确解析所有的汉语句子了。\n\n那么，这样一来，计算机是否就已经能从句子中获取到理解语义需要的所有信息了呢？答案是否定的。还有这么一些句子，它从分词到词义到结构都没有两可的情况，但整个句子仍然有歧义。考虑这句话“鸡不吃了”，它有两种意思：鸡不吃东西了，或者我们不吃鸡了。但是，这种歧义并不是由分词或者词义或者结构导致的，两种意思所对应的语法结构完全相同，都是“鸡”加上“不吃了”。但为什么歧义仍然产生了呢？这是因为，在句法结构内部，还有更深层次的语义结构，两者并不相同。\n\n汉语就是这么奇怪，位于主语位置上的事物既有可能是动作的发出者，也有可能是动作的承受者。“我吃完了”可以说，“苹果吃完了”也能讲。然而，“鸡”这个东西既能吃，也能被吃，歧义由此产生。\n\n位于宾语位置上的事物也不一定就是动作的承受者，“来客人了”、“住了一个人”都是属于宾语反而是动作发出者的情况。记得某次数理逻辑课上老师感叹，汉语的谓词非常不规范，明明是太阳在晒我，为什么要说成是“我晒太阳”呢？事实上，汉语的动宾搭配范围极其广泛，还有很多更怪异的例子：“写字”是我们真正在写的东西，“写书”是写的结果，“写毛笔”是写的工具，“写楷体”是写的方式，“写地上”是写的场所，“写一只狗”，等等，什么叫做“写一只狗”啊？我们能说“写一只狗”吗？当然可以，这是写的内容嘛，“同学们这周作文写什么啊”，“我写一只狗”。大家可以想像，学中文的老外看了这个会是什么表情。虽然通过句法分析，我们能够判断出句子中的每样东西都和哪个动词相关联，但从语义层面上看这个关联是什么，我们还需要新的模型。\n\n汉语语言学家把事物与动词的语义关系分为了 17 种，叫做 17 种“语义角色”，它们是施事、感事、当事、动力、受事、结果、系事、工具、材料、方式、内容、与事、对象、场所、目标、起点、时间。你可以看到，语义角色的划分非常详细。同样是动作的发出者，施事指的是真正意义上的发出动作，比如“他吃饭”中的“他”；感事则是指某种感知活动的经验者，比如“他知道这件事了”中的“他”；当事则是指性质状态的主体，比如“他病了”中的“他”；动力则是自然力量的发出者，比如“洪水淹没了村庄”中的“洪水”。语义角色的具体划分以及 17 这个数目是有争议的，不过不管怎样，这个模型本身能够非常贴切地回答“什么是语义”这个问题。\n\n汉语有一种“投射理论”，即一个句子的结构是由这个句子中的谓语投射出来的。给定一个动词后，这个动词能够带多少个语义角色，这几个语义角色都是什么，基本上都已经确定了。因而，完整的句子所应有的结构实际上也就已经确定了。比如，说到“休息”这个动词，你就会觉得它缺少一个施事，而且也不缺别的了。我们只会说“老王休息”，不会说“老王休息手”或者“老王休息沙发”。因而我们认为，“休息”只有一个“论元”。它的“论元结构”是：\n\n> 休息 <施事>\n\n因此，一旦在句子中看到“休息”这个词，我们就需要在句内或者句外寻找“休息”所需要的施事。这个过程有一个很帅的名字，叫做“配价”。“休息”就是一个典型的“一价动词”。我们平时接触的比较多的则是二价动词。不过，它们具体的论元有可能不一样：\n\n> 吃 <施事，受事>\n>\n> 去 <施事，目标>\n>\n> 淹没 <动力，受事>\n\n三价动词也是有的，例如\n\n> 送 <施事，受事，与事>\n\n甚至还有零价动词，例如\n\n> 下雨 <Ф>\n\n下面我们要教计算机做的，就是怎样给动词配价。之前，我们已经给出了解析句法结构的方法，这样计算机便能判断出每个动词究竟在和哪些词发生关系。语义分析的实质，就是确定出它们具体是什么关系。因此，语义识别的问题，也就转化为了“语义角色标注”的问题。然而，语义角色出现的位置并不固定，施事也能出现在动词后面，受事也能出现在动词前面，怎样让计算机识别语义角色呢？在回答这个问题之前，我们不妨问问自己：我们是怎么知道，“我吃完了”中的“我”是“吃”的施事，“苹果吃完了”中的“苹果”是“吃”的受事的呢？大家肯定会说，废话，“我”当然只能是“吃”的施事，因为我显然不会“被吃”；“苹果”当然只能是“吃”的受事，因为苹果显然不能发出“吃”动作。也就是说，“吃”的两个论元都有语义类的要求。我们把“吃”的论元结构写得更详细一些：\n\n> 吃 <施事[语义类：人|动物]，受事[语义类：食物|药物]>\n\n而“淹没”一词的论元结构则可以补充为：\n\n> 淹没 <动力[语义类：自然事物]，受事[语义类：建筑物|空间]>\n\n所以，为了完成计算机自动标注语义角色的任务，我们需要人肉建立两个庞大的数据库：语义类词典和论元结构词典。这样的人肉工程早就已经做过了。北京语言大学 1990 年 5 月启动的“九〇￼五语义工程”就是人工构建的一棵规模相当大的语义树。它把词语分成了事物、运动、时空、属性四大类，其中事物类分为事类和物类，物类又分为具体物和抽象物，具体物则再分为生物和非生物，生物之下则分了人类、动物、植物、微生物、生物构件五类，非生物之下则分了天然物、人工物、遗弃物、几何图形和非生物构件五类，其中人工物之下又包括设施物、运载物、器具物、原材料、耗散物、信息物、钱财七类。整棵语义树有 414 个结点，其中叶子结点 309 个，深度最大的地方达到了 9 层。论元结构方面则有清华大学和人民大学共同完成的《现代汉语述语动词机器词典》，词典中包括了各种动词的拼音、释义、分类、论元数、论元的语义角色、论元的语义限制等语法和语义信息。\n\n说到语义工程，不得不提到董振东先生的[知网](http://www.keenage.com/html/c_index.html)。这是一个综合了语义分类和语义关系的知识库，不但通过语义树反映了词与词的共性，还通过语义关系反映了每个词的个性。它不但能告诉你“医生”和“病人”都是人，还告诉了你“医生”可以对“病人”发出一个“医治”的动作。知网的理念和 WordNet 工程很相似，后者是 Princeton 在 1985 年就已经开始构建的英文单词语义关系词典，背后也是一个语义关系网的概念，词与词的关系涉及同义词、反义词、上下位词、整体与部分、子集与超集、材料与成品等等。如果你装了 Mathematica，你可以通过 WordData 函数获取到 WordNet 的数据。至于前面说的那几个中文知识库嘛，别问我，我也不知道上哪儿取去。\n\n看到这里，想必大家会欢呼，啊，这下子，在中文信息处理领域，从语法到语义都已经漂亮的解决了吧。其实并没有。上面的论元语义角色的模型有很多问题。其中一个很容易想到的就是隐喻的问题，比如“信息淹没了我”、“悲伤淹没了我”。一旦出现动词的新用法，我们只能更新论元结构：\n\n> 淹没 <动力[语义类：自然事物|抽象事物]，受事[语义类：建筑物|空间|人类]>\n\n但更麻烦的则是下面这些违背语义规则的情况。一个是否定句，比如“张三不可能吃思想”。一个是疑问句，比如“张三怎么可能吃思想”。更麻烦的就是超常现象。随便在新闻网站上一搜，你就会发现各种不符合语义规则的情形。我搜了一个“吃金属”，立即看到某新闻标题《法国一位老人以吃金属为生》。要想解决这些问题，需要给配价模型打上不少补丁。\n\n然而，配价模型也仅仅解决了动词的语义问题。其他词呢？好在，我们也可以为名词发展一套类似的配价理论。我们通常认为“教师”是一个零价名词，而“老师”则是一个一价名词，因为说到“老师”时，我们通常会说“谁的老师”。“态度”则是一个二价的名词，因为我们通常要说“谁对谁的态度”才算完整。事实上，形容词也有配价，“优秀”就是一个一价形容词，“友好”则是一个二价形容词，原因也是类似的。配价理论还有很多更复杂的内容，这里我们就不再详说了。\n\n但还有很多配价理论完全无法解决的问题。比如，语义有指向的问题。“砍光了”、“砍累了”、“砍钝了”、“砍快了”，都是动词后面跟形容词作补语，但实际意义各不相同。“砍光了”指的是“树砍光了”，“砍累了”指的是“人砍累了”，“砍钝了”指的是“斧子砍钝了”，“砍快了”指的是“砍砍快了”。看来，一个动词的每个论元不但有语义类的限制，还有“评价方式”的限制。\n\n两个动词连用，也有语义关系的问题。“抓住不放”中，“抓住”和“不放”这两个动作构成一种反复的关系，抓住就等于不放。“说起来气人”中，“说起来”和“气人”这两个动作构成了一种条件关系，即每次发生了“说起来”这个事件后，都会产生“气人”这个结果。大家或许又会说，这两种情况真的有区别吗？是的，而且我能证明这一点。让我们造一个句子“留着没用”，你会发现它出现了歧义：既可以像“抓住不放”一样理解为反复关系，一直把它留着一直没有使用；又可以像“说起来气人”一样理解为条件关系，留着的话是不会有用的。因此，动词与动词连用确实会产生不同的语义关系，这需要另一套模型来处理。\n\n虚词的语义更麻烦。别以为“了”就是表示完成，“这本书看了三天”表示这本书看完了，“这本书看了三天了”反而表示这本书没看完。“了”到底有多少个义项，现在也没有一个定论。副词也算虚词，副词的语义同样捉摸不定。比较“张三和李四结婚了”与“张三和李四都结婚了”，你会发现描述“都”字的语义没那么简单。\n\n不过，在实际的产品应用中，前面所说的这些问题都不大。这篇文章中讲到的基本上都是基于规则的语言学处理方法。目前更实用的，则是对大规模真实语料的概率统计分析与机器学习算法，这条路子可以无视很多具体的语言学问题，并且效果也相当理想。最大熵模型和条件随机场都是目前非常常用的自然语言处理手段，感兴趣的朋友可以深入研究一下。但是，这些方法也有它们自己的缺点，就是它们的不可预测性。不管哪条路，似乎都离目标还有很远的一段距离。期待在未来的某一日，自然语言处理领域会迎来一套全新的语言模型，一举解决前面提到的所有难题。\n\n> **转载：**\n>\n> [Matrix67](http://www.matrix67.com/blog/archives/4870)\n","slug":"2012/04/nlp-repost-semantic","published":1,"updated":"2015-12-31T09:49:19.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93a005m3x8f7rgkta39"},{"title":"转 – 漫话中文自动分词和语义识别（上）：中文分词算法","id":"497","date":"2012-04-01T09:48:00.000Z","_content":"\nM牛的这篇博文实在是太精彩了，读了一遍又一遍，最后干脆就直接转过来了。有兴趣的可以直接看他的博客：[matrix67](http://www.matrix67.com/blog/archives/4212)\n\n<!--more-->\n\n记得第一次了解中文分词算法是在 [Google 黑板报](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_7327.html) 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。\n\n中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。\n\n有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。\n\n最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。\n\n维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。\n\n还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。\n\n不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。\n\n当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：\n\n对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。\n\n这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：\n\n> 他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）\n> 他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）\n> 他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）\n\n正确答案胜出。\n\n需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。\n\n算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。\n\n何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。\n\n以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10<sup>-9</sup> ，但“有意／见／分歧”的得分只有 1.0×10<sup>-11</sup> ，正确方案完胜。\n\n这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：\n\n> 这／事／的确／定／不／下来  \n\n但是概率算法却会把这个句子分成：\n\n> 这／事／的／确定／不／下来  \n\n原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。\n\n其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。\n\n于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w<sub>1</sub> 、 w<sub>2</sub> ，统计在语料库中词语 w<sub>1</sub> 后面恰好是 w<sub>2</sub> 的概率 P(w<sub>1</sub>, w<sub>2</sub>) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w<sub>1</sub>) · P(w<sub>1</sub>, w<sub>2</sub>) · … · P(w<sub>n-1</sub>, w<sub>n</sub>) ，其中 w<sub>1</sub>, w<sub>2</sub>, …, w<sub>n</sub> 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。\n\n至此，中文自动分词算是有了一个漂亮而实用的算法。\n\n但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。\n\n在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。\n\n可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。\n\n但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。\n\n还有那些恰好与上下文组合成词的人名，例如：\n\n> 费孝通向人大常委会提交书面报告     \n> 邓颖超生前使用过的物品  \n\n这就是最考验分词算法的句子了。\n\n相比之下，中国地名的用字就分散得多了。北京有一个地方叫“臭泥坑”，网上搜索“臭泥坑”，第一页全是“臭泥坑地图”、“臭泥坑附近酒店”之类的信息。某年《重庆晨报》刊登停电通知，上面赫然印着“停电范围包括沙坪坝区的犀牛屙屎和犀牛屙屎抽水”，读者纷纷去电投诉印刷错误。记者仔细一查，你猜怎么着，印刷并无错误，重庆真的就有叫“犀牛屙屎”和“犀牛屙屎抽水”的地方。\n\n好在，中国地名数量有限，这是可以枚举的。中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。\n\n真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。\n\n最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。\n\n汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。\n\n说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。\n","source":"_posts/2012/04/nlp-repost-segmentation.md","raw":"title: 转 – 漫话中文自动分词和语义识别（上）：中文分词算法\ntags:\n  - 算法\n  - 自然语言处理\n  - 语义\nid: 497\ncategories:\n  - 技术分享\ndate: 2012-04-01 17:48:00\n---\n\nM牛的这篇博文实在是太精彩了，读了一遍又一遍，最后干脆就直接转过来了。有兴趣的可以直接看他的博客：[matrix67](http://www.matrix67.com/blog/archives/4212)\n\n<!--more-->\n\n记得第一次了解中文分词算法是在 [Google 黑板报](http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_7327.html) 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。\n\n中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。\n\n有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。\n\n最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。\n\n维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。\n\n还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。\n\n不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。\n\n当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：\n\n对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。\n\n这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：\n\n> 他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）\n> 他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）\n> 他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）\n\n正确答案胜出。\n\n需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。\n\n算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。\n\n何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。\n\n以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10<sup>-9</sup> ，但“有意／见／分歧”的得分只有 1.0×10<sup>-11</sup> ，正确方案完胜。\n\n这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：\n\n> 这／事／的确／定／不／下来  \n\n但是概率算法却会把这个句子分成：\n\n> 这／事／的／确定／不／下来  \n\n原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。\n\n其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。\n\n于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w<sub>1</sub> 、 w<sub>2</sub> ，统计在语料库中词语 w<sub>1</sub> 后面恰好是 w<sub>2</sub> 的概率 P(w<sub>1</sub>, w<sub>2</sub>) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w<sub>1</sub>) · P(w<sub>1</sub>, w<sub>2</sub>) · … · P(w<sub>n-1</sub>, w<sub>n</sub>) ，其中 w<sub>1</sub>, w<sub>2</sub>, …, w<sub>n</sub> 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。\n\n至此，中文自动分词算是有了一个漂亮而实用的算法。\n\n但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。\n\n在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。\n\n可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。\n\n但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。\n\n还有那些恰好与上下文组合成词的人名，例如：\n\n> 费孝通向人大常委会提交书面报告     \n> 邓颖超生前使用过的物品  \n\n这就是最考验分词算法的句子了。\n\n相比之下，中国地名的用字就分散得多了。北京有一个地方叫“臭泥坑”，网上搜索“臭泥坑”，第一页全是“臭泥坑地图”、“臭泥坑附近酒店”之类的信息。某年《重庆晨报》刊登停电通知，上面赫然印着“停电范围包括沙坪坝区的犀牛屙屎和犀牛屙屎抽水”，读者纷纷去电投诉印刷错误。记者仔细一查，你猜怎么着，印刷并无错误，重庆真的就有叫“犀牛屙屎”和“犀牛屙屎抽水”的地方。\n\n好在，中国地名数量有限，这是可以枚举的。中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。\n\n真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。\n\n最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。\n\n汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。\n\n说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。\n","slug":"2012/04/nlp-repost-segmentation","published":1,"updated":"2015-12-31T09:50:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93c005t3x8ff9z19p5v"},{"title":"Redis五大数据结构","id":"451","date":"2012-03-07T09:30:26.000Z","_content":"\n### 1、Redis介绍\n\nRedis是REmote DIctionary Server的缩写，作者定位于一个内存KV存储数据库（In-memory key-value Store），让Redis自豪的并不是那每秒10K的读写速度，而是它那可以应对很多情况的数据结构，我这里就简单的介绍一下它五大数据结构，也可以方便的让自个翻翻API，并给以后翻阅源码打下一个基础。\n\n<!--more-->\n\n### 2、Strings\n\n#### 1）简介\n\nString是Redis最基本的数据结构，它的String是二进制安全的，即String中可以存放任意的二进制数据，比如说JPG图片、序列化对象等。String值长度最大可到512mb。\n\n#### 2）结构定义\n\n``` c\nstruct sdshdr{  \n    long len;  \n    long free;  \n    char buf[];  \n}\n```\n\n#### 3）支持命令\n\n[APPEND](http://redis.io/commands/append)、[GET](http://redis.io/commands/get)、[GETBIT](http://redis.io/commands/getbit)、[GETRANGE](http://redis.io/commands/getrange)、[GETSET](http://redis.io/commands/getset)、[STRLEN](http://redis.io/commands/strlen)\n\n[MGET](http://redis.io/commands/mget)、[MSET](http://redis.io/commands/mset)、[MSETNX](http://redis.io/commands/msetnx)、[SET](http://redis.io/commands/set)、[SETBIT](http://redis.io/commands/setbit)、[SETEX](http://redis.io/commands/setex)、[SETNX](http://redis.io/commands/setnx)、[SETRANGE](http://redis.io/commands/setrange)\n\n[INCR](http://redis.io/commands/incr)、[INCRBY](http://redis.io/commands/incrby)、[DECR](http://redis.io/commands/decr)、[DECRBY](http://redis.io/commands/decrby)\n\n### 3、Hashes\n\n#### 1）简介\n\nHashes中存放了多个键值对（field/value），所以Hash结构可方便的表示一个对象。如：\n\n`HMSET user:00001 username wikie password  gender male`\n\n一个Hash可以存放2^32 – 1个键值对。Hash对象是用zipmap存储的，查找、删除均为O(n)，但一般来说对象的field对象不会大多，所以说操作评价还是近似O(1)。如果field/value的大小超过一定限制后，Redis会在内部自动将zipmap替换成正常的Hash实现，可在配置文件中指定：\n\nhash-max-zipmap-entries 64 # 字段最多64个\n\nhash-max-zipmap-value 512 # value最大为512字节\n\n#### 2）结构定义\n\n``` c\n//Please check in dict.h  \ntypedef struct dictht {  \n    dictEntry table;  \n    unsigned long size;  \n    unsigned long sizemask;  \n    unsigned long used;  \n} dictht;\n```\n\n#### 3）支持命令\n\n[HDEL](http://redis.io/commands/hdel)、[HEXISTS](http://redis.io/commands/hexists)、[HGET](http://redis.io/commands/hget)、[HGETALL](http://redis.io/commands/hgetall)、[HINCRBY](http://redis.io/commands/hincrby)、[HKEYS](http://redis.io/commands/hkeys)、[HLEN](http://redis.io/commands/hlen)\n\n[HMGET](http://redis.io/commands/hmget)、[HMSET](http://redis.io/commands/hmset)、[HSET](http://redis.io/commands/hset)、[HSETNX](http://redis.io/commands/hsetnx)、[HVALS](http://redis.io/commands/hvals)\n\n### 4、Lists\n\n#### 1）简介\n\nLists是一个简单的strings类型的双向链表，按照插入顺序排序。\n\n最大长度支持2^32-1，可以通过命令从头部或者尾部添加删除元素，即可很方便的实现栈与队列操作。List还可以阻塞，很容易就实现了一个工作队列，而不用轮询。\n\n#### 2）结构定义\n\n``` c\n// Check in adlist.h  \ntypedef struct listNode {  \n    struct listNode *prev;  \n    struct listNode *next;  \n    void *value;  \n} listNode;  \n\ntypedef struct listIter {  \n    listNode *next;  \n    int direction;  \n} listIter;  \n\ntypedef struct list {  \n    listNode *head;  \n    listNode *tail;  \n    void *(*dup)(void *ptr);  \n    void (*free)(void *ptr);  \n    int (*match)(void *ptr, void *key);  \n    unsigned int len;  \n} list;\n```\n\n#### 3）支持命令\n\n[BLPOP](http://redis.io/commands/blpop) 、[BRPOP](http://redis.io/commands/brpop) 、[BRPOPLPUSH](http://redis.io/commands/brpoplpush)、[LINDEX](http://redis.io/commands/lindex)、[LINSERT](http://redis.io/commands/linsert)、[LLEN](http://redis.io/commands/llen)\n\n[LPOP](http://redis.io/commands/lpop)、[LPUSH](http://redis.io/commands/lpush)、[LPUSHX](http://redis.io/commands/lpushx)、[LRANGE](http://redis.io/commands/lrange)、[LREM](http://redis.io/commands/lrem)、[LSET](http://redis.io/commands/lset)、[LTRIM](http://redis.io/commands/ltrim)\n\n[RPOP](http://redis.io/commands/rpop)、[RPOPLPUSH](http://redis.io/commands/rpoplpush)、[RPUSH](http://redis.io/commands/rpush)、[RPUSHX](http://redis.io/commands/rpushx)\n\n### 5、Sets\n\n#### 1）简介\n\n与数学的中的集合概念类似，没有重复的值，对其有添加删除操作，可对都个结合求交、并等操作，key理解为集合的名字。新浪微博中的：“我和她都关注了”只需要一个SINTER命令就可以实现。\n\nSets通过Hash Table实现，添加删除的时间复杂度均为O(n)，HashTable会随着添加或者删除自动调整大小。需要注意的是，调整HashTable大小需要同步（获取写锁）阻塞读写操作，后期可能会采用SkipList（无序如何使用SkipList？）实现。\n\n和其它类型一样，最大支持2^32-1个元素。\n\n#### 2）结构定义\n\n与Hashes中的dict一致。\n\n#### 3）支持的方法\n\n[SADD](http://redis.io/commands/sadd)、[SCAR](http://redis.io/commands/scard)、[SDIFF](http://redis.io/commands/sdiff)、[SDIFFSTORE](http://redis.io/commands/sdiffstore)、[SINTER](http://redis.io/commands/sinter)、[SISMEMBER](http://redis.io/commands/sismember)\n\n[SMEMBERS](http://redis.io/commands/smembers)、[SMOVE](http://redis.io/commands/smove)、[SPOP](http://redis.io/commands/spop)、[SRANDMEMBER](http://redis.io/commands/srandmember)、[SREM](http://redis.io/commands/srem)\n\n[SUNION](http://redis.io/commands/sunion)、[SUNIONSTORE](http://redis.io/commands/sunionstore)\n\n### 6、ZSets\n\n#### 1）简介\n\nZSets为Set的升级版本，即排序的Sets，在Set的基础之上增加了顺序（Score）属性，每次插入均需要指定，且会自动重新调整值的顺序。Score为double类型，ZSets实现为SkipList与HashTable的混合体。\n\n元素到Score的映射是添加在HashTable中的，所以给定一个元素获取Score开销为O(1)，Score到元素的映射则为SkipList。\n\n#### 2）结构定义\n\n``` c\n/* ZSETs use a specialized version of Skiplists */\ntypedef struct zskiplistNode {  \n    robj *obj;  \n    double score;  \n    struct zskiplistNode *backward;  \n    struct zskiplistLevel {  \n        struct zskiplistNode *forward;  \n        unsigned int span;  \n    } level[];  \n} zskiplistNode;  \n\ntypedef struct zskiplist {  \n    struct zskiplistNode *header, *tail;  \n    unsigned long length;  \n    int level;  \n} zskiplist;  \n\ntypedef struct zset {  \n    dict *dict;    // Value to Score  \n    zskiplist *zsl;  // Score to Value  \n} zset;\n```\n\n#### 3）支持命令\n\n[ZADD](http://redis.io/commands/zadd)、[ZCARD](http://redis.io/commands/zcard)、[ZCOUNT](http://redis.io/commands/zcount)、[ZINCRBY](http://redis.io/commands/zincrby)、[ZINTERSTORE](http://redis.io/commands/zinterstore)\n\n[ZRANGE](http://redis.io/commands/zrange)、[ZRANGEBYSCORE](http://redis.io/commands/zrangebyscore)、[ZRANK](http://redis.io/commands/zrank)、[ZREM](http://redis.io/commands/zrem)\n\n[ZREMRANGEBYRANK](http://redis.io/commands/zremrangebyrank)、[ZREMRANGEBYSCORE](http://redis.io/commands/zremrangebyscore)、[ZREVRANGE](http://redis.io/commands/zrevrange)\n\n[ZREVRANGEBYSCORE](http://redis.io/commands/zrevrangebyscore)、[ZREVRANK](http://redis.io/commands/zrevrank)、[ZSCORE](http://redis.io/commands/zscore)、[ZUNIONSTORE](http://redis.io/commands/zunionstore)\n\n> 参考资料：\n>\n> [Redis.io](http://redis.io)\n>\n> [The Little Redis Book](http://openmymind.net/2012/1/23/The-Little-Redis-Book/)\n","source":"_posts/2012/03/redis-data-strutrue.md","raw":"title: Redis五大数据结构\ntags:\n  - NoSQL\n  - Redis\n  - 数据结构\nid: 451\ncategories:\n  - 技术分享\ndate: 2012-03-07 17:30:26\n---\n\n### 1、Redis介绍\n\nRedis是REmote DIctionary Server的缩写，作者定位于一个内存KV存储数据库（In-memory key-value Store），让Redis自豪的并不是那每秒10K的读写速度，而是它那可以应对很多情况的数据结构，我这里就简单的介绍一下它五大数据结构，也可以方便的让自个翻翻API，并给以后翻阅源码打下一个基础。\n\n<!--more-->\n\n### 2、Strings\n\n#### 1）简介\n\nString是Redis最基本的数据结构，它的String是二进制安全的，即String中可以存放任意的二进制数据，比如说JPG图片、序列化对象等。String值长度最大可到512mb。\n\n#### 2）结构定义\n\n``` c\nstruct sdshdr{  \n    long len;  \n    long free;  \n    char buf[];  \n}\n```\n\n#### 3）支持命令\n\n[APPEND](http://redis.io/commands/append)、[GET](http://redis.io/commands/get)、[GETBIT](http://redis.io/commands/getbit)、[GETRANGE](http://redis.io/commands/getrange)、[GETSET](http://redis.io/commands/getset)、[STRLEN](http://redis.io/commands/strlen)\n\n[MGET](http://redis.io/commands/mget)、[MSET](http://redis.io/commands/mset)、[MSETNX](http://redis.io/commands/msetnx)、[SET](http://redis.io/commands/set)、[SETBIT](http://redis.io/commands/setbit)、[SETEX](http://redis.io/commands/setex)、[SETNX](http://redis.io/commands/setnx)、[SETRANGE](http://redis.io/commands/setrange)\n\n[INCR](http://redis.io/commands/incr)、[INCRBY](http://redis.io/commands/incrby)、[DECR](http://redis.io/commands/decr)、[DECRBY](http://redis.io/commands/decrby)\n\n### 3、Hashes\n\n#### 1）简介\n\nHashes中存放了多个键值对（field/value），所以Hash结构可方便的表示一个对象。如：\n\n`HMSET user:00001 username wikie password  gender male`\n\n一个Hash可以存放2^32 – 1个键值对。Hash对象是用zipmap存储的，查找、删除均为O(n)，但一般来说对象的field对象不会大多，所以说操作评价还是近似O(1)。如果field/value的大小超过一定限制后，Redis会在内部自动将zipmap替换成正常的Hash实现，可在配置文件中指定：\n\nhash-max-zipmap-entries 64 # 字段最多64个\n\nhash-max-zipmap-value 512 # value最大为512字节\n\n#### 2）结构定义\n\n``` c\n//Please check in dict.h  \ntypedef struct dictht {  \n    dictEntry table;  \n    unsigned long size;  \n    unsigned long sizemask;  \n    unsigned long used;  \n} dictht;\n```\n\n#### 3）支持命令\n\n[HDEL](http://redis.io/commands/hdel)、[HEXISTS](http://redis.io/commands/hexists)、[HGET](http://redis.io/commands/hget)、[HGETALL](http://redis.io/commands/hgetall)、[HINCRBY](http://redis.io/commands/hincrby)、[HKEYS](http://redis.io/commands/hkeys)、[HLEN](http://redis.io/commands/hlen)\n\n[HMGET](http://redis.io/commands/hmget)、[HMSET](http://redis.io/commands/hmset)、[HSET](http://redis.io/commands/hset)、[HSETNX](http://redis.io/commands/hsetnx)、[HVALS](http://redis.io/commands/hvals)\n\n### 4、Lists\n\n#### 1）简介\n\nLists是一个简单的strings类型的双向链表，按照插入顺序排序。\n\n最大长度支持2^32-1，可以通过命令从头部或者尾部添加删除元素，即可很方便的实现栈与队列操作。List还可以阻塞，很容易就实现了一个工作队列，而不用轮询。\n\n#### 2）结构定义\n\n``` c\n// Check in adlist.h  \ntypedef struct listNode {  \n    struct listNode *prev;  \n    struct listNode *next;  \n    void *value;  \n} listNode;  \n\ntypedef struct listIter {  \n    listNode *next;  \n    int direction;  \n} listIter;  \n\ntypedef struct list {  \n    listNode *head;  \n    listNode *tail;  \n    void *(*dup)(void *ptr);  \n    void (*free)(void *ptr);  \n    int (*match)(void *ptr, void *key);  \n    unsigned int len;  \n} list;\n```\n\n#### 3）支持命令\n\n[BLPOP](http://redis.io/commands/blpop) 、[BRPOP](http://redis.io/commands/brpop) 、[BRPOPLPUSH](http://redis.io/commands/brpoplpush)、[LINDEX](http://redis.io/commands/lindex)、[LINSERT](http://redis.io/commands/linsert)、[LLEN](http://redis.io/commands/llen)\n\n[LPOP](http://redis.io/commands/lpop)、[LPUSH](http://redis.io/commands/lpush)、[LPUSHX](http://redis.io/commands/lpushx)、[LRANGE](http://redis.io/commands/lrange)、[LREM](http://redis.io/commands/lrem)、[LSET](http://redis.io/commands/lset)、[LTRIM](http://redis.io/commands/ltrim)\n\n[RPOP](http://redis.io/commands/rpop)、[RPOPLPUSH](http://redis.io/commands/rpoplpush)、[RPUSH](http://redis.io/commands/rpush)、[RPUSHX](http://redis.io/commands/rpushx)\n\n### 5、Sets\n\n#### 1）简介\n\n与数学的中的集合概念类似，没有重复的值，对其有添加删除操作，可对都个结合求交、并等操作，key理解为集合的名字。新浪微博中的：“我和她都关注了”只需要一个SINTER命令就可以实现。\n\nSets通过Hash Table实现，添加删除的时间复杂度均为O(n)，HashTable会随着添加或者删除自动调整大小。需要注意的是，调整HashTable大小需要同步（获取写锁）阻塞读写操作，后期可能会采用SkipList（无序如何使用SkipList？）实现。\n\n和其它类型一样，最大支持2^32-1个元素。\n\n#### 2）结构定义\n\n与Hashes中的dict一致。\n\n#### 3）支持的方法\n\n[SADD](http://redis.io/commands/sadd)、[SCAR](http://redis.io/commands/scard)、[SDIFF](http://redis.io/commands/sdiff)、[SDIFFSTORE](http://redis.io/commands/sdiffstore)、[SINTER](http://redis.io/commands/sinter)、[SISMEMBER](http://redis.io/commands/sismember)\n\n[SMEMBERS](http://redis.io/commands/smembers)、[SMOVE](http://redis.io/commands/smove)、[SPOP](http://redis.io/commands/spop)、[SRANDMEMBER](http://redis.io/commands/srandmember)、[SREM](http://redis.io/commands/srem)\n\n[SUNION](http://redis.io/commands/sunion)、[SUNIONSTORE](http://redis.io/commands/sunionstore)\n\n### 6、ZSets\n\n#### 1）简介\n\nZSets为Set的升级版本，即排序的Sets，在Set的基础之上增加了顺序（Score）属性，每次插入均需要指定，且会自动重新调整值的顺序。Score为double类型，ZSets实现为SkipList与HashTable的混合体。\n\n元素到Score的映射是添加在HashTable中的，所以给定一个元素获取Score开销为O(1)，Score到元素的映射则为SkipList。\n\n#### 2）结构定义\n\n``` c\n/* ZSETs use a specialized version of Skiplists */\ntypedef struct zskiplistNode {  \n    robj *obj;  \n    double score;  \n    struct zskiplistNode *backward;  \n    struct zskiplistLevel {  \n        struct zskiplistNode *forward;  \n        unsigned int span;  \n    } level[];  \n} zskiplistNode;  \n\ntypedef struct zskiplist {  \n    struct zskiplistNode *header, *tail;  \n    unsigned long length;  \n    int level;  \n} zskiplist;  \n\ntypedef struct zset {  \n    dict *dict;    // Value to Score  \n    zskiplist *zsl;  // Score to Value  \n} zset;\n```\n\n#### 3）支持命令\n\n[ZADD](http://redis.io/commands/zadd)、[ZCARD](http://redis.io/commands/zcard)、[ZCOUNT](http://redis.io/commands/zcount)、[ZINCRBY](http://redis.io/commands/zincrby)、[ZINTERSTORE](http://redis.io/commands/zinterstore)\n\n[ZRANGE](http://redis.io/commands/zrange)、[ZRANGEBYSCORE](http://redis.io/commands/zrangebyscore)、[ZRANK](http://redis.io/commands/zrank)、[ZREM](http://redis.io/commands/zrem)\n\n[ZREMRANGEBYRANK](http://redis.io/commands/zremrangebyrank)、[ZREMRANGEBYSCORE](http://redis.io/commands/zremrangebyscore)、[ZREVRANGE](http://redis.io/commands/zrevrange)\n\n[ZREVRANGEBYSCORE](http://redis.io/commands/zrevrangebyscore)、[ZREVRANK](http://redis.io/commands/zrevrank)、[ZSCORE](http://redis.io/commands/zscore)、[ZUNIONSTORE](http://redis.io/commands/zunionstore)\n\n> 参考资料：\n>\n> [Redis.io](http://redis.io)\n>\n> [The Little Redis Book](http://openmymind.net/2012/1/23/The-Little-Redis-Book/)\n","slug":"2012/03/redis-data-strutrue","published":1,"updated":"2015-12-30T11:43:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93e005y3x8f1b14ubcz"},{"title":"Java装箱/拆箱技术","id":"409","date":"2012-03-03T17:07:14.000Z","_content":"\n今天群里提出了一个有趣的问题：\n\n<!--more-->\n\n```\nInteger num1 = 128;  \nInteger num2 = 128;  \nint num3 = 128;  \nint num4 = 128;  \nSystem.out.println(num1 == num2);  \nSystem.out.println(num1 == num3);  \nSystem.out.println(num2 == num3);  \nSystem.out.println(num3 == num4);  \nSystem.out.println(\"============\");  \nInteger num5 = 12;  \nInteger num6 = 12;  \nint num7 = 12;  \nint num8 = 12;  \nSystem.out.println(num5 == num6);  \nSystem.out.println(num5 == num7);  \nSystem.out.println(num6 == num7);  \nSystem.out.println(num7 == num8);\n```\n\n乍一看，好似是常量池的题目，但是运行一遍结果之后，好像有些迷糊，结果只有第一个输出为false，其它地方均为true。\n\n`（num3==num4）`以及`（num7==num8）`这个倒是没什么，常量池基本知识。\n\nInteger与int比较，以及Integer内部比较是怎么样的呢？这就涉及到自动装箱/拆箱技术了，一般来说，具有包装类的语言都有这种技术，如Objective-C、Java等，该技术只是编译器给用户提供的一些方便（编译器糖：Compiler Sugar），那么我们来看看Java是怎么实现的吧。附上字节码：\n\n![image](/images/2012/03/image.png)\n\nJava的自动装箱直接调用了Integer的静态方法Integer.valueOf，实质上还是创建了一个Integer对象。\n\n![image](/images/2012/03/image1.png)\n\n而拆箱呢，有人说是将int变量先装箱，再比较。但是字节码说明了，是获得对象的值，再比较。\n\n还有一个疑问，为什么在128的时候num1 != num2，而改一个常量12就等于了呢？需要说明的是，在自动装箱时对于值从[–128, 127]之间的值，它们被装箱为Integer对象后，会在内存中被重用（装箱对象池？）。即如果创建时值位于界内，它会先判断是否存在该对象，有的话直接指向地址。如果不位于界内，则会直接创建新对象。\n\n这样的功能嘛，还是推荐使用标准的格式（=new Integer(value);），自个明白呢，以后的人也明白。不过自个私底下研究研究还是很开脑的。\n\n> **P.S.**: 反编译字节码命令 javap –c ClassName，不能带.class后缀\n","source":"_posts/2012/03/java-boxing.md","raw":"title: Java装箱/拆箱技术\ntags:\n  - Java\nid: 409\ncategories:\n  - 技术分享\ndate: 2012-03-04 01:07:14\n---\n\n今天群里提出了一个有趣的问题：\n\n<!--more-->\n\n```\nInteger num1 = 128;  \nInteger num2 = 128;  \nint num3 = 128;  \nint num4 = 128;  \nSystem.out.println(num1 == num2);  \nSystem.out.println(num1 == num3);  \nSystem.out.println(num2 == num3);  \nSystem.out.println(num3 == num4);  \nSystem.out.println(\"============\");  \nInteger num5 = 12;  \nInteger num6 = 12;  \nint num7 = 12;  \nint num8 = 12;  \nSystem.out.println(num5 == num6);  \nSystem.out.println(num5 == num7);  \nSystem.out.println(num6 == num7);  \nSystem.out.println(num7 == num8);\n```\n\n乍一看，好似是常量池的题目，但是运行一遍结果之后，好像有些迷糊，结果只有第一个输出为false，其它地方均为true。\n\n`（num3==num4）`以及`（num7==num8）`这个倒是没什么，常量池基本知识。\n\nInteger与int比较，以及Integer内部比较是怎么样的呢？这就涉及到自动装箱/拆箱技术了，一般来说，具有包装类的语言都有这种技术，如Objective-C、Java等，该技术只是编译器给用户提供的一些方便（编译器糖：Compiler Sugar），那么我们来看看Java是怎么实现的吧。附上字节码：\n\n![image](/images/2012/03/image.png)\n\nJava的自动装箱直接调用了Integer的静态方法Integer.valueOf，实质上还是创建了一个Integer对象。\n\n![image](/images/2012/03/image1.png)\n\n而拆箱呢，有人说是将int变量先装箱，再比较。但是字节码说明了，是获得对象的值，再比较。\n\n还有一个疑问，为什么在128的时候num1 != num2，而改一个常量12就等于了呢？需要说明的是，在自动装箱时对于值从[–128, 127]之间的值，它们被装箱为Integer对象后，会在内存中被重用（装箱对象池？）。即如果创建时值位于界内，它会先判断是否存在该对象，有的话直接指向地址。如果不位于界内，则会直接创建新对象。\n\n这样的功能嘛，还是推荐使用标准的格式（=new Integer(value);），自个明白呢，以后的人也明白。不过自个私底下研究研究还是很开脑的。\n\n> **P.S.**: 反编译字节码命令 javap –c ClassName，不能带.class后缀\n","slug":"2012/03/java-boxing","published":1,"updated":"2015-12-30T11:37:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93h00653x8fvpecf1l8"},{"title":"趣味数据结构 - SkipLists","id":"460","date":"2012-03-09T13:13:11.000Z","_content":"\n### 1、简介\n\n给一串有序的数据，如何存储可以增删查改快速方便、扩容简单、实现也简单呢？用数组吧，实现简单，二分法也老快了，但是删除就很麻烦了，且扩容也需要开辟新的空间。用链表吧，新增删除都很快，但是查找就得遍历了。用平衡树（AVL、红黑树）吧，新增删除扩容都很方便，但是实现起来非常麻烦。\n\n<!--more-->\n\n说完上面的，来推荐一个有趣的数据结构，可方便实现上面几个要求，那就是SkipLists（跳表）。我们先来看看SkipList作者William Pugh对它的定义吧：\n\n> Skip lists are a data structure that can be used in place of balanced trees. Skip lists use probabilistic balancing rather than strictly enforced balancing and as a result the algorithms for insertion and deletion in skip lists are much simpler and significantly faster than equivalent algorithms for balanced trees.(1990)\n\nSkipLists设计出来就是用来取代平衡树的，SkipList依靠随机的思想，从某种程序上实现了平衡，且插入与删除都比较简单。\n\n### 2、SkipLists思想\n\n![image](/images/2012/03/image4.png)\n\n我们先来看看上面的有序链表（例子Copy自文献），需要查找一个节点，我们得获得头节点，再依次遍历下去，时间复杂度为O(n)。\n\n![image](/images/2012/03/image5.png)\n\n如果有上面这种结构，每间隔一个节点添加一个额外指针指向下下个节点。那么每次查找先查下下节点，大于继续查找，小于的话，查找下个节点。这样的话，时间复杂度就降到了O(n/2)。\n\n这基本上就是跳表的核心思想，即通过“空间来换取时间”的一个算法，通过在每个节点中增加了向前的指针，从而提升查找的效率。\n\n### 3、构造过程\n\n1. 给定一个有序的链表；\n2. 选择链表最上层的最大、最小节点，然后从其它节点中按照一定算法随机选出一些节点，将这些节点组成有序链表。新链表称之为第一层，原链表称之为下一层；\n3. 为刚选出的每个节点添加一个指针域，这个指针指向下一层中相应的节点。Top指针指向首层首节点；\n4. 重复2、3步，直到不能选选择出除最大最小节点之外的节点。\n\n如下图：\n\n![clip_image006](/images/2012/03/clip_image006.jpg)\n\n### 4、结构特征\n\n1. 一个SkipList应由多个层组成（Level）；\n2. SkipList的最底层包含所有的节点；\n3. 每一层都是一个有序链表；\n4. 如果节点X出现在第i层，那么所有&lt;i的层都含有X；\n5. Top指向最高层的首节点；\n6. 第i层的节点通过down指针指向下一层相应的节点；\n7. 每一层均有全局最大最小值。\n\n### 5、其它\n\n#### 1）随机算法\n\n调用一个随机函数，该函数返回节点会撑到第几层，伪代码如下：\n\n```\nrandomLevel()\n    lvl := 1\n    — random() that returns a random value in [0…1)\n    while random() < p and lvl < MaxLevel do\n         lvl := lvl + 1\n    return lvl\n```\n\n一般来说，p为0.5，那么能撑到第i的概率为0.5^i。\n\n#### 2）查找\n\nSkipLists查找从Top节点开始，如二分查找一样，大于下一节点就继续，小于则跳到下一层。直到找到节点或者到NULL为止。时间复杂度O(logn)。\n\n#### 3）插入\n\n插入也用上面的查找，但是每下一层均要记录转下的那个节点，将其存入update中。待找到插入的位置之后，需要在所有下层插入节点，根据update节点更新。\n\n同时，插入一个元素也需要调用随机算法，判断是否需要更新链表。使用的随机算法和构造时的算法是一致的。时间复杂度O(logn)。\n\n#### 4）删除\n\n删除找到之后，就直接删除了，没什么特别的地方。时间复杂度O(logn)。\n\n> 参考资料：\n>\n> William Pugh：[Skip lists: a probabilistic alternative to balanced trees](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.9211&rep=rep1&type=pdf)\n>\n> [跳表](http://www.cnblogs.com/xuqiang/archive/2011/05/22/2053516.html)\n","source":"_posts/2012/03/data-structure-skiplists.md","raw":"title: 趣味数据结构 - SkipLists\ntags:\n  - 数据结构\nid: 460\ncategories:\n  - 技术分享\ndate: 2012-03-09 21:13:11\n---\n\n### 1、简介\n\n给一串有序的数据，如何存储可以增删查改快速方便、扩容简单、实现也简单呢？用数组吧，实现简单，二分法也老快了，但是删除就很麻烦了，且扩容也需要开辟新的空间。用链表吧，新增删除都很快，但是查找就得遍历了。用平衡树（AVL、红黑树）吧，新增删除扩容都很方便，但是实现起来非常麻烦。\n\n<!--more-->\n\n说完上面的，来推荐一个有趣的数据结构，可方便实现上面几个要求，那就是SkipLists（跳表）。我们先来看看SkipList作者William Pugh对它的定义吧：\n\n> Skip lists are a data structure that can be used in place of balanced trees. Skip lists use probabilistic balancing rather than strictly enforced balancing and as a result the algorithms for insertion and deletion in skip lists are much simpler and significantly faster than equivalent algorithms for balanced trees.(1990)\n\nSkipLists设计出来就是用来取代平衡树的，SkipList依靠随机的思想，从某种程序上实现了平衡，且插入与删除都比较简单。\n\n### 2、SkipLists思想\n\n![image](/images/2012/03/image4.png)\n\n我们先来看看上面的有序链表（例子Copy自文献），需要查找一个节点，我们得获得头节点，再依次遍历下去，时间复杂度为O(n)。\n\n![image](/images/2012/03/image5.png)\n\n如果有上面这种结构，每间隔一个节点添加一个额外指针指向下下个节点。那么每次查找先查下下节点，大于继续查找，小于的话，查找下个节点。这样的话，时间复杂度就降到了O(n/2)。\n\n这基本上就是跳表的核心思想，即通过“空间来换取时间”的一个算法，通过在每个节点中增加了向前的指针，从而提升查找的效率。\n\n### 3、构造过程\n\n1. 给定一个有序的链表；\n2. 选择链表最上层的最大、最小节点，然后从其它节点中按照一定算法随机选出一些节点，将这些节点组成有序链表。新链表称之为第一层，原链表称之为下一层；\n3. 为刚选出的每个节点添加一个指针域，这个指针指向下一层中相应的节点。Top指针指向首层首节点；\n4. 重复2、3步，直到不能选选择出除最大最小节点之外的节点。\n\n如下图：\n\n![clip_image006](/images/2012/03/clip_image006.jpg)\n\n### 4、结构特征\n\n1. 一个SkipList应由多个层组成（Level）；\n2. SkipList的最底层包含所有的节点；\n3. 每一层都是一个有序链表；\n4. 如果节点X出现在第i层，那么所有&lt;i的层都含有X；\n5. Top指向最高层的首节点；\n6. 第i层的节点通过down指针指向下一层相应的节点；\n7. 每一层均有全局最大最小值。\n\n### 5、其它\n\n#### 1）随机算法\n\n调用一个随机函数，该函数返回节点会撑到第几层，伪代码如下：\n\n```\nrandomLevel()\n    lvl := 1\n    — random() that returns a random value in [0…1)\n    while random() < p and lvl < MaxLevel do\n         lvl := lvl + 1\n    return lvl\n```\n\n一般来说，p为0.5，那么能撑到第i的概率为0.5^i。\n\n#### 2）查找\n\nSkipLists查找从Top节点开始，如二分查找一样，大于下一节点就继续，小于则跳到下一层。直到找到节点或者到NULL为止。时间复杂度O(logn)。\n\n#### 3）插入\n\n插入也用上面的查找，但是每下一层均要记录转下的那个节点，将其存入update中。待找到插入的位置之后，需要在所有下层插入节点，根据update节点更新。\n\n同时，插入一个元素也需要调用随机算法，判断是否需要更新链表。使用的随机算法和构造时的算法是一致的。时间复杂度O(logn)。\n\n#### 4）删除\n\n删除找到之后，就直接删除了，没什么特别的地方。时间复杂度O(logn)。\n\n> 参考资料：\n>\n> William Pugh：[Skip lists: a probabilistic alternative to balanced trees](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.9211&rep=rep1&type=pdf)\n>\n> [跳表](http://www.cnblogs.com/xuqiang/archive/2011/05/22/2053516.html)\n","slug":"2012/03/data-structure-skiplists","published":1,"updated":"2015-12-30T11:35:57.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93j00683x8f4x6awurv"},{"title":"趣味数据结构 - 并查集","id":"477","date":"2012-03-16T14:45:30.000Z","_content":"\n### 1、简介\n\n在某些应用中，会将n个不同的元素分成一组不相交的集合（disjoint）。不相交的集合有两个重要的操作，即找到给定的元素所属的集合（find）和合并两个集合（union）。为了更好的支持这两种操作，就出现了并查集（Disjoint-Set or Union-find set）。\n\n<!--more-->\n\n并查集保持了一组不相交的动态集合，每个集合通过一个代表来识别，代表即集合中的某个成员。哪个成员被选中无所谓iwom关心的是如果寻找某一动态集合的代表两次，并且在两次寻找之间不修改集合，两次得到的答案应该是一样的。\n\n### 2、基本操作\n\n它主要涉及两个基本操作，分别为：\n\n* **Union-Set(x, y)**：合并两个不相交集合\n\n* **Find-Set(x)**：判断两个元素是否属于同一个集合\n\n还需要另外一个基本操作，即：\n\n* **Make-Set(x)**：新建一个集合，唯一的成员也是代表就是x\n\n### 3、实现方法\n\n现有不相交集合：`{1, 3, 7}，{4}，{2, 5, 9, 10}，{6, 8}`\n\n#### 1）用编号最小的元素标记所在集合\n\n`{1, 3, 7}，{4}，{2, 5, 9, 10}，{6, 8}`\n\n#### 2）定义一个数组set[1...n]，其中set[i]表示元素i所在集合\n\n![clip_image002](/images/2012/03/clip_image002.jpg)\n\n#### 3）find操作\n\n```\nfind(x):\n     return set[x];\n```\n\n#### 4）Union操作\n\n```\nunion(x, y):\n    for k in [0, n): // 遍历所有集合，更新其中一个集合的代表\n        if (set[k] == find(a)):\n            set[k] := find(b);\n```\n\n### 4、实现分析\n\n上面实现很简单，find操作只需要返回其代表即可，时间复杂为O(1)。但是Union操作则需要修改其中一个集合所有的代表，同时由于是用的数组存的，元素为数组的索引，必须要遍历所有元素才可以修改，时间复杂为O(n)。要优化操作，就必须优化数据结构。\n\n### 5、优化\n\n#### 1）链表\n\n每个集合建立一个链表，有头尾指针，头结点为代表。所有结点都添加了指向代表的指针。\n\n很容易知道，find操作时间复杂度为O(1)，合并只需要将较小的集合添加到另一个集合的后尾，再更新代表即可，时间复杂度也为O(n)，与数组相比，在时间上优化了一点点。\n\n#### 2）有根树\n\n并查集目前最好的实现是用有根树，即建立一个森林，每棵树是一个集合，树根元素就是代表，每个结点存储指向其父亲结点的指针（而不是指向子结点的指针）。\n\n可执行三种不相交集合操作：\n\n1. Make-Set：创建一颗仅包含一个结点的树；\n2. Find-Set：查找可以描述为找两个元素各自的根，判断其是否相等。实现中需要沿着父结点指针一直找下去，直到找到树根为止。（时间复杂度O(n)）\n3. Union-Set：并集可以描述为把一棵树接到另一个棵树的根结点上，并更新某颗树的代表。（时间复杂度O(n)）\n\n![clip_image003](/images/2012/03/clip_image003.jpg)\n\n其实这个与链表来说，性能没有本质上的提高。合并也需要更新结点代表，且如果树构造的时候，构造了一颗线性链的树，查找复杂度也提高了。对其进行优化，有两种策略：\n\n**a. 按秩合并（union by rank）**\n\n秩（Rank）就是一颗树的结点数，即使包含较少结点的树根指向包含较多结点的树根。\n\n**b. 路径压缩**\n\n![clip_image005](/images/2012/03/clip_image005.jpg)\n\n如上图，使查找路径上的每个结点都直接指向根结点。简单而有效。\n\n### 6、小结\n\n在实现中，并查集均是使用有根树结合按秩合并和路径压缩来实现。按秩合并提高了Union-Set操作效率，而路径压缩提高了Find-Set操作效率。\n\n空间复杂度为O(N)，建立一个集合的时间复杂度为O(1)， N次合并M查找的时间复杂度为O(M Alpha(N))，这里Alpha是Ackerman函数的某个反函数，在很大的范围内（人类目前观测到的宇宙范围估算有10的80次方个原子，这小于前面所说的范围）这个函数的值可以看成是不大于4的，所以并查集的操作可以看作是线性的。具体证明得参加《算法导论》。\n\n并查集常作为另一种复杂的数据结构或者算法的存储结构。常见的应用有：求无向图的连通分量个数、最近公共祖先（LCA）、最小生成树等。\n\n> 参考资料：\n>\n> 《算法导论》，第二十二章\n>\n> 董的博客：[数据结构之并查集](http://dongxicheng.org/structure/union-find-set/)\n","source":"_posts/2012/03/data-structure-disjoint-set.md","raw":"title: 趣味数据结构 - 并查集\ntags:\n  - 数据结构\nid: 477\ncategories:\n  - 技术分享\ndate: 2012-03-16 22:45:30\n---\n\n### 1、简介\n\n在某些应用中，会将n个不同的元素分成一组不相交的集合（disjoint）。不相交的集合有两个重要的操作，即找到给定的元素所属的集合（find）和合并两个集合（union）。为了更好的支持这两种操作，就出现了并查集（Disjoint-Set or Union-find set）。\n\n<!--more-->\n\n并查集保持了一组不相交的动态集合，每个集合通过一个代表来识别，代表即集合中的某个成员。哪个成员被选中无所谓iwom关心的是如果寻找某一动态集合的代表两次，并且在两次寻找之间不修改集合，两次得到的答案应该是一样的。\n\n### 2、基本操作\n\n它主要涉及两个基本操作，分别为：\n\n* **Union-Set(x, y)**：合并两个不相交集合\n\n* **Find-Set(x)**：判断两个元素是否属于同一个集合\n\n还需要另外一个基本操作，即：\n\n* **Make-Set(x)**：新建一个集合，唯一的成员也是代表就是x\n\n### 3、实现方法\n\n现有不相交集合：`{1, 3, 7}，{4}，{2, 5, 9, 10}，{6, 8}`\n\n#### 1）用编号最小的元素标记所在集合\n\n`{1, 3, 7}，{4}，{2, 5, 9, 10}，{6, 8}`\n\n#### 2）定义一个数组set[1...n]，其中set[i]表示元素i所在集合\n\n![clip_image002](/images/2012/03/clip_image002.jpg)\n\n#### 3）find操作\n\n```\nfind(x):\n     return set[x];\n```\n\n#### 4）Union操作\n\n```\nunion(x, y):\n    for k in [0, n): // 遍历所有集合，更新其中一个集合的代表\n        if (set[k] == find(a)):\n            set[k] := find(b);\n```\n\n### 4、实现分析\n\n上面实现很简单，find操作只需要返回其代表即可，时间复杂为O(1)。但是Union操作则需要修改其中一个集合所有的代表，同时由于是用的数组存的，元素为数组的索引，必须要遍历所有元素才可以修改，时间复杂为O(n)。要优化操作，就必须优化数据结构。\n\n### 5、优化\n\n#### 1）链表\n\n每个集合建立一个链表，有头尾指针，头结点为代表。所有结点都添加了指向代表的指针。\n\n很容易知道，find操作时间复杂度为O(1)，合并只需要将较小的集合添加到另一个集合的后尾，再更新代表即可，时间复杂度也为O(n)，与数组相比，在时间上优化了一点点。\n\n#### 2）有根树\n\n并查集目前最好的实现是用有根树，即建立一个森林，每棵树是一个集合，树根元素就是代表，每个结点存储指向其父亲结点的指针（而不是指向子结点的指针）。\n\n可执行三种不相交集合操作：\n\n1. Make-Set：创建一颗仅包含一个结点的树；\n2. Find-Set：查找可以描述为找两个元素各自的根，判断其是否相等。实现中需要沿着父结点指针一直找下去，直到找到树根为止。（时间复杂度O(n)）\n3. Union-Set：并集可以描述为把一棵树接到另一个棵树的根结点上，并更新某颗树的代表。（时间复杂度O(n)）\n\n![clip_image003](/images/2012/03/clip_image003.jpg)\n\n其实这个与链表来说，性能没有本质上的提高。合并也需要更新结点代表，且如果树构造的时候，构造了一颗线性链的树，查找复杂度也提高了。对其进行优化，有两种策略：\n\n**a. 按秩合并（union by rank）**\n\n秩（Rank）就是一颗树的结点数，即使包含较少结点的树根指向包含较多结点的树根。\n\n**b. 路径压缩**\n\n![clip_image005](/images/2012/03/clip_image005.jpg)\n\n如上图，使查找路径上的每个结点都直接指向根结点。简单而有效。\n\n### 6、小结\n\n在实现中，并查集均是使用有根树结合按秩合并和路径压缩来实现。按秩合并提高了Union-Set操作效率，而路径压缩提高了Find-Set操作效率。\n\n空间复杂度为O(N)，建立一个集合的时间复杂度为O(1)， N次合并M查找的时间复杂度为O(M Alpha(N))，这里Alpha是Ackerman函数的某个反函数，在很大的范围内（人类目前观测到的宇宙范围估算有10的80次方个原子，这小于前面所说的范围）这个函数的值可以看成是不大于4的，所以并查集的操作可以看作是线性的。具体证明得参加《算法导论》。\n\n并查集常作为另一种复杂的数据结构或者算法的存储结构。常见的应用有：求无向图的连通分量个数、最近公共祖先（LCA）、最小生成树等。\n\n> 参考资料：\n>\n> 《算法导论》，第二十二章\n>\n> 董的博客：[数据结构之并查集](http://dongxicheng.org/structure/union-find-set/)\n","slug":"2012/03/data-structure-disjoint-set","published":1,"updated":"2015-12-30T11:32:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93k006b3x8fvwoogfg3"},{"title":"趣味数据结构 - BitMap","id":"422","date":"2012-03-05T13:37:49.000Z","_content":"\n### 1、什么是Bit-Map\n\nBit-Map被译为位图，和人讨论的时候，常常会与.BMP搞混，这个Map我觉得翻译成映射更为合适，Bit-Map也算是Hash的一直极致运用吧。Bit-Map会用Bit来标记某个元素对应的value，如何标记的呢，见下例：\n\n<!--more-->\n\n我们现在有(1,2,5,8,10)数组，常规来说是这样声明的：\n\n`int[] array = {1, 2, 5, 8, 10}`\n\n上面这样声明会占用4×5个字节，即20个字节，少量数据可能没有什么特别大的感觉，如果数组长度为10,000,000，这样的方式就会占用4G的内存。\n\n如果用Bit-Map的话，可以这样来组织：\n\n```\nbyte[] bytes = new bytes[2];\nbytes[0] = 01100100; // 就直接写二进制了\nbytes[1] = 10100000;\n```\n\n### 2、Bit-Map建立\n\n有了上面的例子之后，不知道对Bit-Map是否有了一个感性的认识。下面说下Bit-Map的建立过程。\n\n#### 1）开辟定长数组\n\nBit-Map会声明一个定长的byte/int数组，之后将数组内元素的所有Bit位均置为0，如下图：\n\n![image](/images/2012/03/image2.png)\n\n#### 2）遍历数据，并插入Bit-Map\n\n上例来说，就会遍历`array{1, 2, 5, 8, 10}`，并将所有的元素均插入Bit-Map中。Bit-Map是Hash的极致，那么key即为`array[i]/8`，value即在byte中的位置`array[i]%8`。而实际中为了效率，hash函数可能会有些出入。如下：\n\n```\nByte: MASK = 0X07; SHIFT = 3; Integer: MASK = 0X1F; SHIFT = 5;\n// i>> SHIFT => i/8; i & MASK => i%8\nset(i): array[i>>SHITF] |= (1 << (i & MASK);\n// return 0: not exist\nisExist(i) : return array[i>>SHIFT] & (1 << (i & MASK);\n```\n\n遍历插入之后的数据应该是这样的：\n\n![image](/images/2012/03/image3.png)\n\n### 3、Bit-Map应用\n\n建立了Bit-Map之后，就可以方便的使用了。一般来说Bit-Map可作为数据的查找、去重、排序等操作。\n\n如上面提及的10,000,000个数据存储问题，用Integer存储，耗费4G内存。改成Bit-Map，耗费125MB内存。但是实际中，可能由于数据中最大最小值相差太大，如`{1,2 99999}`，只有三个数，但是最大最小相差悬殊，该方法就不适用了。\n\n查找和去重都好理解，至于排序，有点类似桶排序，每个byte都是一个桶。至于应用实例，自个用的少，copy别人的吧。\n\n#### 1)已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数\n\n8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。可以理解为从0-99 999 999的数字，每个数字对应一个Bit位，所以只需要99M个Bit==1.2MBytes，这样，就用了小小的1.2M左右的内存表示了所有的8位数的电话。\n\n#### 2)2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数\n\n将bit-map扩展一下，用2bit表示一个数即可：0表示未出现；1表示出现一次；2表示出现2次及以上，即重复，在遍历这些数的时候，如果对应位置的值是0，则将其置为1；如果是1，将其置为2；如果是2，则保持不变。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map，都是一样的道理。\n\n> 参考资料：\n>\n> [海量数据处理专题（四）——Bit-map](http://blog.redfox66.com/post/2010/09/26/mass-data-4-bitmap.aspx)\n","source":"_posts/2012/03/data-structure-bitmap.md","raw":"title: 趣味数据结构 - BitMap\ntags:\n  - 数据结构\nid: 422\ncategories:\n  - 技术分享\ndate: 2012-03-05 21:37:49\n---\n\n### 1、什么是Bit-Map\n\nBit-Map被译为位图，和人讨论的时候，常常会与.BMP搞混，这个Map我觉得翻译成映射更为合适，Bit-Map也算是Hash的一直极致运用吧。Bit-Map会用Bit来标记某个元素对应的value，如何标记的呢，见下例：\n\n<!--more-->\n\n我们现在有(1,2,5,8,10)数组，常规来说是这样声明的：\n\n`int[] array = {1, 2, 5, 8, 10}`\n\n上面这样声明会占用4×5个字节，即20个字节，少量数据可能没有什么特别大的感觉，如果数组长度为10,000,000，这样的方式就会占用4G的内存。\n\n如果用Bit-Map的话，可以这样来组织：\n\n```\nbyte[] bytes = new bytes[2];\nbytes[0] = 01100100; // 就直接写二进制了\nbytes[1] = 10100000;\n```\n\n### 2、Bit-Map建立\n\n有了上面的例子之后，不知道对Bit-Map是否有了一个感性的认识。下面说下Bit-Map的建立过程。\n\n#### 1）开辟定长数组\n\nBit-Map会声明一个定长的byte/int数组，之后将数组内元素的所有Bit位均置为0，如下图：\n\n![image](/images/2012/03/image2.png)\n\n#### 2）遍历数据，并插入Bit-Map\n\n上例来说，就会遍历`array{1, 2, 5, 8, 10}`，并将所有的元素均插入Bit-Map中。Bit-Map是Hash的极致，那么key即为`array[i]/8`，value即在byte中的位置`array[i]%8`。而实际中为了效率，hash函数可能会有些出入。如下：\n\n```\nByte: MASK = 0X07; SHIFT = 3; Integer: MASK = 0X1F; SHIFT = 5;\n// i>> SHIFT => i/8; i & MASK => i%8\nset(i): array[i>>SHITF] |= (1 << (i & MASK);\n// return 0: not exist\nisExist(i) : return array[i>>SHIFT] & (1 << (i & MASK);\n```\n\n遍历插入之后的数据应该是这样的：\n\n![image](/images/2012/03/image3.png)\n\n### 3、Bit-Map应用\n\n建立了Bit-Map之后，就可以方便的使用了。一般来说Bit-Map可作为数据的查找、去重、排序等操作。\n\n如上面提及的10,000,000个数据存储问题，用Integer存储，耗费4G内存。改成Bit-Map，耗费125MB内存。但是实际中，可能由于数据中最大最小值相差太大，如`{1,2 99999}`，只有三个数，但是最大最小相差悬殊，该方法就不适用了。\n\n查找和去重都好理解，至于排序，有点类似桶排序，每个byte都是一个桶。至于应用实例，自个用的少，copy别人的吧。\n\n#### 1)已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数\n\n8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。可以理解为从0-99 999 999的数字，每个数字对应一个Bit位，所以只需要99M个Bit==1.2MBytes，这样，就用了小小的1.2M左右的内存表示了所有的8位数的电话。\n\n#### 2)2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数\n\n将bit-map扩展一下，用2bit表示一个数即可：0表示未出现；1表示出现一次；2表示出现2次及以上，即重复，在遍历这些数的时候，如果对应位置的值是0，则将其置为1；如果是1，将其置为2；如果是2，则保持不变。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map，都是一样的道理。\n\n> 参考资料：\n>\n> [海量数据处理专题（四）——Bit-map](http://blog.redfox66.com/post/2010/09/26/mass-data-4-bitmap.aspx)\n","slug":"2012/03/data-structure-bitmap","published":1,"updated":"2015-12-30T11:28:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93l006e3x8f72kgd4iy"},{"title":"MapReduce优化（一）","id":"265","date":"2012-02-11T15:36:10.000Z","_content":"\n### 一、概述\n\n这篇博文主要是解决上一篇[迭代式MapReduce解决方案（一）](http://www.hongweiyi.com/?p=250)中总结所提到的第三个问题，与网上大多数Hadoop调优（[董的博客](http://dongxicheng.org/tag/hadoop%E4%BC%98%E5%8C%96/);、[淘宝数据平台](http://www.tbdata.org/archives/1470)）不太一样，网上告诉的是方法，但是方法是什么以及优化后能达到什么效果没有一个直观的感受。这篇博文讲述了一些简单的优化手段，可将140M的临时文件缩小到4.9M，期望能有一些对优化一些更为直观的感受，起到抛砖引玉的作用。\n <!--more-->  \n\n### 二、问题的提出\n\n用的例子依然是上篇博客讲到的PageRank计算，其中输入数据为随机生成的100W行记录，大小3.22G。我们也可以来粗略的估算一下，单个map task生成的临时文件大小：\n\n3.22G数据，100W记录。每行平均32kb，一个split为64M，约2W行数据。由于是随机生成的数据，所以每行平均约为500个外链地址，每个连接地址都会生成一行临时结果<URL_ID AER_PR>，算每行结果15字节，那么最后的生成结果为2W×500×15b = 150M。\n\n而实际上，在不进行任何优化的情况下，一个map task生成的临时文件为140.6M，很大的结果啊！\n\n### 三、优化方案\n\n#### 1、设置combiner\n\nMapreduce中的Combiner就是为了避免map任务和reduce任务之间的数据传输而设置的，Hadoop允许用户针对map task的输出指定一个合并函数。\n\n对于Combiner有几点需要说明的是：\n\n1）有很多人认为这个combiner和map输出的数据合并是一个过程，其实不然，map输出的数据合并只会产生在有数据spill出的时候，即进行merge操作。\n2）与mapper与reducer不同的是，combiner没有默认的实现，需要显式的设置在conf中才有作用。\n3）并不是所有的job都适用combiner，只有操作满足结合律的才可设置combiner。combine操作类似于：opt(opt(1, 2, 3), opt(4, 5, 6))。如果opt为求和、求最大值的话，可以使用，但是如果是求中值的话，不适用。\n4）一般来说，combiner即reducer，它们俩进行同样的操作。\n\n对于PageRank计算来说，单个reduce操作即对值求和，适用combine操作。添加代码如下：\n\n```\njob.setCombinerClass(PRReducer.class);  \n```\n\n最后输出结果大小28.3M，“压缩”比约为20%。\n\n#### 2、数据压缩\n\n顾名思义，对输出结果进行压缩，Hadoop称之为codec。下面列举一些常见的codec：\n\n|压缩格式|HadoopCompressionCodec|\n|-------|---------------------|\n|DEFLATE| org.apache.hadoop.io.compress.DefaultCodec|\n| gzip | org.apache.hadoop.io.compress.GzipCodec|\n| bzip2 | org.apache.hadoop.io.compress.BZip2Codec |\n| LZO | com.hadoop.compression.lzo.LzopCodec |\n\n以下两行代码即可：\n\n```\nconf.setBoolean(\"mapred.compress.map.output\", true);\nconf.set (\"mapred.map.output.compression.codec\", \"xxxCodec\");\n```\n\n但需要注意的是，时间与空间永远是矛盾的，若要获得大的压缩比就会降低一些时间效率。通常来说，想要达到cpu和磁盘压缩比的平衡取舍，LzoCodec比较适合。不过由于GPL许可的原因，该库没有包含在Apache的发行版中，需要单独从[Google Code](http://code.google.com/p/hadoop-gpl-compression)或[GitHub](https://github.com/kevinweil/hadoop-lzo)下载，其中后者包含有修正的软件错误及其它一些工具。\n\n本文使用的是默认的压缩方式DefaultCodec，压缩比约为29%。\n\n#### 3、查看临时文件内部，具体情况具体分析\n\n![clip_image002](/images/2012/02/clip_image002.jpg)\n\n上面的文件就是我的临时文件内部格式， value在内存中为DoubleWritable，没有考虑精度问题。一个value数据输出后，就会占20字节，我们是否需要这么高的精度呢？\n\n我觉得是不需要，不需要的话，就是将输出数据精度降低，实验中将double精度降至6位，“压缩”比约为59%。这个例子很实在，即对于每个任务来说，不仅仅是job conf需要优化，其自身算法或者说数据格式都还有很大的优化空间。没有最好，只有更好！\n\n### 四、总结\n\n经过上面几个“简单”的优化，代码行数修改寥寥几行，临时数据从140.6M降到了4.9M，压缩比为3.49%。需要注意的是，实验所用数据是模拟的，且数据分布较为均匀，故在实际生产环境中压缩比应该没这么高，所以需要根据job的实际情况，选择combine、压缩、数据格式，但其所带来的优化结果仍会很可观。\n\n本文只是简要的抽出了一些方便做实验的优化方法，更多的更广的配置、代码优化方法，敬请期待以后的博文。\n\n> **参考资料：**\n>\n> * [hadoop作业调优参数整理及原理](http://www.tbdata.org/archives/1470)\n> * Hadoop权威指南\n","source":"_posts/2012/02/mapred-optimize.md","raw":"title: MapReduce优化（一）\ntags:\n  - Hadoop\n  - MapReduce\nid: 265\ncategories:\n  - 技术分享\ndate: 2012-02-11 23:36:10\n---\n\n### 一、概述\n\n这篇博文主要是解决上一篇[迭代式MapReduce解决方案（一）](http://www.hongweiyi.com/?p=250)中总结所提到的第三个问题，与网上大多数Hadoop调优（[董的博客](http://dongxicheng.org/tag/hadoop%E4%BC%98%E5%8C%96/);、[淘宝数据平台](http://www.tbdata.org/archives/1470)）不太一样，网上告诉的是方法，但是方法是什么以及优化后能达到什么效果没有一个直观的感受。这篇博文讲述了一些简单的优化手段，可将140M的临时文件缩小到4.9M，期望能有一些对优化一些更为直观的感受，起到抛砖引玉的作用。\n <!--more-->  \n\n### 二、问题的提出\n\n用的例子依然是上篇博客讲到的PageRank计算，其中输入数据为随机生成的100W行记录，大小3.22G。我们也可以来粗略的估算一下，单个map task生成的临时文件大小：\n\n3.22G数据，100W记录。每行平均32kb，一个split为64M，约2W行数据。由于是随机生成的数据，所以每行平均约为500个外链地址，每个连接地址都会生成一行临时结果<URL_ID AER_PR>，算每行结果15字节，那么最后的生成结果为2W×500×15b = 150M。\n\n而实际上，在不进行任何优化的情况下，一个map task生成的临时文件为140.6M，很大的结果啊！\n\n### 三、优化方案\n\n#### 1、设置combiner\n\nMapreduce中的Combiner就是为了避免map任务和reduce任务之间的数据传输而设置的，Hadoop允许用户针对map task的输出指定一个合并函数。\n\n对于Combiner有几点需要说明的是：\n\n1）有很多人认为这个combiner和map输出的数据合并是一个过程，其实不然，map输出的数据合并只会产生在有数据spill出的时候，即进行merge操作。\n2）与mapper与reducer不同的是，combiner没有默认的实现，需要显式的设置在conf中才有作用。\n3）并不是所有的job都适用combiner，只有操作满足结合律的才可设置combiner。combine操作类似于：opt(opt(1, 2, 3), opt(4, 5, 6))。如果opt为求和、求最大值的话，可以使用，但是如果是求中值的话，不适用。\n4）一般来说，combiner即reducer，它们俩进行同样的操作。\n\n对于PageRank计算来说，单个reduce操作即对值求和，适用combine操作。添加代码如下：\n\n```\njob.setCombinerClass(PRReducer.class);  \n```\n\n最后输出结果大小28.3M，“压缩”比约为20%。\n\n#### 2、数据压缩\n\n顾名思义，对输出结果进行压缩，Hadoop称之为codec。下面列举一些常见的codec：\n\n|压缩格式|HadoopCompressionCodec|\n|-------|---------------------|\n|DEFLATE| org.apache.hadoop.io.compress.DefaultCodec|\n| gzip | org.apache.hadoop.io.compress.GzipCodec|\n| bzip2 | org.apache.hadoop.io.compress.BZip2Codec |\n| LZO | com.hadoop.compression.lzo.LzopCodec |\n\n以下两行代码即可：\n\n```\nconf.setBoolean(\"mapred.compress.map.output\", true);\nconf.set (\"mapred.map.output.compression.codec\", \"xxxCodec\");\n```\n\n但需要注意的是，时间与空间永远是矛盾的，若要获得大的压缩比就会降低一些时间效率。通常来说，想要达到cpu和磁盘压缩比的平衡取舍，LzoCodec比较适合。不过由于GPL许可的原因，该库没有包含在Apache的发行版中，需要单独从[Google Code](http://code.google.com/p/hadoop-gpl-compression)或[GitHub](https://github.com/kevinweil/hadoop-lzo)下载，其中后者包含有修正的软件错误及其它一些工具。\n\n本文使用的是默认的压缩方式DefaultCodec，压缩比约为29%。\n\n#### 3、查看临时文件内部，具体情况具体分析\n\n![clip_image002](/images/2012/02/clip_image002.jpg)\n\n上面的文件就是我的临时文件内部格式， value在内存中为DoubleWritable，没有考虑精度问题。一个value数据输出后，就会占20字节，我们是否需要这么高的精度呢？\n\n我觉得是不需要，不需要的话，就是将输出数据精度降低，实验中将double精度降至6位，“压缩”比约为59%。这个例子很实在，即对于每个任务来说，不仅仅是job conf需要优化，其自身算法或者说数据格式都还有很大的优化空间。没有最好，只有更好！\n\n### 四、总结\n\n经过上面几个“简单”的优化，代码行数修改寥寥几行，临时数据从140.6M降到了4.9M，压缩比为3.49%。需要注意的是，实验所用数据是模拟的，且数据分布较为均匀，故在实际生产环境中压缩比应该没这么高，所以需要根据job的实际情况，选择combine、压缩、数据格式，但其所带来的优化结果仍会很可观。\n\n本文只是简要的抽出了一些方便做实验的优化方法，更多的更广的配置、代码优化方法，敬请期待以后的博文。\n\n> **参考资料：**\n>\n> * [hadoop作业调优参数整理及原理](http://www.tbdata.org/archives/1470)\n> * Hadoop权威指南\n","slug":"2012/02/mapred-optimize","published":1,"updated":"2015-12-29T15:49:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93n006h3x8f2zj104f0"},{"title":"JVM内存结构","id":"270","date":"2012-02-13T05:25:40.000Z","_content":"\n### 1、JVM规定\n\n《The Java Machine Specification》中将JVM内存结构（又称运行时数据区Runtime Data Area）分为六部分（参看第三章）：\n\n1. The pc Register\n2. Java Virtual Machine Stacks\n3. Heap\n4. Method Area\n5. Runtime Constant Pool\n6. Native Method Stacks；\n\n以上数据区的具体描述可参考规范。需要注意的是，以上只是一个规范说明，并没有规定虚拟机如何实现这些数据区。Sun JDK实现将内存空间划分为方法区、堆、本地方法栈、JVM方法栈、PC寄存器五部分。\n\n<!--more-->  \n\n如下图所示：\n\n![clip_image0026](/images/2012/02/clip_image0026.jpg)\n\n### 2、内存空间详解\n\n#### 1）PC寄存器和JVM方法栈\n\n每个线程都会拥有以及创建一个属于自己的PC寄存器和JVM方法栈，PC寄存器占用的有可能为CPU寄存器或者OS内存，而JVM栈占用的为OC内存。\n\n每运行一个方法，便会将方法的信息压入JVM方法栈中，同时将当前执行方法放入PC寄存器中（需要注意的是，如果当前方法为Native方法，PC寄存器的值为空）。可以想到，如果方法栈太深，如递归方法，便会报StackOverflowError，同样如果占用空间太多，也会报OutOfMemoryError。需要修改JVM参数设置：-Xss××k，在××中填入数字。\n\n#### 2）本地方法栈\n\n同JVM方法栈一样，本地方法栈存放的是native方法的调用的状态。在Sun JDK的实现中，本地方法栈和JVM方法栈是同一个。\n\n#### 3）方法区\n\n方法区存放了要加载的类的信息（名称、修饰符等）、类的静态变量、类中定义为fianl类型的常量、类中的Field信息、类中的方法信息，你用Class对象的方法，如getName()、getFields()等来获取信息时，这些数据都来自方法区。需要注意的是，Runtime Constant Pool（常量池）也存放在方法区中。\n\n方法区是被同一个JVM所有线程所共享的，在Sun JDK中这块区域对应Permanet Generation（持久代），默认最小值为16MB，最大值为64MB，可通过-XX:PermSize及-XX:MaxPermSize来指定。当方法区无法满足分配请求时，会报OutOfMemoryError。\n\n#### 4）堆\n\n堆用于存放对象实例以及数组值，可以认为所有通过new来创建的对象的内存均在此分配。一般所说的GC，大部分都是对堆进行的。\n\n堆在32位操作系统上最大为2GB，在64位的则没有限制，大小通过-Xms和-Xmx来控制。-Xms为JVM启动时申请的最小堆内存，默认为物理内存的1/64但小于1GB；-Xmx为JVM可申请的最大堆内存，默认为物理内存的1/4但小于1GB，默认当空余堆内存小于40%的时候，JVM会将堆增大到-Xmx指定大小，可通过-XX:MinHeapFreeRatio=来指定比例，空余堆大于70%时，会将堆大小降到-Xms指定大小，这个参数可用-XX:MaxHeapFreeRatio=来指定。但对于运行系统来说，会避免频繁调整堆大小，会将-Xms和-Xmx的值设为一样。\n\n为了让内存回收更加高效，Sun JDK从1.2开始对堆采取了分代管理的方法，如下图：\n\n![clip_image0046](/images/2012/02/clip_image0046.jpg)\n\n#### 4.1) 新生代（New Generation）\n\n大多数的新建对象都是从新生代中分配内存，新生代由Eden（伊甸园） Space和两块相同的Survivor Space（S0，S1或者From，To）构成。\n\n可通过-Xmn参数来指定新生代大小，-XX:SurvivorRatio来调整Eden与S Space的大小。\n\n#### 4.2）旧生代（Old Generation）\n\n用于存放新生代经过多次垃圾回收仍然存活的对象，像Cache。同时新建的对象也有可能在旧生代上直接分配内存，一般来说是比较的对象，即：单一大对象以及大数组，-XX:PretenureSizeThreshold = 1024 (byte, default = 0)可用来代表单一对象超过多大即不在新生代分配。\n\n旧生代所占内存大小为-Xmx-（-Xmn）。\n\n3、典型JVM参数配置汇总\n\n| 配置 | 解释 |\n| ----|-----|\n| -Xss××k | 方法栈深度 |\n|-XX:PermSize |方法区内存最小值|\n|-XX:MaxPermSize|方法区内存最大值|\n|-Xms|JVM启动分配最小堆内存|\n|-Xmx|JVM启动分配最大堆内存|\n|-XX:MinHeapFreeRatio=|堆内存需扩展时，剩余内存最小比例，默认40%|\n|-XX:MaxHeapFreeRatio=|堆内存需收缩时，剩余内存最大比例，默认70%|\n|-Xmn|堆新生代内存大小|\n|-XX:NewRatio=|如参数为4，则新生代与旧生代比例为1:4|\n|-XX:SurvivorRatio=|S0/S1占新生代内存的比例|\n|-XX:PretenureSizeThreshold=|需要内存超过参数的对象，直接在旧生代分配|\n|-XX:MaxTenuringThreshold=|设置垃圾最大年龄。如果为0，新生代对象不经过S区，直接进行旧生代，值较大的话，会增加新生代对象再GC的概率|\n\n> PretenureSizeThreshold 不一定完全生效，其中取决于 tlab 是否可用。tlab是每个线程在eden里分配的一块内存区域，主要为了提高内存分配效率。如果 tlab 可用的话，优先分配在 tlab 中。\n\n### 4、小结\n\n总的来说，所有语言的内存结构都大同小异，均分为堆、栈、区，堆放动态分配（alloc）的对象，栈存放临时变量、方法过程等，区则存放编译时确定的方法签名、常量池等。\n\nJVM的内存结构需要结合GC一起学习，有兴趣的可以参考<The Java™ Virtual Machine Specification>以及《分布式Java应用》这两本书。\n","source":"_posts/2012/02/jvm-structure.md","raw":"title: JVM内存结构\ntags:\n  - JVM\nid: 270\ncategories:\n  - 技术分享\ndate: 2012-02-13 13:25:40\n---\n\n### 1、JVM规定\n\n《The Java Machine Specification》中将JVM内存结构（又称运行时数据区Runtime Data Area）分为六部分（参看第三章）：\n\n1. The pc Register\n2. Java Virtual Machine Stacks\n3. Heap\n4. Method Area\n5. Runtime Constant Pool\n6. Native Method Stacks；\n\n以上数据区的具体描述可参考规范。需要注意的是，以上只是一个规范说明，并没有规定虚拟机如何实现这些数据区。Sun JDK实现将内存空间划分为方法区、堆、本地方法栈、JVM方法栈、PC寄存器五部分。\n\n<!--more-->  \n\n如下图所示：\n\n![clip_image0026](/images/2012/02/clip_image0026.jpg)\n\n### 2、内存空间详解\n\n#### 1）PC寄存器和JVM方法栈\n\n每个线程都会拥有以及创建一个属于自己的PC寄存器和JVM方法栈，PC寄存器占用的有可能为CPU寄存器或者OS内存，而JVM栈占用的为OC内存。\n\n每运行一个方法，便会将方法的信息压入JVM方法栈中，同时将当前执行方法放入PC寄存器中（需要注意的是，如果当前方法为Native方法，PC寄存器的值为空）。可以想到，如果方法栈太深，如递归方法，便会报StackOverflowError，同样如果占用空间太多，也会报OutOfMemoryError。需要修改JVM参数设置：-Xss××k，在××中填入数字。\n\n#### 2）本地方法栈\n\n同JVM方法栈一样，本地方法栈存放的是native方法的调用的状态。在Sun JDK的实现中，本地方法栈和JVM方法栈是同一个。\n\n#### 3）方法区\n\n方法区存放了要加载的类的信息（名称、修饰符等）、类的静态变量、类中定义为fianl类型的常量、类中的Field信息、类中的方法信息，你用Class对象的方法，如getName()、getFields()等来获取信息时，这些数据都来自方法区。需要注意的是，Runtime Constant Pool（常量池）也存放在方法区中。\n\n方法区是被同一个JVM所有线程所共享的，在Sun JDK中这块区域对应Permanet Generation（持久代），默认最小值为16MB，最大值为64MB，可通过-XX:PermSize及-XX:MaxPermSize来指定。当方法区无法满足分配请求时，会报OutOfMemoryError。\n\n#### 4）堆\n\n堆用于存放对象实例以及数组值，可以认为所有通过new来创建的对象的内存均在此分配。一般所说的GC，大部分都是对堆进行的。\n\n堆在32位操作系统上最大为2GB，在64位的则没有限制，大小通过-Xms和-Xmx来控制。-Xms为JVM启动时申请的最小堆内存，默认为物理内存的1/64但小于1GB；-Xmx为JVM可申请的最大堆内存，默认为物理内存的1/4但小于1GB，默认当空余堆内存小于40%的时候，JVM会将堆增大到-Xmx指定大小，可通过-XX:MinHeapFreeRatio=来指定比例，空余堆大于70%时，会将堆大小降到-Xms指定大小，这个参数可用-XX:MaxHeapFreeRatio=来指定。但对于运行系统来说，会避免频繁调整堆大小，会将-Xms和-Xmx的值设为一样。\n\n为了让内存回收更加高效，Sun JDK从1.2开始对堆采取了分代管理的方法，如下图：\n\n![clip_image0046](/images/2012/02/clip_image0046.jpg)\n\n#### 4.1) 新生代（New Generation）\n\n大多数的新建对象都是从新生代中分配内存，新生代由Eden（伊甸园） Space和两块相同的Survivor Space（S0，S1或者From，To）构成。\n\n可通过-Xmn参数来指定新生代大小，-XX:SurvivorRatio来调整Eden与S Space的大小。\n\n#### 4.2）旧生代（Old Generation）\n\n用于存放新生代经过多次垃圾回收仍然存活的对象，像Cache。同时新建的对象也有可能在旧生代上直接分配内存，一般来说是比较的对象，即：单一大对象以及大数组，-XX:PretenureSizeThreshold = 1024 (byte, default = 0)可用来代表单一对象超过多大即不在新生代分配。\n\n旧生代所占内存大小为-Xmx-（-Xmn）。\n\n3、典型JVM参数配置汇总\n\n| 配置 | 解释 |\n| ----|-----|\n| -Xss××k | 方法栈深度 |\n|-XX:PermSize |方法区内存最小值|\n|-XX:MaxPermSize|方法区内存最大值|\n|-Xms|JVM启动分配最小堆内存|\n|-Xmx|JVM启动分配最大堆内存|\n|-XX:MinHeapFreeRatio=|堆内存需扩展时，剩余内存最小比例，默认40%|\n|-XX:MaxHeapFreeRatio=|堆内存需收缩时，剩余内存最大比例，默认70%|\n|-Xmn|堆新生代内存大小|\n|-XX:NewRatio=|如参数为4，则新生代与旧生代比例为1:4|\n|-XX:SurvivorRatio=|S0/S1占新生代内存的比例|\n|-XX:PretenureSizeThreshold=|需要内存超过参数的对象，直接在旧生代分配|\n|-XX:MaxTenuringThreshold=|设置垃圾最大年龄。如果为0，新生代对象不经过S区，直接进行旧生代，值较大的话，会增加新生代对象再GC的概率|\n\n> PretenureSizeThreshold 不一定完全生效，其中取决于 tlab 是否可用。tlab是每个线程在eden里分配的一块内存区域，主要为了提高内存分配效率。如果 tlab 可用的话，优先分配在 tlab 中。\n\n### 4、小结\n\n总的来说，所有语言的内存结构都大同小异，均分为堆、栈、区，堆放动态分配（alloc）的对象，栈存放临时变量、方法过程等，区则存放编译时确定的方法签名、常量池等。\n\nJVM的内存结构需要结合GC一起学习，有兴趣的可以参考<The Java™ Virtual Machine Specification>以及《分布式Java应用》这两本书。\n","slug":"2012/02/jvm-structure","published":1,"updated":"2015-12-30T12:16:30.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93o006l3x8fdo11rek9"},{"title":"迭代式MapReduce解决方案（一）","id":"250","date":"2012-02-08T15:11:00.000Z","_content":"\n### 一、迭代式Mapreduce简介\n\n普通的MapReduce任务是将一个任务分割成map与reduce两个阶段。map阶段负责过滤、筛选、检查输入数据，并将处理后的结果写入本地磁盘中；reduce阶段则负责远程读入map的本地输出结果，对数据进行归并、分析等处理，之后再将结果写入HDFS中。其数据流过程如下：\n\n <!--more-->  \n\n`(k, v) -> map -> (k1, v1), (k1, v2), (k2,v3) -> sort&shuffle -> (k1, list(v1, v2)), (k2, v3)`\n\n而迭代式的MapReduce任务需要迭代执行以上过程多次，由于每次任务都是独立的，则需要不断的读取、写入、传输数据，如果还是按照普通的MapReduce一样运行MR任务，性能将会非常低下。\n\n本文拿PageRank做一个例子，PageRank是Google的网页排名算法，是基于网页与网页之间的链接关系计算而得，计算过程需要不断的迭代（单次MR任务），获取一个新的PR值后，再继续迭代，直到两次迭代之间的PR差值小于某一个阈值即停止。\n\nPageRank计算数据分为两个部分：\n\n| URL | RANK | URL | OUT_LINK|\n|-----|------|-----|---------|\n|www.a.com|1|www.a.com|www.b.com|\n|www.b.com|1|www.a.com|www.c.com|\n|www.c.com|1|www.b.com|www.a.com|\n|www.d.com|1|www.b.com|www.c.com|\n|||www.c.com|www.d.com|\n|||www.d.com|www.b.com|\n|PR值表|-|网页链接关系表|-|\n\n### 二、问题分析说明\n\n迭代式作业的缺点很突出，在[这篇博客](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)有详细的介绍，本篇主要需要解决的问题是：**如何减少不必要的数据传输与读写**。\n\n正如前面所示，PageRank的计算数据分为了两种：PR值表以及网页链接关系表。其中PR值是随着迭代而不断变化，称之为动态数据；而网页链接关系，在计算中，不会有任何的改变，称之为静态数据。\n\n我现在能想到的，再参考了[网上](http://blog.xebia.com/2011/09/27/wiki-pagerank-with-hadoop/)的实现方式，基本上都是将静态数据与动态数据合并成一个文件，同时`读入(mapper)->写出(mapper)->传输(reducer)->写出(reducer)`。\n\n![image](/images/2012/02/image.png)\n\n我们可以来估算一下时间，先不考虑磁盘IO，仅算静态数据传输时间一项。其中模拟实验数据为：100w个链接地址；随机生成最多1000个外链；结果数据3.22G（动态数据8.5M）；实验环境网络带宽100M；迭代次数20次。\n\n单轮迭代，3.22G的数据会从mapper中读入再全部写入到本地磁盘，reducer再从mapper中将3.22G的临时数据传输到相应的taskTracker上。100M带宽的网络，传输速率约为10M/s，计算公式即为：3.22G×1024 / 10 = 330s = 5.5min。迭代20次，5.5×20 = 110min = 1.8h。简单的估算一下，3G左右的数据，在百兆带宽的网络环境，仅静态数据传输这一项就会占去近两小时（这是最坏情况，不考虑数据在本地的情况）！而网页数据远远不止3.22G，如果到了TB乃至PB级的话，耗时应该就不是开发者所能接受的了。\n\n### 三、问题解决方法\n\n为了减少不必要数据的传输与读写，开发者就一定要做到以下几点：\n\n1. 将静态数据与动态数据分离，但需要保证在一次（以及下一次）迭代中，结合动静数据；\n2. 输出结果中尽量减少数据量，原则上只能有动态数据，不能包含静态数据。\n\n每次map过程中，都需要读入一行PR值表元组，同时也要读入多行对应的链接关系表元组，虽然在map中无法控制两个分离文件的读入顺序，但我们可以预先将动态数据加载进内存作为索引，读入一行后，再查找内存获取需要的数据。这样的方式很容易的就可以想到分布式缓存技术，先前我还在考虑是用Memocached还是Redis，但多看看后好像是多此一举了。MapReduce自带了Distributed Cache技术，可以参见《[Mapreduce API文档](http://hadoop.apache.org/common/docs/r0.20.203.0/api/org/apache/hadoop/filecache/DistributedCache.html)》。\n\nHaoop中自带的分布式缓存，即DistributedCache对象，可以方便map task之间或者reduce task之间共享一些信息，缓存数据会被分发到集群的所有节点上。需要注意的是，DistributedCache是read-only的。\n\n操作步骤：\n\n1. 将数据分发到每个节点上：\n\n`DistributedCache.addCacheFile(new Path(args[0]).toUri(), conf);`\n\n2. 在每个mapper上获取cache文件，便可加载进内存：\n\n`DistributedCache.getLocalCacheFiles(conf);`\n\n3. Reducer写出动态数据，下一次迭代中，再将新的动态数据加载至DistributedCache中。\n\n将动态数据作为缓存文件的后，整个迭代过程，只有大量减少磁盘IO，且在很大程度上减少了网络带宽负荷与无效数据传输时间。\n\n### 四、总结\n\n以上的方法理论上支持大多数迭代式Mapreduce模型，如pagerank、SSSP（Single Source Shortest Path）等。参考: [董的博客](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)，再加上自己的实践，提出以下一些问题：\n\n**（1） 每次迭代，如果所有task重复重新创建，代价将非常高。怎样重用task以提高效率（task pool）？**\n\n说明： hadoop自身提供了task JVM reuse的功能。不过该功能仅限于同一个Job内，而我们每次迭代都会重新运行一个job，故自带功能不适用（或者我还不会用）。但是我们可否考虑job复用呢？\n\n**（2） 何时迭代终止，怎样改变编程模型，允许用户指定合适终止迭代。**\n\n说明：就PageRank来说，迭代中止的条件是每次迭代结果相差小于一个阈值，即PR结果达到平衡。我们就可以将前一次结果直接输出到Reducer中，或者可以从DistributedCache读取前一次PR值，并做判断。\n\n但是一个PR结果符合条件并不能说明任务就结束了，需要所有的（或者说大多数）的结果均满足条件才能中止任务。那么，这个大多数结果满足条件的数据该怎么存放以及读取呢？还有就是，怎么找到一个通过的编程模型去适应其它的迭代式MR任务呢？\n\n**（3）就算没有静态数据，动态数据生成也不小**\n\n100W行数据3.22G，64M的split有52个，每个2W行数据。由于是随机生成的，平均每行500个链接地址，每个连接地址都会生成一行临时结果&lt;URL_ID AER_PR&gt;，估算一下也有150M（实际140M），那么3.22数据，最后生成临时数据为7G+。\n\n如不加任何优化的话，那铁定是不行的。后面的文章再说说优化问题，在这个实验环境下，可将7G的文件压缩到不到300M。\n\n**（4）DistributedCache API的使用**\n\n一直觉得Hadoop的版本管理十分混乱，新旧API杂乱，文档不更新！所以DistributedCache API一直没用好，到时候整理一下，顺带说说如何添加第三方jar包。\n\n以上的讨论还待我的继续研究了，性能分析比较以后的文章给填上。如对迭代式MapReduce任务感兴趣的童鞋可以参考一下Apache开源项目[Mahout](http://mahout.apache.org/)，还有Google的一篇论文&lt;Pregel: A System for Large-Scale Graph Processing&gt;：[中文](http://blog.csdn.net/ae86_fc/article/details/5796640)；[英文](http://kowshik.github.com/JPregel/pregel_paper.pdf)。\n\n> **参考资料：**\n>\n> [迭代式MapReduce框架介绍](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)\n> [MapReduce Tutorial](http://hadoop.apache.org/common/docs/r0.20.203.0/mapred_tutorial.html)\n","source":"_posts/2012/02/iterative-mapred.md","raw":"title: 迭代式MapReduce解决方案（一）\ntags:\n  - Hadoop\n  - MapReduce\nid: 250\ncategories:\n  - 技术分享\ndate: 2012-02-08 23:11:00\n---\n\n### 一、迭代式Mapreduce简介\n\n普通的MapReduce任务是将一个任务分割成map与reduce两个阶段。map阶段负责过滤、筛选、检查输入数据，并将处理后的结果写入本地磁盘中；reduce阶段则负责远程读入map的本地输出结果，对数据进行归并、分析等处理，之后再将结果写入HDFS中。其数据流过程如下：\n\n <!--more-->  \n\n`(k, v) -> map -> (k1, v1), (k1, v2), (k2,v3) -> sort&shuffle -> (k1, list(v1, v2)), (k2, v3)`\n\n而迭代式的MapReduce任务需要迭代执行以上过程多次，由于每次任务都是独立的，则需要不断的读取、写入、传输数据，如果还是按照普通的MapReduce一样运行MR任务，性能将会非常低下。\n\n本文拿PageRank做一个例子，PageRank是Google的网页排名算法，是基于网页与网页之间的链接关系计算而得，计算过程需要不断的迭代（单次MR任务），获取一个新的PR值后，再继续迭代，直到两次迭代之间的PR差值小于某一个阈值即停止。\n\nPageRank计算数据分为两个部分：\n\n| URL | RANK | URL | OUT_LINK|\n|-----|------|-----|---------|\n|www.a.com|1|www.a.com|www.b.com|\n|www.b.com|1|www.a.com|www.c.com|\n|www.c.com|1|www.b.com|www.a.com|\n|www.d.com|1|www.b.com|www.c.com|\n|||www.c.com|www.d.com|\n|||www.d.com|www.b.com|\n|PR值表|-|网页链接关系表|-|\n\n### 二、问题分析说明\n\n迭代式作业的缺点很突出，在[这篇博客](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)有详细的介绍，本篇主要需要解决的问题是：**如何减少不必要的数据传输与读写**。\n\n正如前面所示，PageRank的计算数据分为了两种：PR值表以及网页链接关系表。其中PR值是随着迭代而不断变化，称之为动态数据；而网页链接关系，在计算中，不会有任何的改变，称之为静态数据。\n\n我现在能想到的，再参考了[网上](http://blog.xebia.com/2011/09/27/wiki-pagerank-with-hadoop/)的实现方式，基本上都是将静态数据与动态数据合并成一个文件，同时`读入(mapper)->写出(mapper)->传输(reducer)->写出(reducer)`。\n\n![image](/images/2012/02/image.png)\n\n我们可以来估算一下时间，先不考虑磁盘IO，仅算静态数据传输时间一项。其中模拟实验数据为：100w个链接地址；随机生成最多1000个外链；结果数据3.22G（动态数据8.5M）；实验环境网络带宽100M；迭代次数20次。\n\n单轮迭代，3.22G的数据会从mapper中读入再全部写入到本地磁盘，reducer再从mapper中将3.22G的临时数据传输到相应的taskTracker上。100M带宽的网络，传输速率约为10M/s，计算公式即为：3.22G×1024 / 10 = 330s = 5.5min。迭代20次，5.5×20 = 110min = 1.8h。简单的估算一下，3G左右的数据，在百兆带宽的网络环境，仅静态数据传输这一项就会占去近两小时（这是最坏情况，不考虑数据在本地的情况）！而网页数据远远不止3.22G，如果到了TB乃至PB级的话，耗时应该就不是开发者所能接受的了。\n\n### 三、问题解决方法\n\n为了减少不必要数据的传输与读写，开发者就一定要做到以下几点：\n\n1. 将静态数据与动态数据分离，但需要保证在一次（以及下一次）迭代中，结合动静数据；\n2. 输出结果中尽量减少数据量，原则上只能有动态数据，不能包含静态数据。\n\n每次map过程中，都需要读入一行PR值表元组，同时也要读入多行对应的链接关系表元组，虽然在map中无法控制两个分离文件的读入顺序，但我们可以预先将动态数据加载进内存作为索引，读入一行后，再查找内存获取需要的数据。这样的方式很容易的就可以想到分布式缓存技术，先前我还在考虑是用Memocached还是Redis，但多看看后好像是多此一举了。MapReduce自带了Distributed Cache技术，可以参见《[Mapreduce API文档](http://hadoop.apache.org/common/docs/r0.20.203.0/api/org/apache/hadoop/filecache/DistributedCache.html)》。\n\nHaoop中自带的分布式缓存，即DistributedCache对象，可以方便map task之间或者reduce task之间共享一些信息，缓存数据会被分发到集群的所有节点上。需要注意的是，DistributedCache是read-only的。\n\n操作步骤：\n\n1. 将数据分发到每个节点上：\n\n`DistributedCache.addCacheFile(new Path(args[0]).toUri(), conf);`\n\n2. 在每个mapper上获取cache文件，便可加载进内存：\n\n`DistributedCache.getLocalCacheFiles(conf);`\n\n3. Reducer写出动态数据，下一次迭代中，再将新的动态数据加载至DistributedCache中。\n\n将动态数据作为缓存文件的后，整个迭代过程，只有大量减少磁盘IO，且在很大程度上减少了网络带宽负荷与无效数据传输时间。\n\n### 四、总结\n\n以上的方法理论上支持大多数迭代式Mapreduce模型，如pagerank、SSSP（Single Source Shortest Path）等。参考: [董的博客](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)，再加上自己的实践，提出以下一些问题：\n\n**（1） 每次迭代，如果所有task重复重新创建，代价将非常高。怎样重用task以提高效率（task pool）？**\n\n说明： hadoop自身提供了task JVM reuse的功能。不过该功能仅限于同一个Job内，而我们每次迭代都会重新运行一个job，故自带功能不适用（或者我还不会用）。但是我们可否考虑job复用呢？\n\n**（2） 何时迭代终止，怎样改变编程模型，允许用户指定合适终止迭代。**\n\n说明：就PageRank来说，迭代中止的条件是每次迭代结果相差小于一个阈值，即PR结果达到平衡。我们就可以将前一次结果直接输出到Reducer中，或者可以从DistributedCache读取前一次PR值，并做判断。\n\n但是一个PR结果符合条件并不能说明任务就结束了，需要所有的（或者说大多数）的结果均满足条件才能中止任务。那么，这个大多数结果满足条件的数据该怎么存放以及读取呢？还有就是，怎么找到一个通过的编程模型去适应其它的迭代式MR任务呢？\n\n**（3）就算没有静态数据，动态数据生成也不小**\n\n100W行数据3.22G，64M的split有52个，每个2W行数据。由于是随机生成的，平均每行500个链接地址，每个连接地址都会生成一行临时结果&lt;URL_ID AER_PR&gt;，估算一下也有150M（实际140M），那么3.22数据，最后生成临时数据为7G+。\n\n如不加任何优化的话，那铁定是不行的。后面的文章再说说优化问题，在这个实验环境下，可将7G的文件压缩到不到300M。\n\n**（4）DistributedCache API的使用**\n\n一直觉得Hadoop的版本管理十分混乱，新旧API杂乱，文档不更新！所以DistributedCache API一直没用好，到时候整理一下，顺带说说如何添加第三方jar包。\n\n以上的讨论还待我的继续研究了，性能分析比较以后的文章给填上。如对迭代式MapReduce任务感兴趣的童鞋可以参考一下Apache开源项目[Mahout](http://mahout.apache.org/)，还有Google的一篇论文&lt;Pregel: A System for Large-Scale Graph Processing&gt;：[中文](http://blog.csdn.net/ae86_fc/article/details/5796640)；[英文](http://kowshik.github.com/JPregel/pregel_paper.pdf)。\n\n> **参考资料：**\n>\n> [迭代式MapReduce框架介绍](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)\n> [MapReduce Tutorial](http://hadoop.apache.org/common/docs/r0.20.203.0/mapred_tutorial.html)\n","slug":"2012/02/iterative-mapred","published":1,"updated":"2015-12-29T15:39:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93q006o3x8fq7frllex"},{"title":"迭代式MapReduce解决方案（三）","id":"401","date":"2012-02-29T15:14:46.000Z","_content":"\n### 1、前言\n\n前面两篇[（一）](http://www.hongweiyi.com/2012/02/mapred-optimize/)[（二）](http://www.hongweiyi.com/2012/02/iterative-mapred-distcache/)解决方案分别从静态数据（Invariant Data）分离以及分布式缓存来优化迭代式Mapreduce，但是由于Mapreduce天生的缺陷，再加上分布式缓存是分布存放在本地磁盘的，没有一个好的读取方案的话，就会大大提高了每个task的磁盘IO次数。这篇博客算是迭代式Mapreduce的收尾了，来整体分析一下我的解决方案和Haloop方案吧。\n\n<!--more-->\n\n### 2、现存框架的缺陷&我的方案\n\nHaloop发布的文献中，说了两个缺陷，再加上董的一个，共仨：\n\n1）动静态数据无法分离，浪费大量资源（磁盘IO，网络带宽，CPU时间），Haloop原文：The first problem is that even though much of the data may be unchanged from iteration to iteration, the data must be re-loaded and re-processed at each iteration, wasting I/O, network bandwidth, and CPU resources。\n\n我的解决方案：利用分布式缓存来缓存动态数据，可以有效的减少临时数据大小，大量的减少网络带宽压力（10G-&gt;0.25G）。但是，通过我的方案，磁盘IO虽然有所下降，但是仍然有待加强的地方，因磁盘IO主要集中在了map task的read阶段，而在坏的情况下，有可能会从其它node远程读取。Haloop修改了一下这种方式，我待会儿说方法。\n\n2）没有一个客观的停止迭代的标准，Haloop原文：The second problem is that the termination condition may involve detecting when a fixpoint has been reached。\n\n我的解决方案：这个方案没有写成博客，因为觉得太普通了。和大多数应用一样，开启一个新的任务，来计算两次迭代之间的差别，Pagerank计算两次迭代过程之间所有页面PR差之和，SSSP计算所有点的状态。但是需要注意的是，由于一个文件就会开启一个map task，所以需要动脑筋思考一下如何“合并”起来。\n\n3）每次迭代，如果所有task重复重新创建，代价将非常高。怎样重用task以提高效率（task pool）。这个缺陷是[董的博客](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)提出的，这个虽然没有在Haloop中单独提出，但是实现中已经考虑到了。这个在现有框架下，基本是上没可能了，迭代式Mapreduce需要解决的是Job复用的问题，整个task pool就得修改框架了。\n\n### 3、Haloop解决方案\n\n![clip_image002](/images/2012/02/clip_image0024.jpg)\n\n\n![clip_image004](/images/2012/02/clip_image0043.jpg)\n\n\nHaloop进行的改进有：\n\n#### 1）提供了一套新的编程接口，以方便用户进行迭代式程序开发\n\nHaloop提供了一些有用的方法，如下：\n\n* SetFixedPointThreshold：设置两次迭代的终止条件，即距离差是否达到某一个阈值\n* ResultDistance：计算两次距离的方法\n* setMaxNumOfIterations：设置迭代次数\n* setIterationInput：设置变化的输入数据\n* AddInvariantTable：设置不变的输入数据  \n\n有了上面的方法，整个迭代式MR过程很清晰，确实提供了很大的方便。\n\n#### 2） master node（jobtracker）包含一个循环控制模块，它不断的启动map-reduce计算知道满足迭代终止条件\n\n从Haloop计算流程图可以看出，Haloop基本实现了job“复用”，只有一个job就可以了，它可以开启多个map/reduce对，而传统的每次迭代过程都需要开启一个job，且一个job只有一个map/reduce对。且迭代终止条件控制在job内部，无需再启新job来计算。\n\n#### 3）设计了新的Task Scheduler，以便更好的利用data locality特性\n\nHaloop有一个Loop-aware 任务调度机制。Haloop在首次迭代时会将不变的输入数据保存到相应计算节点上，以后每次调度task，尽量放在固定的那些节点上（locality）。这样，每次迭代，不变的数据就不必重复传输了。\n\n#### 4）数据在各个task tracker会被缓存（cache）和建索引（index）\n\nMap task的输入与输出，Reduce task的输出都会被建索引和缓存，以加快数据处理速度。这个部分在论文中占的大量份额，所以我也没有仔细看，整体来说就是和分布式缓存有异曲同工之妙。需要说明的是，缓存是指数据被写到本次磁盘，以供下一轮循环迭代时直接使用，Haloop也并没有完全存入内存，应该是担心内存不够使的。\n\n### 4、总结\n\n迭代式Mapreduce还有待继续研究，按照董的说法，haloop模型抽象还不够高，支持计算模型有限，而现有的解决方案都不是最优的。我所提出的方案只是在不修改源码的情况下，最大限度的优化计算过程，还是不够优！不过Yahoo!要推出下一代的Mapreduce，从它发表的文章来看，解决迭代式问题好似有戏，可以参考这里：[The Next Generation of Apache Hadoop MapReduce](http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/)。\n\n **参考资料**\n>\n> 1. Haloop主页：[http://code.google.com/p/haloop/](http://code.google.com/p/haloop/)\n>\n> 2. 董的博客：[http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)\n","source":"_posts/2012/02/iterative-mapred-summary-haloop.md","raw":"title: 迭代式MapReduce解决方案（三）\ntags:\n  - Hadoop\n  - Haloop\n  - MapReduce\nid: 401\ncategories:\n  - 技术分享\ndate: 2012-02-29 23:14:46\n---\n\n### 1、前言\n\n前面两篇[（一）](http://www.hongweiyi.com/2012/02/mapred-optimize/)[（二）](http://www.hongweiyi.com/2012/02/iterative-mapred-distcache/)解决方案分别从静态数据（Invariant Data）分离以及分布式缓存来优化迭代式Mapreduce，但是由于Mapreduce天生的缺陷，再加上分布式缓存是分布存放在本地磁盘的，没有一个好的读取方案的话，就会大大提高了每个task的磁盘IO次数。这篇博客算是迭代式Mapreduce的收尾了，来整体分析一下我的解决方案和Haloop方案吧。\n\n<!--more-->\n\n### 2、现存框架的缺陷&我的方案\n\nHaloop发布的文献中，说了两个缺陷，再加上董的一个，共仨：\n\n1）动静态数据无法分离，浪费大量资源（磁盘IO，网络带宽，CPU时间），Haloop原文：The first problem is that even though much of the data may be unchanged from iteration to iteration, the data must be re-loaded and re-processed at each iteration, wasting I/O, network bandwidth, and CPU resources。\n\n我的解决方案：利用分布式缓存来缓存动态数据，可以有效的减少临时数据大小，大量的减少网络带宽压力（10G-&gt;0.25G）。但是，通过我的方案，磁盘IO虽然有所下降，但是仍然有待加强的地方，因磁盘IO主要集中在了map task的read阶段，而在坏的情况下，有可能会从其它node远程读取。Haloop修改了一下这种方式，我待会儿说方法。\n\n2）没有一个客观的停止迭代的标准，Haloop原文：The second problem is that the termination condition may involve detecting when a fixpoint has been reached。\n\n我的解决方案：这个方案没有写成博客，因为觉得太普通了。和大多数应用一样，开启一个新的任务，来计算两次迭代之间的差别，Pagerank计算两次迭代过程之间所有页面PR差之和，SSSP计算所有点的状态。但是需要注意的是，由于一个文件就会开启一个map task，所以需要动脑筋思考一下如何“合并”起来。\n\n3）每次迭代，如果所有task重复重新创建，代价将非常高。怎样重用task以提高效率（task pool）。这个缺陷是[董的博客](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)提出的，这个虽然没有在Haloop中单独提出，但是实现中已经考虑到了。这个在现有框架下，基本是上没可能了，迭代式Mapreduce需要解决的是Job复用的问题，整个task pool就得修改框架了。\n\n### 3、Haloop解决方案\n\n![clip_image002](/images/2012/02/clip_image0024.jpg)\n\n\n![clip_image004](/images/2012/02/clip_image0043.jpg)\n\n\nHaloop进行的改进有：\n\n#### 1）提供了一套新的编程接口，以方便用户进行迭代式程序开发\n\nHaloop提供了一些有用的方法，如下：\n\n* SetFixedPointThreshold：设置两次迭代的终止条件，即距离差是否达到某一个阈值\n* ResultDistance：计算两次距离的方法\n* setMaxNumOfIterations：设置迭代次数\n* setIterationInput：设置变化的输入数据\n* AddInvariantTable：设置不变的输入数据  \n\n有了上面的方法，整个迭代式MR过程很清晰，确实提供了很大的方便。\n\n#### 2） master node（jobtracker）包含一个循环控制模块，它不断的启动map-reduce计算知道满足迭代终止条件\n\n从Haloop计算流程图可以看出，Haloop基本实现了job“复用”，只有一个job就可以了，它可以开启多个map/reduce对，而传统的每次迭代过程都需要开启一个job，且一个job只有一个map/reduce对。且迭代终止条件控制在job内部，无需再启新job来计算。\n\n#### 3）设计了新的Task Scheduler，以便更好的利用data locality特性\n\nHaloop有一个Loop-aware 任务调度机制。Haloop在首次迭代时会将不变的输入数据保存到相应计算节点上，以后每次调度task，尽量放在固定的那些节点上（locality）。这样，每次迭代，不变的数据就不必重复传输了。\n\n#### 4）数据在各个task tracker会被缓存（cache）和建索引（index）\n\nMap task的输入与输出，Reduce task的输出都会被建索引和缓存，以加快数据处理速度。这个部分在论文中占的大量份额，所以我也没有仔细看，整体来说就是和分布式缓存有异曲同工之妙。需要说明的是，缓存是指数据被写到本次磁盘，以供下一轮循环迭代时直接使用，Haloop也并没有完全存入内存，应该是担心内存不够使的。\n\n### 4、总结\n\n迭代式Mapreduce还有待继续研究，按照董的说法，haloop模型抽象还不够高，支持计算模型有限，而现有的解决方案都不是最优的。我所提出的方案只是在不修改源码的情况下，最大限度的优化计算过程，还是不够优！不过Yahoo!要推出下一代的Mapreduce，从它发表的文章来看，解决迭代式问题好似有戏，可以参考这里：[The Next Generation of Apache Hadoop MapReduce](http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/)。\n\n **参考资料**\n>\n> 1. Haloop主页：[http://code.google.com/p/haloop/](http://code.google.com/p/haloop/)\n>\n> 2. 董的博客：[http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/](http://dongxicheng.org/mapreduce/iterative-mapreduce-intro/)\n","slug":"2012/02/iterative-mapred-summary-haloop","published":1,"updated":"2015-12-29T15:41:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93r006s3x8f6xj4ufr4"},{"title":"迭代式MapReduce解决方案（二） DistributedCache","id":"337","date":"2012-02-23T08:01:52.000Z","_content":"\n### 1、DistributedCache In Hadoop\n\n此篇文章主要是[前一篇](http://www.hongweiyi.com/?p=250)的后续，主要讲Hadoop的分布式缓存机制的原理与运用。\n\n分布式缓存在MapReduce中称之为DistributedCache，它可以方便map task之间或者reduce task之间共享一些信息，同时也可以将第三方包添加到其classpath路径中去。Hadoop会将缓存数据分发到集群的所有准备启动的节点上，复制到在mapred.temp.dir中配置的目录。\n\n <!--more-->  \n\n### 2、DistributedCache的使用\n\nDistributedCache的使用的本质其实是添加Configuraton中的属性：mapred.cache.{files|archives}。图方便的话，可以使用DistributedCache类的静态方法。\n\n不省事法：\n\n```\nconf.set(\"mapred.cache.files\", \"/data/data\");\nconf.set(\"mapred.cache. archives\", \"/data/data.zip\");\n```\n\n省事法：\n\n* [DistributedCache](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html). `**[addCacheFile](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html#addCacheFile(java.net.URI, org.apache.hadoop.conf.Configuration))**``([URI](http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true),` `[Configuration](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/conf/Configuration.html))`\n* [DistributedCache](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html).`**[addArchiveToClassPath](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem))**``([Path](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/Path.html),` `[Configuration](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/conf/Configuration.html),` `[FileSystem](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/FileSystem.html))`\n\n\n需要注意的是，上面几行代码需要写在Job类初始化之前，否则在运行会中找不到文件（被折磨了很长时间），因为Job初始化时将传入Configuration对象克隆一份给了JobContext。\n\n在MapReduce的0.21版本以后的org.apache.hadoop.mapreduce均移到org.apache.hadoop.mapred包下。但文档中提供的configure方法是重写的MapReduceBase中的，而新版本中map继承于mapper，reduce继承于reducer，所以configure方法一律改成了setup。要获得cache数据，就得在map/reduce task中的setup方法中取得cache数据，再进行相应操作：  \n\n``` java\n@Override\nprotected void setup(Context context) throws IOException,  \n        InterruptedException {  \n    super.setup(context);  \n    URI[] uris = DistributedCache.getCacheFiles(context  \n                .getConfiguration());  \n    Path[] paths = DistributedCache.getLocalCacheFiles(context  \n                .getConfiguration());  \n    // TODO  \n}  \n```\n\n而三方库的使用稍微简单，只需要将库上传至hdfs，再用代码添加至classpath即可：\n\n```\nDistributedCache.addArchiveToClassPath(new Path(\"/data/test.jar\"), conf);\n```\n\n### 3、symlink的使用\n\nSymlink其实就是hdfs文件的一个快捷方式，只需要在路径名后加入#linkname，之后在task中使用linkname即使用相应文件，如下：\n\n```\nconf.set(\"mapred.cache.files\", \"/data/data#mData\");\nconf.set(\"mapred.cache. archives\", \"/data/data.zip#mDataZip\");\n```\n\n``` java\n@Override\nprotected void setup(Context context) throws IOException,  \n        InterruptedException {  \n    super.setup(context);  \n    FileReader reader = new FileReader(new File(\"mData\"));  \n    BufferedReader bReader = new BufferedReader(reader);  \n    // TODO  \n}\n```\n\n在使用symlink之前，需要告知hadoop，如下：\n\n* [DistributedCache.createSymlink(Configuration)](/images/2012/02/DistributedCache.html)\n\n### 4、注意事项\n\n1. 缓存文件（数据、三方库）需上传至HDFS，方能使用；\n2. 存较小的情况下，建议将数据全部读入相应节点内存，提高访问速度；\n3. 缓存文件是read-only的，不能修改。若要修改得重新输出，将新输出文件作为新缓存进入下一次迭代。\n","source":"_posts/2012/02/iterative-mapred-distcache.md","raw":"title: 迭代式MapReduce解决方案（二） DistributedCache\ntags:\n  - Hadoop\n  - MapReduce\nid: 337\ncategories:\n  - 技术分享\ndate: 2012-02-23 16:01:52\n---\n\n### 1、DistributedCache In Hadoop\n\n此篇文章主要是[前一篇](http://www.hongweiyi.com/?p=250)的后续，主要讲Hadoop的分布式缓存机制的原理与运用。\n\n分布式缓存在MapReduce中称之为DistributedCache，它可以方便map task之间或者reduce task之间共享一些信息，同时也可以将第三方包添加到其classpath路径中去。Hadoop会将缓存数据分发到集群的所有准备启动的节点上，复制到在mapred.temp.dir中配置的目录。\n\n <!--more-->  \n\n### 2、DistributedCache的使用\n\nDistributedCache的使用的本质其实是添加Configuraton中的属性：mapred.cache.{files|archives}。图方便的话，可以使用DistributedCache类的静态方法。\n\n不省事法：\n\n```\nconf.set(\"mapred.cache.files\", \"/data/data\");\nconf.set(\"mapred.cache. archives\", \"/data/data.zip\");\n```\n\n省事法：\n\n* [DistributedCache](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html). `**[addCacheFile](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html#addCacheFile(java.net.URI, org.apache.hadoop.conf.Configuration))**``([URI](http://java.sun.com/javase/6/docs/api/java/net/URI.html?is-external=true),` `[Configuration](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/conf/Configuration.html))`\n* [DistributedCache](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html).`**[addArchiveToClassPath](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/filecache/DistributedCache.html#addArchiveToClassPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem))**``([Path](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/Path.html),` `[Configuration](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/conf/Configuration.html),` `[FileSystem](http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/FileSystem.html))`\n\n\n需要注意的是，上面几行代码需要写在Job类初始化之前，否则在运行会中找不到文件（被折磨了很长时间），因为Job初始化时将传入Configuration对象克隆一份给了JobContext。\n\n在MapReduce的0.21版本以后的org.apache.hadoop.mapreduce均移到org.apache.hadoop.mapred包下。但文档中提供的configure方法是重写的MapReduceBase中的，而新版本中map继承于mapper，reduce继承于reducer，所以configure方法一律改成了setup。要获得cache数据，就得在map/reduce task中的setup方法中取得cache数据，再进行相应操作：  \n\n``` java\n@Override\nprotected void setup(Context context) throws IOException,  \n        InterruptedException {  \n    super.setup(context);  \n    URI[] uris = DistributedCache.getCacheFiles(context  \n                .getConfiguration());  \n    Path[] paths = DistributedCache.getLocalCacheFiles(context  \n                .getConfiguration());  \n    // TODO  \n}  \n```\n\n而三方库的使用稍微简单，只需要将库上传至hdfs，再用代码添加至classpath即可：\n\n```\nDistributedCache.addArchiveToClassPath(new Path(\"/data/test.jar\"), conf);\n```\n\n### 3、symlink的使用\n\nSymlink其实就是hdfs文件的一个快捷方式，只需要在路径名后加入#linkname，之后在task中使用linkname即使用相应文件，如下：\n\n```\nconf.set(\"mapred.cache.files\", \"/data/data#mData\");\nconf.set(\"mapred.cache. archives\", \"/data/data.zip#mDataZip\");\n```\n\n``` java\n@Override\nprotected void setup(Context context) throws IOException,  \n        InterruptedException {  \n    super.setup(context);  \n    FileReader reader = new FileReader(new File(\"mData\"));  \n    BufferedReader bReader = new BufferedReader(reader);  \n    // TODO  \n}\n```\n\n在使用symlink之前，需要告知hadoop，如下：\n\n* [DistributedCache.createSymlink(Configuration)](/images/2012/02/DistributedCache.html)\n\n### 4、注意事项\n\n1. 缓存文件（数据、三方库）需上传至HDFS，方能使用；\n2. 存较小的情况下，建议将数据全部读入相应节点内存，提高访问速度；\n3. 缓存文件是read-only的，不能修改。若要修改得重新输出，将新输出文件作为新缓存进入下一次迭代。\n","slug":"2012/02/iterative-mapred-distcache","published":1,"updated":"2015-12-29T15:45:05.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93s006y3x8f95j31wof"},{"title":"Hadoop源码 - ipc.Server","id":"387","date":"2012-02-27T04:57:25.000Z","_content":"\n### 1、前言\n\n昨天分析了ipc包下的RPC、Client类，今天来分析下ipc.Server。Server类因为是Hadoop自己使用，所以代码结构以及流程都很清晰，可以清楚的看到实例化、停止、运行等过程。\n\n<!--more-->\n\n### 2、Server类结构\n\n上面是Server的五个内部类，分别介绍一下：\n\n1）Call\n\n用以存储客户端发来的请求，这个请求会放入一个BlockQueue中；\n\n2）Listener\n\n监听类，用以监听客户端发来的请求。同时Listener下面还有一个静态类，Listener.Reader，当监听器监听到用户请求，便用让Reader读取用户请求。\n\n3）Responder\n\n响应RPC请求类，请求处理完毕，由Responder发送给请求客户端。\n\n4）Connection\n\n连接类，真正的客户端请求读取逻辑在这个类中。\n\n5）Handler\n\n请求（blockQueueCall）处理类，会循环阻塞读取callQueue中的call对象，并对其进行操作。\n\n3、Server初始化\n\n第一篇[博客](http://www.hongweiyi.com/2012/02/hadoop-ipc-rpc/)说了，Server的初始化入口在RPC.getServer中，getServer其实是调用的RPC.Server静态类中的构造方法，我们看看Namenode创建RPCServer的方法和RPC.Server构造方法代码：\n\n``` java\nprivate void initialize(Configuration conf) throws IOException {  \n    …  \n    this.serviceRpcServer = RPC.getServer(this, dnSocketAddr.getHostName(),   \n          dnSocketAddr.getPort(), serviceHandlerCount,  \n          false, conf, namesystem.getDelegationTokenSecretManager());  \n    this.serviceRpcServer.start(); // 运行服务器  \n}  \n```         \n\n``` java\npublic Server(Object instance, Configuration conf, String bindAddress,  int port,  \n                  int numHandlers, boolean verbose,   \n                  SecretManager<? extends TokenIdentifier> secretManager)   \n        throws IOException {  \n      super(bindAddress, port, Invocation.class, numHandlers, conf,  \n          classNameBase(instance.getClass().getName()), secretManager);  \n      this.instance = instance;  \n      this.verbose = verbose;  \n}  \n```\n\n该方法调用了父类的构造方法，如下：\n\n``` java\nprotected Server(String bindAddress, int port,   \n                  Class<? extends Writable> paramClass, int handlerCount,   \n                  Configuration conf, String serverName, SecretManager<? extends TokenIdentifier> secretManager)   \n    throws IOException {  \n    this.bindAddress = bindAddress;  \n    this.conf = conf;  \n    this.port = port;  \n    this.paramClass = paramClass;  \n    this.handlerCount = handlerCount;  \n    this.socketSendBufferSize = 0;  \n    this.maxQueueSize = handlerCount * conf.getInt(  \n                                IPC_SERVER_HANDLER_QUEUE_SIZE_KEY,  \n                                IPC_SERVER_HANDLER_QUEUE_SIZE_DEFAULT);  \n    this.maxRespSize = conf.getInt(IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY,  \n                                   IPC_SERVER_RPC_MAX_RESPONSE_SIZE_DEFAULT);  \n    this.readThreads = conf.getInt(  \n        IPC_SERVER_RPC_READ_THREADS_KEY,  \n        IPC_SERVER_RPC_READ_THREADS_DEFAULT);  \n    this.callQueue  = new LinkedBlockingQueue<Call>(maxQueueSize);   \n    this.maxIdleTime = 2*conf.getInt(\"ipc.client.connection.maxidletime\", 1000);  \n    this.maxConnectionsToNuke = conf.getInt(\"ipc.client.kill.max\", 10);  \n    this.thresholdIdleConnections = conf.getInt(\"ipc.client.idlethreshold\", 4000);  \n    this.secretManager = (SecretManager<TokenIdentifier>) secretManager;  \n    this.authorize =   \n      conf.getBoolean(HADOOP_SECURITY_AUTHORIZATION, false);  \n    this.isSecurityEnabled = UserGroupInformation.isSecurityEnabled();  \n\n    // Start the listener here and let it bind to the port  \n    listener = new Listener();  \n    this.port = listener.getAddress().getPort();      \n    this.rpcMetrics = RpcInstrumentation.create(serverName, this.port);  \n    this.tcpNoDelay = conf.getBoolean(\"ipc.server.tcpnodelay\", false);  \n\n    // Create the responder here  \n    responder = new Responder();  \n\n    if (isSecurityEnabled) {  \n      SaslRpcServer.init(conf);  \n    }  \n}  \n```\n\n不难看出，父类的构造方法就初始化了一些配置和变量。\n\n### 4、Server运行\n\n在上面第一段代码中，还有一句RpcServer.start()的方法，在调用构造函数初始化一些变量之后，Server就可以正式运行起来了：\n\n``` java\npublic synchronized void start() {  \n    responder.start();  \n    listener.start();  \n    handlers = new Handler[handlerCount];  \n\n    for (int i = 0; i < handlerCount; i++) {  \n      handlers[i] = new Handler(i);  \n      handlers[i].start();  \n    }  \n}\n```\n\nresponder、listener、handlers三个对象的线程均阻塞了，前两个阻塞在selector.select()方法上，handler阻塞在callQueue.take()方法，都在等待客户端请求。Responder设置了超时时间，为15分钟。而listener还开启了Reader线程，该线程也阻塞了。\n\n### 4、Server接受请求流程\n\n1）监听到请求\n\nListener监听到请求，获得所有请求的SelectionKey，执行doAccept(key)方法，该方法将所有的连接对象放入list中，并将connection对象与key绑定，以供reader使用。初始化玩所有的conne对象之后，就可以激活Reader线程了。\n\n``` java\nvoid doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {  \n      Connection c = null;  \n      ServerSocketChannel server = (ServerSocketChannel) key.channel();  \n      SocketChannel channel;  \n      while ((channel = server.accept()) != null) {  \n        channel.configureBlocking(false);  \n        channel.socket().setTcpNoDelay(tcpNoDelay);  \n        Reader reader = getReader();  \n        try {  \n          reader.startAdd();  // 激活readSelector，设置adding为true  \n          SelectionKey readKey = reader.registerChannel(channel);  \n          c = new Connection(readKey, channel, System.currentTimeMillis());  \n          readKey.attach(c);  \n          synchronized (connectionList) {  \n            connectionList.add(numConnections, c);  \n            numConnections++;  \n          }  \n          …           \n        } finally {  \n          reader.finishAdd(); // add完毕，设置adding为false，Reader开始工作  \n        }  \n    }  \n}\n```\n\n2）接收请求\n\nReader的run方法和Listener基本一致，也是获得所有的SelectionKey，再执行doRead(key)方法。该方法获得key中绑定的connection，并执行conection的readAndProcess()方法：\n\n``` java\nvoid doRead(SelectionKey key) throws InterruptedException {  \n      int count = 0;  \n      Connection c = (Connection)key.attachment(); // 获得连接对象  \n      if (c == null) {  \n        return;    \n      }  \n      c.setLastContact(System.currentTimeMillis());  \n\n      try {  \n        count = c.readAndProcess(); // 接受并处理请求  \n      } catch (InterruptedException ieo) {  \n        …  \n      }  \n      if (count < 0) {  \n        …  \n        closeConnection(c);  \n        c = null;  \n      }  \n      else {  \n        c.setLastContact(System.currentTimeMillis());  \n      }  \n}  \n```            \n\n``` java\npublic int readAndProcess() throws IOException, InterruptedException {  \n    // 一次最多读取一次RPC请求，如果头没读完，继续迭代直到  \n    // 读完所有请求数据    \n    while (true) {   \n        int count = –1;  \n        if (dataLengthBuffer.remaining() > 0) {  \n          count = channelRead(channel, dataLengthBuffer);         \n          …  \n        if (!rpcHeaderRead) {  \n          //读取请求头.  \n          if (rpcHeaderBuffer == null) {  \n            rpcHeaderBuffer = ByteBuffer.allocate(2);  \n          }  \n          count = channelRead(channel, rpcHeaderBuffer);  \n          if (count < 0 || rpcHeaderBuffer.remaining() > 0) {  \n            return count;  \n          }  \n          // 读取请求版本号  \n          int version = rpcHeaderBuffer.get(0);  \n          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};  \n          authMethod = AuthMethod.read(new DataInputStream(  \n              new ByteArrayInputStream(method)));  \n          dataLengthBuffer.flip();            \n          …  \n          dataLengthBuffer.clear();  \n          …  \n\n          rpcHeaderBuffer = null;  \n          rpcHeaderRead = true;  \n          continue;  \n        }   \n        …  \n          data = ByteBuffer.allocate(dataLength);  \n        }  \n\n        // 读取请求  \n        count = channelRead(channel, data);  \n\n        if (data.remaining() == 0) {  \n          …  \n          if (useSasl) {  \n            saslReadAndProcess(data.array());  \n          } else {  \n            // 执行RPC请求，先解析header请求，下次循环解析param请求  \n            processOneRpc(data.array());  \n          }  \n          …  \n        }   \n        return count;  \n    }  \n}  \n```\n\n3）获得call请求\n\n在Connection中解析param请求中，解析了请求数据，并构造Call对象，将其加入callQueue。\n\n``` java\nprivate void processData(byte[] buf) throws  IOException, InterruptedException {  \n      DataInputStream dis =  \n        new DataInputStream(new ByteArrayInputStream(buf));  \n      int id = dis.readInt();         // 读取请求id  \n        …  \n\n      Writable param = ReflectionUtils.newInstance(paramClass, conf);// 获取参数，paramClass是参数的实体类，在构造Server对象的时候传入  \n      param.readFields(dis);          \n\n      Call call = new Call(id, param, this);  \n      callQueue.put(call);              // 添加进阻塞队列，不过队列有max限制，有可能也会阻塞  \n      incRpcCount();   \n}  \n```\n\n4）处理call对象\n\nConnection给callQueue添加了call对象，阻塞的Handler可以继续运行了，拿出一个call对象，并调用RPC.Call方法。\n\n``` java\n// 关键代码  \nwhile (running) {  \n   final Call call = callQueue.take(); // 弹出call对象  \n   CurCall.set(call);  \n   value = call(call.connection.protocol, call.param,   \n                         call.timestamp); // 调用RPC.Server中的call  \n   CurCall.set(null);  \n\n   synchronized (call.connection.responseQueue) {  \n       setupResponse(buf, call,   \n                      (error == null) ? Status.SUCCESS : Status.ERROR,   \n                      value, errorClass, error);  \n       …  \n       responder.doRespond(call);  \n   }  \n}  \n```\n\n5）响应请求\n\n上面代码中的setupResponse将call的id和状态发送回去，再设置了call中的response:ByteBuffer，之后就开始responder.doRespond(call)了，processResponse以及Responder.run()没太弄明白，就先不说了。\n\n``` java\nvoid doRespond(Call call) throws IOException {  \n  synchronized (call.connection.responseQueue) {  \n    // 这行没懂  \n    call.connection.responseQueue.addLast(call);  \n    if (call.connection.responseQueue.size() == 1) {  \n      // 返回响应结果，并激活writeSelector  \n      processResponse(call.connection.responseQueue, true);  \n    }  \n  }  \n}  \n  ```\n\n### 6、总结\n\nServer用的标准的Java TCP/IP NIO通信，同时请求的超时使用基于BlockingQueue以及wait/notify机制实现。使用的模式是reactor模式，关于nio和reactor可以参考这个[博客](http://www.cnblogs.com/ericchen/archive/2011/05/08/2036993.html)。\n\n对于服务器端接收多个连接请求的需求，Server采用Listener来监听连接的事件，并用Listener.Reader来监听网络流读以及Responder监听写的事件，当有实际的网络流读写时间发生之后，解析了请求Call之后，添加进阻塞队列，并交由多个Handlers来处理请求。\n\n这个方法比TCP/IP BIO好处就是可接受很多的连接，而这些连接只在真实的请求时才会创建线程处理，称之为一请求一处理。但是，连接上的请求发送非常频繁时，TCP/IP NIO的方法并不会带来太大的优势。\n\n但是Hadoop实际场景中，通常是服务器端支持大量的连接数（Namenode连上几千个Datanode），但是连接发送的请求并不会太多（heartbeat、blockreport都有较长间隔）。这样就造成了Hadoop不适合实时的、多请求的运算，带来的代价是模型、实现简单，但是这也为以后的扩展埋下了祸根。\n\nP.S.: 以上分析基于稳定版0.20.203.0rc1。\n","source":"_posts/2012/02/hadoop-ipc-server.md","raw":"title: Hadoop源码 - ipc.Server\ntags:\n  - Hadoop\n  - RPC\nid: 387\ncategories:\n  - 技术分享\ndate: 2012-02-27 12:57:25\n---\n\n### 1、前言\n\n昨天分析了ipc包下的RPC、Client类，今天来分析下ipc.Server。Server类因为是Hadoop自己使用，所以代码结构以及流程都很清晰，可以清楚的看到实例化、停止、运行等过程。\n\n<!--more-->\n\n### 2、Server类结构\n\n上面是Server的五个内部类，分别介绍一下：\n\n1）Call\n\n用以存储客户端发来的请求，这个请求会放入一个BlockQueue中；\n\n2）Listener\n\n监听类，用以监听客户端发来的请求。同时Listener下面还有一个静态类，Listener.Reader，当监听器监听到用户请求，便用让Reader读取用户请求。\n\n3）Responder\n\n响应RPC请求类，请求处理完毕，由Responder发送给请求客户端。\n\n4）Connection\n\n连接类，真正的客户端请求读取逻辑在这个类中。\n\n5）Handler\n\n请求（blockQueueCall）处理类，会循环阻塞读取callQueue中的call对象，并对其进行操作。\n\n3、Server初始化\n\n第一篇[博客](http://www.hongweiyi.com/2012/02/hadoop-ipc-rpc/)说了，Server的初始化入口在RPC.getServer中，getServer其实是调用的RPC.Server静态类中的构造方法，我们看看Namenode创建RPCServer的方法和RPC.Server构造方法代码：\n\n``` java\nprivate void initialize(Configuration conf) throws IOException {  \n    …  \n    this.serviceRpcServer = RPC.getServer(this, dnSocketAddr.getHostName(),   \n          dnSocketAddr.getPort(), serviceHandlerCount,  \n          false, conf, namesystem.getDelegationTokenSecretManager());  \n    this.serviceRpcServer.start(); // 运行服务器  \n}  \n```         \n\n``` java\npublic Server(Object instance, Configuration conf, String bindAddress,  int port,  \n                  int numHandlers, boolean verbose,   \n                  SecretManager<? extends TokenIdentifier> secretManager)   \n        throws IOException {  \n      super(bindAddress, port, Invocation.class, numHandlers, conf,  \n          classNameBase(instance.getClass().getName()), secretManager);  \n      this.instance = instance;  \n      this.verbose = verbose;  \n}  \n```\n\n该方法调用了父类的构造方法，如下：\n\n``` java\nprotected Server(String bindAddress, int port,   \n                  Class<? extends Writable> paramClass, int handlerCount,   \n                  Configuration conf, String serverName, SecretManager<? extends TokenIdentifier> secretManager)   \n    throws IOException {  \n    this.bindAddress = bindAddress;  \n    this.conf = conf;  \n    this.port = port;  \n    this.paramClass = paramClass;  \n    this.handlerCount = handlerCount;  \n    this.socketSendBufferSize = 0;  \n    this.maxQueueSize = handlerCount * conf.getInt(  \n                                IPC_SERVER_HANDLER_QUEUE_SIZE_KEY,  \n                                IPC_SERVER_HANDLER_QUEUE_SIZE_DEFAULT);  \n    this.maxRespSize = conf.getInt(IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY,  \n                                   IPC_SERVER_RPC_MAX_RESPONSE_SIZE_DEFAULT);  \n    this.readThreads = conf.getInt(  \n        IPC_SERVER_RPC_READ_THREADS_KEY,  \n        IPC_SERVER_RPC_READ_THREADS_DEFAULT);  \n    this.callQueue  = new LinkedBlockingQueue<Call>(maxQueueSize);   \n    this.maxIdleTime = 2*conf.getInt(\"ipc.client.connection.maxidletime\", 1000);  \n    this.maxConnectionsToNuke = conf.getInt(\"ipc.client.kill.max\", 10);  \n    this.thresholdIdleConnections = conf.getInt(\"ipc.client.idlethreshold\", 4000);  \n    this.secretManager = (SecretManager<TokenIdentifier>) secretManager;  \n    this.authorize =   \n      conf.getBoolean(HADOOP_SECURITY_AUTHORIZATION, false);  \n    this.isSecurityEnabled = UserGroupInformation.isSecurityEnabled();  \n\n    // Start the listener here and let it bind to the port  \n    listener = new Listener();  \n    this.port = listener.getAddress().getPort();      \n    this.rpcMetrics = RpcInstrumentation.create(serverName, this.port);  \n    this.tcpNoDelay = conf.getBoolean(\"ipc.server.tcpnodelay\", false);  \n\n    // Create the responder here  \n    responder = new Responder();  \n\n    if (isSecurityEnabled) {  \n      SaslRpcServer.init(conf);  \n    }  \n}  \n```\n\n不难看出，父类的构造方法就初始化了一些配置和变量。\n\n### 4、Server运行\n\n在上面第一段代码中，还有一句RpcServer.start()的方法，在调用构造函数初始化一些变量之后，Server就可以正式运行起来了：\n\n``` java\npublic synchronized void start() {  \n    responder.start();  \n    listener.start();  \n    handlers = new Handler[handlerCount];  \n\n    for (int i = 0; i < handlerCount; i++) {  \n      handlers[i] = new Handler(i);  \n      handlers[i].start();  \n    }  \n}\n```\n\nresponder、listener、handlers三个对象的线程均阻塞了，前两个阻塞在selector.select()方法上，handler阻塞在callQueue.take()方法，都在等待客户端请求。Responder设置了超时时间，为15分钟。而listener还开启了Reader线程，该线程也阻塞了。\n\n### 4、Server接受请求流程\n\n1）监听到请求\n\nListener监听到请求，获得所有请求的SelectionKey，执行doAccept(key)方法，该方法将所有的连接对象放入list中，并将connection对象与key绑定，以供reader使用。初始化玩所有的conne对象之后，就可以激活Reader线程了。\n\n``` java\nvoid doAccept(SelectionKey key) throws IOException,  OutOfMemoryError {  \n      Connection c = null;  \n      ServerSocketChannel server = (ServerSocketChannel) key.channel();  \n      SocketChannel channel;  \n      while ((channel = server.accept()) != null) {  \n        channel.configureBlocking(false);  \n        channel.socket().setTcpNoDelay(tcpNoDelay);  \n        Reader reader = getReader();  \n        try {  \n          reader.startAdd();  // 激活readSelector，设置adding为true  \n          SelectionKey readKey = reader.registerChannel(channel);  \n          c = new Connection(readKey, channel, System.currentTimeMillis());  \n          readKey.attach(c);  \n          synchronized (connectionList) {  \n            connectionList.add(numConnections, c);  \n            numConnections++;  \n          }  \n          …           \n        } finally {  \n          reader.finishAdd(); // add完毕，设置adding为false，Reader开始工作  \n        }  \n    }  \n}\n```\n\n2）接收请求\n\nReader的run方法和Listener基本一致，也是获得所有的SelectionKey，再执行doRead(key)方法。该方法获得key中绑定的connection，并执行conection的readAndProcess()方法：\n\n``` java\nvoid doRead(SelectionKey key) throws InterruptedException {  \n      int count = 0;  \n      Connection c = (Connection)key.attachment(); // 获得连接对象  \n      if (c == null) {  \n        return;    \n      }  \n      c.setLastContact(System.currentTimeMillis());  \n\n      try {  \n        count = c.readAndProcess(); // 接受并处理请求  \n      } catch (InterruptedException ieo) {  \n        …  \n      }  \n      if (count < 0) {  \n        …  \n        closeConnection(c);  \n        c = null;  \n      }  \n      else {  \n        c.setLastContact(System.currentTimeMillis());  \n      }  \n}  \n```            \n\n``` java\npublic int readAndProcess() throws IOException, InterruptedException {  \n    // 一次最多读取一次RPC请求，如果头没读完，继续迭代直到  \n    // 读完所有请求数据    \n    while (true) {   \n        int count = –1;  \n        if (dataLengthBuffer.remaining() > 0) {  \n          count = channelRead(channel, dataLengthBuffer);         \n          …  \n        if (!rpcHeaderRead) {  \n          //读取请求头.  \n          if (rpcHeaderBuffer == null) {  \n            rpcHeaderBuffer = ByteBuffer.allocate(2);  \n          }  \n          count = channelRead(channel, rpcHeaderBuffer);  \n          if (count < 0 || rpcHeaderBuffer.remaining() > 0) {  \n            return count;  \n          }  \n          // 读取请求版本号  \n          int version = rpcHeaderBuffer.get(0);  \n          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};  \n          authMethod = AuthMethod.read(new DataInputStream(  \n              new ByteArrayInputStream(method)));  \n          dataLengthBuffer.flip();            \n          …  \n          dataLengthBuffer.clear();  \n          …  \n\n          rpcHeaderBuffer = null;  \n          rpcHeaderRead = true;  \n          continue;  \n        }   \n        …  \n          data = ByteBuffer.allocate(dataLength);  \n        }  \n\n        // 读取请求  \n        count = channelRead(channel, data);  \n\n        if (data.remaining() == 0) {  \n          …  \n          if (useSasl) {  \n            saslReadAndProcess(data.array());  \n          } else {  \n            // 执行RPC请求，先解析header请求，下次循环解析param请求  \n            processOneRpc(data.array());  \n          }  \n          …  \n        }   \n        return count;  \n    }  \n}  \n```\n\n3）获得call请求\n\n在Connection中解析param请求中，解析了请求数据，并构造Call对象，将其加入callQueue。\n\n``` java\nprivate void processData(byte[] buf) throws  IOException, InterruptedException {  \n      DataInputStream dis =  \n        new DataInputStream(new ByteArrayInputStream(buf));  \n      int id = dis.readInt();         // 读取请求id  \n        …  \n\n      Writable param = ReflectionUtils.newInstance(paramClass, conf);// 获取参数，paramClass是参数的实体类，在构造Server对象的时候传入  \n      param.readFields(dis);          \n\n      Call call = new Call(id, param, this);  \n      callQueue.put(call);              // 添加进阻塞队列，不过队列有max限制，有可能也会阻塞  \n      incRpcCount();   \n}  \n```\n\n4）处理call对象\n\nConnection给callQueue添加了call对象，阻塞的Handler可以继续运行了，拿出一个call对象，并调用RPC.Call方法。\n\n``` java\n// 关键代码  \nwhile (running) {  \n   final Call call = callQueue.take(); // 弹出call对象  \n   CurCall.set(call);  \n   value = call(call.connection.protocol, call.param,   \n                         call.timestamp); // 调用RPC.Server中的call  \n   CurCall.set(null);  \n\n   synchronized (call.connection.responseQueue) {  \n       setupResponse(buf, call,   \n                      (error == null) ? Status.SUCCESS : Status.ERROR,   \n                      value, errorClass, error);  \n       …  \n       responder.doRespond(call);  \n   }  \n}  \n```\n\n5）响应请求\n\n上面代码中的setupResponse将call的id和状态发送回去，再设置了call中的response:ByteBuffer，之后就开始responder.doRespond(call)了，processResponse以及Responder.run()没太弄明白，就先不说了。\n\n``` java\nvoid doRespond(Call call) throws IOException {  \n  synchronized (call.connection.responseQueue) {  \n    // 这行没懂  \n    call.connection.responseQueue.addLast(call);  \n    if (call.connection.responseQueue.size() == 1) {  \n      // 返回响应结果，并激活writeSelector  \n      processResponse(call.connection.responseQueue, true);  \n    }  \n  }  \n}  \n  ```\n\n### 6、总结\n\nServer用的标准的Java TCP/IP NIO通信，同时请求的超时使用基于BlockingQueue以及wait/notify机制实现。使用的模式是reactor模式，关于nio和reactor可以参考这个[博客](http://www.cnblogs.com/ericchen/archive/2011/05/08/2036993.html)。\n\n对于服务器端接收多个连接请求的需求，Server采用Listener来监听连接的事件，并用Listener.Reader来监听网络流读以及Responder监听写的事件，当有实际的网络流读写时间发生之后，解析了请求Call之后，添加进阻塞队列，并交由多个Handlers来处理请求。\n\n这个方法比TCP/IP BIO好处就是可接受很多的连接，而这些连接只在真实的请求时才会创建线程处理，称之为一请求一处理。但是，连接上的请求发送非常频繁时，TCP/IP NIO的方法并不会带来太大的优势。\n\n但是Hadoop实际场景中，通常是服务器端支持大量的连接数（Namenode连上几千个Datanode），但是连接发送的请求并不会太多（heartbeat、blockreport都有较长间隔）。这样就造成了Hadoop不适合实时的、多请求的运算，带来的代价是模型、实现简单，但是这也为以后的扩展埋下了祸根。\n\nP.S.: 以上分析基于稳定版0.20.203.0rc1。\n","slug":"2012/02/hadoop-ipc-server","published":1,"updated":"2015-12-30T11:25:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93u00723x8fln18ucz0"},{"title":"Hadoop源码 - ipc.RPC","id":"380","date":"2012-02-25T15:00:23.000Z","_content":"\n### 1、前言\n\nHadoop是典型的单元数据服务器模型，它将控制流与数据流分离开来，同时两种流的通信机制也不一样，分别为RPC和流式通信，这篇博客主要介绍Hadoop的RPC流程……。关于RPC的简介可以参考：[百度百科](http://baike.baidu.com/view/32726.htm)。\n\n<!--more-->\n\nHadoop的RPC主要是通过Java的动态代理（Dynamic Proxy）与反射（Reflect）实现，源代码在org.apache.hadoop.ipc下，有以下几个主要类：\n\n* Client：RPC服务的客户端\n* RPC：实现了一个简单的RPC模型\n* Server：服务端的抽象类\n* RPC.Server：服务端的具体类\n* VersionedProtocol：所有的使用RPC服务的类都要实现该接口，在创建代理时，用来判断代理对象是否创建正确。\n\n### 2、通信发生在？\n\nHadoop是master-slave模型，master只会接受请求并相应，slave在发送请求的同时，也有可能会接受其它请求，其它请求来自slave伙伴或者client。\n\nVersionedProtocol说了，所有要使用RPC服务的类都要实现该接口，我们可以来看一下有哪些接口继承了该接口。\n\n![clip_image002](/images/2012/02/clip_image0023.jpg)\n\n#### 1）HDFS相关\n\n* **ClientDatanodeProtocol**：client与datanode交互的接口，操作不多，只有一个block恢复的方法。那么，其它数据请求的方法呢？client与datanode主要交互是通过流式的socket实现，源码在DataXceiver，在这里先不说了；\n\n* **ClientProtocol**：client与Namenode交互的接口，所有控制流的请求均在这里，如：创建文件、删除文件等；\n\n* **DatanodeProtocol**：Datanode与Namenode交互的接口，如心跳、blockreport等；\n\n* **NamenodeProtocol**：SecondaryNode与Namenode交互的接口。\n\n#### 2）Mapreduce相关\n\n* **InterDatanodeProtocol**：Datanode内部交互的接口，用来更新block的元数据；\n* **InnerTrackerProtocol**：TaskTracker与JobTracker交互的接口，功能与DatanodeProtocol相似；\n* **JobSubmissionProtocol**：JobClient与JobTracker交互的接口，用来提交Job、获得Job等与Job相关的操作；\n* **TaskUmbilicalProtocol**：Task中子进程与母进程交互的接口，子进程即map、reduce等操作，母进程即TaskTracker，该接口可以回报子进程的运行状态（词汇扫盲: umbilical 脐带的, 关系亲密的） 。\n\n#### 3）其它\n\n* **AdminOperationProtocol**：不用用户操作的接口，提供一些管理操作，如刷新JobTracker的node列表；\n\n* **RefreshAuthorizationPolicyProtocol****，RefreshUserMappingsProtocol\n\n### 3、RPC方法\n\nRPC提供了一个简单的RPC机制，提供以下几种静态方法：\n\n![clip_image004](/images/2012/02/clip_image0042.jpg)\n\n#### 1）Proxy\n\nwaitForProxy、getProxy、stopProxy均是与代理有关的方法，其中wait需要保证namenode启动正常且连接正常，主要由SecondayNode、Datanode、JobTracker使用。\n\nstop方法即停止代理。\n\nget则是一般的获取代理的方法， 创建代理实例，获得代理实例的versioncode，再与getProxy方法传入的versioncode做对比，相同返回代理，不同抛出VersionMismatch异常。\n\n#### 2）getServer\n\n创建并返回一个Server实例，由TaskTracker、JobTracker、NameNode、DataNode使用。\n\n#### 3）call\n\n静态方法，向一系列服务器发送一系列请求，在源码中没见到那个类使用该方法。但注释提到了：Expert，应该是给系统管理员使用的接口。\n\n### 4、RPC静态类\n\nRPC方法仅仅提到了方法的作用，但是具体实现没说，具体实现就涉及到了RPC的静态类了，RPC类中有5个静态内部类，分别为：\n\n* **RPC.ClientCache**：用来缓存Client对象；\n\n* **RPC.Invocation**：每次RPC调用传的参数实体类，其中Invocation包括了调用方法（Method）和配置文件；\n\n* **RPC.Invoker**：具体的调用类，采用Java的动态代理机制，继承自InvocationHandler，有remoteId和client成员，id用以标识异步请求对象，client用以调用实现代码；\n\n* **RPC.Server**：org.apache.hadoop.ipc.Server的具体类，实现了抽象类的call方法，获得传入参数的call实例，再获取method方法，调用即可。用的是反射机制，反射很绝，再没使用之前，完全不知道该代码会怎么执行；\n\n* **RPC.VersionMismatch**：版本不匹配异常。\n\n### 5、小结\n\n以上是`org.apache.hadoop.ipc.RPC`类的实现分析，接下来再分析ipc包下的Server与Client类吧！\n","source":"_posts/2012/02/hadoop-ipc-rpc.md","raw":"title: Hadoop源码 - ipc.RPC\ntags:\n  - Hadoop\n  - RPC\nid: 380\ncategories:\n  - 技术分享\ndate: 2012-02-25 23:00:23\n---\n\n### 1、前言\n\nHadoop是典型的单元数据服务器模型，它将控制流与数据流分离开来，同时两种流的通信机制也不一样，分别为RPC和流式通信，这篇博客主要介绍Hadoop的RPC流程……。关于RPC的简介可以参考：[百度百科](http://baike.baidu.com/view/32726.htm)。\n\n<!--more-->\n\nHadoop的RPC主要是通过Java的动态代理（Dynamic Proxy）与反射（Reflect）实现，源代码在org.apache.hadoop.ipc下，有以下几个主要类：\n\n* Client：RPC服务的客户端\n* RPC：实现了一个简单的RPC模型\n* Server：服务端的抽象类\n* RPC.Server：服务端的具体类\n* VersionedProtocol：所有的使用RPC服务的类都要实现该接口，在创建代理时，用来判断代理对象是否创建正确。\n\n### 2、通信发生在？\n\nHadoop是master-slave模型，master只会接受请求并相应，slave在发送请求的同时，也有可能会接受其它请求，其它请求来自slave伙伴或者client。\n\nVersionedProtocol说了，所有要使用RPC服务的类都要实现该接口，我们可以来看一下有哪些接口继承了该接口。\n\n![clip_image002](/images/2012/02/clip_image0023.jpg)\n\n#### 1）HDFS相关\n\n* **ClientDatanodeProtocol**：client与datanode交互的接口，操作不多，只有一个block恢复的方法。那么，其它数据请求的方法呢？client与datanode主要交互是通过流式的socket实现，源码在DataXceiver，在这里先不说了；\n\n* **ClientProtocol**：client与Namenode交互的接口，所有控制流的请求均在这里，如：创建文件、删除文件等；\n\n* **DatanodeProtocol**：Datanode与Namenode交互的接口，如心跳、blockreport等；\n\n* **NamenodeProtocol**：SecondaryNode与Namenode交互的接口。\n\n#### 2）Mapreduce相关\n\n* **InterDatanodeProtocol**：Datanode内部交互的接口，用来更新block的元数据；\n* **InnerTrackerProtocol**：TaskTracker与JobTracker交互的接口，功能与DatanodeProtocol相似；\n* **JobSubmissionProtocol**：JobClient与JobTracker交互的接口，用来提交Job、获得Job等与Job相关的操作；\n* **TaskUmbilicalProtocol**：Task中子进程与母进程交互的接口，子进程即map、reduce等操作，母进程即TaskTracker，该接口可以回报子进程的运行状态（词汇扫盲: umbilical 脐带的, 关系亲密的） 。\n\n#### 3）其它\n\n* **AdminOperationProtocol**：不用用户操作的接口，提供一些管理操作，如刷新JobTracker的node列表；\n\n* **RefreshAuthorizationPolicyProtocol****，RefreshUserMappingsProtocol\n\n### 3、RPC方法\n\nRPC提供了一个简单的RPC机制，提供以下几种静态方法：\n\n![clip_image004](/images/2012/02/clip_image0042.jpg)\n\n#### 1）Proxy\n\nwaitForProxy、getProxy、stopProxy均是与代理有关的方法，其中wait需要保证namenode启动正常且连接正常，主要由SecondayNode、Datanode、JobTracker使用。\n\nstop方法即停止代理。\n\nget则是一般的获取代理的方法， 创建代理实例，获得代理实例的versioncode，再与getProxy方法传入的versioncode做对比，相同返回代理，不同抛出VersionMismatch异常。\n\n#### 2）getServer\n\n创建并返回一个Server实例，由TaskTracker、JobTracker、NameNode、DataNode使用。\n\n#### 3）call\n\n静态方法，向一系列服务器发送一系列请求，在源码中没见到那个类使用该方法。但注释提到了：Expert，应该是给系统管理员使用的接口。\n\n### 4、RPC静态类\n\nRPC方法仅仅提到了方法的作用，但是具体实现没说，具体实现就涉及到了RPC的静态类了，RPC类中有5个静态内部类，分别为：\n\n* **RPC.ClientCache**：用来缓存Client对象；\n\n* **RPC.Invocation**：每次RPC调用传的参数实体类，其中Invocation包括了调用方法（Method）和配置文件；\n\n* **RPC.Invoker**：具体的调用类，采用Java的动态代理机制，继承自InvocationHandler，有remoteId和client成员，id用以标识异步请求对象，client用以调用实现代码；\n\n* **RPC.Server**：org.apache.hadoop.ipc.Server的具体类，实现了抽象类的call方法，获得传入参数的call实例，再获取method方法，调用即可。用的是反射机制，反射很绝，再没使用之前，完全不知道该代码会怎么执行；\n\n* **RPC.VersionMismatch**：版本不匹配异常。\n\n### 5、小结\n\n以上是`org.apache.hadoop.ipc.RPC`类的实现分析，接下来再分析ipc包下的Server与Client类吧！\n","slug":"2012/02/hadoop-ipc-rpc","published":1,"updated":"2015-12-30T11:19:56.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93v00773x8f3d5pi7bm"},{"title":"Hadoop源码 - ipc.Client","id":"384","date":"2012-02-25T15:06:42.000Z","_content":"\n### 1、前言\n\n上篇博客分析了ipc包下的RPC类，这篇博客来看看Client类吧。\n\n<!--more-->\n\nHadoop的RPC机制还是挺简单的，简述如下：\n\n1. 创建代理对象；\n2. 代理对象调用相应方法（invoke()）；\n3. invoke调用client对象的call方法，向服务器发送请求（参数、方法）；\n4. 再等待call方法的完成；\n5. 返回请求结果。\n\n具体是怎么实现的，下面来看看吧。\n\n### 2、Client类分析\n\n有了RPC大致分析，Client我就挑重要的分析了。\n\n**1）connections**\n\nRPC中就有了ClientCache类，那么client可以复用，所以一个client对象会有多个连接对象，实现中是用`HashTable<ConnectionId, Connection>`存储的连接对象。\n\n其中，ConnectionId是用来唯一标识连接的ID，主要由客户端地址、时间戳再结合一个素数生成；Connection是Client中的静态内部类，用以处理远程连接对象。\n\n**2）Call**\n\n客户端方法调用的实体类，存放了id、参数、返回值等。需要注意的是，Call类中有callComplete()方法，在一次call调用完毕之后调用，并调用notify()通知client接收完毕。\n\n**3）`call(Writable param, ConnectionId remoteId)`**\n\n``` java\npublic Object invoke(Object proxy, Method method, Object[] args)  \n      throws Throwable {  \n      …  \n\n      ObjectWritable value = (ObjectWritable)  \n        client.call(new Invocation(method, args), remoteId); // 调用方法  \n      …  \n      return value.get();  \n}  \n```\n\n``` java\n// client请求方法  \npublic Writable call(Writable param, ConnectionId remoteId)    \n                       throws InterruptedException, IOException {  \n    Call call = new Call(param);  \n    Connection connection = getConnection(remoteId, call); // 获得连接对象  \n    connection.sendParam(call);                 // 发送参数  \n    …  \n    synchronized (call) {  \n      while (!call.done) {  \n        try {  \n          call.wait();                           // 等待response  \n        } catch (InterruptedException ie) {  \n          interrupted = true;  \n        }  \n      }  \n      …  \n      return call.value;  \n      …  \n    }  \n}\n```\n\n``` java\n// Connection线程，等待服务器响应  \npublic void run() {  \n      if (LOG.isDebugEnabled())  \n        LOG.debug(getName() + \": starting, having connections \"   \n            + connections.size());  \n\n      while (waitForWork()) {// 等待工作，主要依据是calls是否为空  \n        receiveResponse();  // 接收响应  \n      }  \n\n      close();  \n\n      …  \n}  \n```\n\n``` java\nprivate void receiveResponse() {  \n      …  \n\n      try {  \n        int id = in.readInt();                    // try to read an id  \n        …  \n        Call call = calls.get(id);  \n\n        int state = in.readInt();     // read call status  \n        if (state == Status.SUCCESS.state) {  \n          Writable value = ReflectionUtils.newInstance(valueClass, conf);  \n          value.readFields(in);                 // read value  \n          call.setValue(value);  \n          calls.remove(id);  \n        }   \n        …  \n      } catch (IOException e) {  \n        markClosed(e);  \n      }  \n}  \n```\n\n``` java\npublic synchronized void setValue(Writable value) {  \n      this.value = value;  \n      callComplete();  \n}  \nprotected synchronized void callComplete() {  \n      this.done = true;  \n      notify();  // notify caller  \n}  \n```\n\n该方法由invoker调用，调用过程如下：\n\n1. 构建Call对象\n2. 用remoteId获得connection对象\n   2.1 如果connections中有remoteId，取得该connection；反之，创建一个，并添加进connections；\n   2.2 connection.setupIOstreams()连接到服务器，并配置好连接对象，发送协议头，接着运行connection线程，等待接收工作（waitForWork()）；\n3. 调用connection.sendParam发送协议体，等待接收响应，call.wait();；\n   3.1 receiveResponse()，依次读入call的值（id，value）；\n   3.2 标记接收结束（markClosed），同时notifyAll()；\n4. 获得返回值，返回调用者。\n\n### 3、异步/同步模型\n\nHadoop的RPC对外的接口其实是同步的，但是，RPC的内部实现其实是异步消息机制。hadoop用线程wait/notify机制实现异步转同步，发送请求（call）之后wait请求处理完毕，接收完响应（connection.receiveResponse()）之后notify，notify()方法在call.setValue中。\n\n但现在有一个问题，一个connection有多个call。可能同时有多个call在等待接收消息，那么是当client接收到response后，怎样确认它到底是之前哪个request的response呢？这个就是依靠的connection中的一个HashTable&lt;Integer, Call&gt;了，其中的Integer是用来标识Call，这样就可以将request和response对应上了。\n\n> **参考资料：**\n>\n> [智障大师的专栏](http://blog.csdn.net/historyasamirror/article/details/6159248)\n","source":"_posts/2012/02/hadoop-ipc-client.md","raw":"title: Hadoop源码 - ipc.Client\ntags:\n  - Hadoop\n  - RPC\nid: 384\ncategories:\n  - 技术分享\ndate: 2012-02-25 23:06:42\n---\n\n### 1、前言\n\n上篇博客分析了ipc包下的RPC类，这篇博客来看看Client类吧。\n\n<!--more-->\n\nHadoop的RPC机制还是挺简单的，简述如下：\n\n1. 创建代理对象；\n2. 代理对象调用相应方法（invoke()）；\n3. invoke调用client对象的call方法，向服务器发送请求（参数、方法）；\n4. 再等待call方法的完成；\n5. 返回请求结果。\n\n具体是怎么实现的，下面来看看吧。\n\n### 2、Client类分析\n\n有了RPC大致分析，Client我就挑重要的分析了。\n\n**1）connections**\n\nRPC中就有了ClientCache类，那么client可以复用，所以一个client对象会有多个连接对象，实现中是用`HashTable<ConnectionId, Connection>`存储的连接对象。\n\n其中，ConnectionId是用来唯一标识连接的ID，主要由客户端地址、时间戳再结合一个素数生成；Connection是Client中的静态内部类，用以处理远程连接对象。\n\n**2）Call**\n\n客户端方法调用的实体类，存放了id、参数、返回值等。需要注意的是，Call类中有callComplete()方法，在一次call调用完毕之后调用，并调用notify()通知client接收完毕。\n\n**3）`call(Writable param, ConnectionId remoteId)`**\n\n``` java\npublic Object invoke(Object proxy, Method method, Object[] args)  \n      throws Throwable {  \n      …  \n\n      ObjectWritable value = (ObjectWritable)  \n        client.call(new Invocation(method, args), remoteId); // 调用方法  \n      …  \n      return value.get();  \n}  \n```\n\n``` java\n// client请求方法  \npublic Writable call(Writable param, ConnectionId remoteId)    \n                       throws InterruptedException, IOException {  \n    Call call = new Call(param);  \n    Connection connection = getConnection(remoteId, call); // 获得连接对象  \n    connection.sendParam(call);                 // 发送参数  \n    …  \n    synchronized (call) {  \n      while (!call.done) {  \n        try {  \n          call.wait();                           // 等待response  \n        } catch (InterruptedException ie) {  \n          interrupted = true;  \n        }  \n      }  \n      …  \n      return call.value;  \n      …  \n    }  \n}\n```\n\n``` java\n// Connection线程，等待服务器响应  \npublic void run() {  \n      if (LOG.isDebugEnabled())  \n        LOG.debug(getName() + \": starting, having connections \"   \n            + connections.size());  \n\n      while (waitForWork()) {// 等待工作，主要依据是calls是否为空  \n        receiveResponse();  // 接收响应  \n      }  \n\n      close();  \n\n      …  \n}  \n```\n\n``` java\nprivate void receiveResponse() {  \n      …  \n\n      try {  \n        int id = in.readInt();                    // try to read an id  \n        …  \n        Call call = calls.get(id);  \n\n        int state = in.readInt();     // read call status  \n        if (state == Status.SUCCESS.state) {  \n          Writable value = ReflectionUtils.newInstance(valueClass, conf);  \n          value.readFields(in);                 // read value  \n          call.setValue(value);  \n          calls.remove(id);  \n        }   \n        …  \n      } catch (IOException e) {  \n        markClosed(e);  \n      }  \n}  \n```\n\n``` java\npublic synchronized void setValue(Writable value) {  \n      this.value = value;  \n      callComplete();  \n}  \nprotected synchronized void callComplete() {  \n      this.done = true;  \n      notify();  // notify caller  \n}  \n```\n\n该方法由invoker调用，调用过程如下：\n\n1. 构建Call对象\n2. 用remoteId获得connection对象\n   2.1 如果connections中有remoteId，取得该connection；反之，创建一个，并添加进connections；\n   2.2 connection.setupIOstreams()连接到服务器，并配置好连接对象，发送协议头，接着运行connection线程，等待接收工作（waitForWork()）；\n3. 调用connection.sendParam发送协议体，等待接收响应，call.wait();；\n   3.1 receiveResponse()，依次读入call的值（id，value）；\n   3.2 标记接收结束（markClosed），同时notifyAll()；\n4. 获得返回值，返回调用者。\n\n### 3、异步/同步模型\n\nHadoop的RPC对外的接口其实是同步的，但是，RPC的内部实现其实是异步消息机制。hadoop用线程wait/notify机制实现异步转同步，发送请求（call）之后wait请求处理完毕，接收完响应（connection.receiveResponse()）之后notify，notify()方法在call.setValue中。\n\n但现在有一个问题，一个connection有多个call。可能同时有多个call在等待接收消息，那么是当client接收到response后，怎样确认它到底是之前哪个request的response呢？这个就是依靠的connection中的一个HashTable&lt;Integer, Call&gt;了，其中的Integer是用来标识Call，这样就可以将request和response对应上了。\n\n> **参考资料：**\n>\n> [智障大师的专栏](http://blog.csdn.net/historyasamirror/article/details/6159248)\n","slug":"2012/02/hadoop-ipc-client","published":1,"updated":"2015-12-30T11:17:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba93x007b3x8fx9pac8zx"},{"title":"算法设计 - 回溯法到搜索浅谈","id":"331","date":"2012-02-21T16:56:00.000Z","_content":"\n### 一、定义说明\n\n回溯法是一个即带有系统性又带有跳跃性的搜索算法，它在问题的解空间树中，按深度优先搜索策略，从根结点出发搜索解空间树。算法搜索至解空间树的任一结点时，先判断该结点是否包含问题的解。如果肯定不包含，则跳过对该结点为根的子树的搜索，逐层向其祖先结点回溯；否则，进入该子树，继续按深度优先策略搜索。\n\n<!--more-->\n\n回溯法是设计递归过程的一种重要方法，它的求解过程实质上是一个先序遍历一颗状态树的过程，只是这一棵树不是遍历前预先建立的，而是隐含在遍历过程中。\n\n### 二、简单示例\n\n经典回溯的例子是求n个元素的幂集，幂集简单点说就是n个元素的全组合再加上空集，它的解空间树（状态树）如下：\n\n![clip_image002](/images/2012/02/clip_image0022.jpg)\n\n上面的树是一颗满二叉树，树中每个结点的状态都是求解过程中可能出现的状态（即解）。递归过程可以简单理解为对n个元素的0、1取舍，伪代码如下：\n\n```\ndef powerset(i : int):\n    if i > n Then: print\n    else:\n        取第i个元素（1）; powerset(i+1);\n        舍第i个元素（0）; powerset(i+1);\n```\n\n### 三、问题说明\n\n很多问题用回溯法求解时，描述过程的树不是一颗满二叉树，这类问题在求解之前需要确定问题的约束函数：&ldquo;约束函数是根据题意定出的，通过描述合法解的一般特征用于去除不合法的解（剪支），从而避免继续搜索出这个不合法解的剩余部分。因此，约束函数是对于任何状态空间树上的节点都有效、等价的。&rdquo;\n\n以四皇后问题为例，剪支后的状态树如下：\n\n![clip_image004](/images/2012/02/clip_image0041.jpg)\n\n伪代码如下：\n\n```\ndef trial(i : int):\n    if i>n Then: print\n    else:\n        for j in [0, n):\n            在(i, j)处放置棋子;\n            if 布局合法 Then: trail(i+1);\n            拿走(i, j)处的棋子\n```\n\n### 4、浅谈搜索\n\n上面所说的回溯法在算法导论中没有提到，觉着有些奇怪，但是仔细一想，好像又不那么奇怪。毕竟，回溯法是一种建立查找树、遍历树的递归应用算法，这些基础的知识在算法导论中均有详细解释，书中没单独辟章说明也不足为奇了。\n\n和朋友[@烟雨华年](http://weibo.com/liangke723)讨论这个的时候，他提到：将整个搜索状态树中的每一个结点看作一种状态，搜索一般是求一种状态到指定状态有没有可行的变化，或者最佳的变化。个人觉得这句概括得还是挺到位的，大部分搜索都是检索从初始状态（状态树的根）到某个状态的变化，而可行的变化即验证是否存在解，最佳的变化即求最优解。\n\n说到这里又不得不说搜索两种最基本的方式，深度优先搜索（DFS）和广度优先搜索（BFS），网络上都是简单的描述一些，并没对它们的运用有一个很好的解释，那么这两种方式如何更好的运用呢？\n\n简单点说，就是验证最优解用DFS，搜索最优解——解空间小用BFS。\n\n1）DFS有着内存需要相对较少的优点，可以利用栈在有限的空间内遍历所有的解，如前面提到的回溯法；\n\n2）BFS类似于树的分层遍历，有限空间的情况下无法使用。但是由于它分层的特性，可以保证当前搜索到的解都是最优解，可以不用完全遍历完解空间的情况下，找到最优解。如最短路径算法。\n\n网上还有一个有趣的说法：一个行不通就用另外一个！哈哈，想图省事的话，找一些模拟数据来跑一圈，是驴是马很快就可以见分晓啦！\n\n### 五、结束语\n\n搜索，算法中最重要的一个环节，以上仅是对它的粗浅认识，对其理解还有待进一步的深入。\n","source":"_posts/2012/02/algorithm-search.md","raw":"title: 算法设计 - 回溯法到搜索浅谈\ntags:\n  - 算法\nid: 331\ncategories:\n  - 技术分享\ndate: 2012-02-22 00:56:00\n---\n\n### 一、定义说明\n\n回溯法是一个即带有系统性又带有跳跃性的搜索算法，它在问题的解空间树中，按深度优先搜索策略，从根结点出发搜索解空间树。算法搜索至解空间树的任一结点时，先判断该结点是否包含问题的解。如果肯定不包含，则跳过对该结点为根的子树的搜索，逐层向其祖先结点回溯；否则，进入该子树，继续按深度优先策略搜索。\n\n<!--more-->\n\n回溯法是设计递归过程的一种重要方法，它的求解过程实质上是一个先序遍历一颗状态树的过程，只是这一棵树不是遍历前预先建立的，而是隐含在遍历过程中。\n\n### 二、简单示例\n\n经典回溯的例子是求n个元素的幂集，幂集简单点说就是n个元素的全组合再加上空集，它的解空间树（状态树）如下：\n\n![clip_image002](/images/2012/02/clip_image0022.jpg)\n\n上面的树是一颗满二叉树，树中每个结点的状态都是求解过程中可能出现的状态（即解）。递归过程可以简单理解为对n个元素的0、1取舍，伪代码如下：\n\n```\ndef powerset(i : int):\n    if i > n Then: print\n    else:\n        取第i个元素（1）; powerset(i+1);\n        舍第i个元素（0）; powerset(i+1);\n```\n\n### 三、问题说明\n\n很多问题用回溯法求解时，描述过程的树不是一颗满二叉树，这类问题在求解之前需要确定问题的约束函数：&ldquo;约束函数是根据题意定出的，通过描述合法解的一般特征用于去除不合法的解（剪支），从而避免继续搜索出这个不合法解的剩余部分。因此，约束函数是对于任何状态空间树上的节点都有效、等价的。&rdquo;\n\n以四皇后问题为例，剪支后的状态树如下：\n\n![clip_image004](/images/2012/02/clip_image0041.jpg)\n\n伪代码如下：\n\n```\ndef trial(i : int):\n    if i>n Then: print\n    else:\n        for j in [0, n):\n            在(i, j)处放置棋子;\n            if 布局合法 Then: trail(i+1);\n            拿走(i, j)处的棋子\n```\n\n### 4、浅谈搜索\n\n上面所说的回溯法在算法导论中没有提到，觉着有些奇怪，但是仔细一想，好像又不那么奇怪。毕竟，回溯法是一种建立查找树、遍历树的递归应用算法，这些基础的知识在算法导论中均有详细解释，书中没单独辟章说明也不足为奇了。\n\n和朋友[@烟雨华年](http://weibo.com/liangke723)讨论这个的时候，他提到：将整个搜索状态树中的每一个结点看作一种状态，搜索一般是求一种状态到指定状态有没有可行的变化，或者最佳的变化。个人觉得这句概括得还是挺到位的，大部分搜索都是检索从初始状态（状态树的根）到某个状态的变化，而可行的变化即验证是否存在解，最佳的变化即求最优解。\n\n说到这里又不得不说搜索两种最基本的方式，深度优先搜索（DFS）和广度优先搜索（BFS），网络上都是简单的描述一些，并没对它们的运用有一个很好的解释，那么这两种方式如何更好的运用呢？\n\n简单点说，就是验证最优解用DFS，搜索最优解——解空间小用BFS。\n\n1）DFS有着内存需要相对较少的优点，可以利用栈在有限的空间内遍历所有的解，如前面提到的回溯法；\n\n2）BFS类似于树的分层遍历，有限空间的情况下无法使用。但是由于它分层的特性，可以保证当前搜索到的解都是最优解，可以不用完全遍历完解空间的情况下，找到最优解。如最短路径算法。\n\n网上还有一个有趣的说法：一个行不通就用另外一个！哈哈，想图省事的话，找一些模拟数据来跑一圈，是驴是马很快就可以见分晓啦！\n\n### 五、结束语\n\n搜索，算法中最重要的一个环节，以上仅是对它的粗浅认识，对其理解还有待进一步的深入。\n","slug":"2012/02/algorithm-search","published":1,"updated":"2015-12-29T15:33:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba940007f3x8f9e6iz1zo"},{"title":"算法设计 - 概率初探","id":"320","date":"2012-02-16T04:36:40.000Z","_content":"\n### 一、问题描述\n\n有n个元素，需要随机选择m个，且要保证每个元素被选的概率相同。\n\n <!--more-->  \n\n### 二、解决思路\n\n#### 1）解法一\n\n拿这题问朋友的时候，很多人都是说：“一个m长度的for循环，取m次random。”\n\n这个方法看似可以，简单方便，代码也一目了然：\n\n```\n// random() -> (0,1)\nfor i in [0, m):\n    results[i] = data[random()*(n-1)]\n```\n\n但是，这段代码有什么问题吗？结果集中会不会出现重复的结果呢？既然是随机的，那肯定就有可能重复，如何消重就得看解法二了。\n\n#### 2）解法二\n\n要消重的话，就需要将已经选择的元素剔出，不进入下一次迭代过程。这个就和中国古代的“抓阄”是一个概念了，用高中知识解释就是“古典概率模型”——在有限多个基本结果、每个结果出现的可能性相同的条件下，先选与后选概率相同。\n\n将已经选择的元素剔出看似很方便，直接删除即可。但是数据集采用数组形式存储，删除的时间复杂度为线性，有人又说可以改成链表，但是链表存储查找又是线性时间了。那么我们能做的就是不删除元素，只是改变一下它的位置：将已选择的元素移到数组末尾。如下图：\n\n![image](/images/2012/02/image1.png)\n\n代码如下：\n\n```\n// random() -> (0,1)\ntmp := n-1;\nfor i in [0, m):\n    swap(data, random()*tmp, tmp);\n    tmp–;\n```\n\n题外话：以上问题源自实际编码中，同时在编程珠玑 II中也有提到，不过书中所提到是现实生活问题：N个地区，随机选择M个做样本。书中提到了一个很有趣的解法：“将N个地区名字打印出来，剪成均等的字条，放入纸篓摇乱，抓取M条。”作者称之为haha!算法，的确，看到这个算法我也哈哈大笑了很久。有时候，我们如果打破我们的概念壁垒，换一种思维思考问题，是一种解法更是一种乐趣。\n\n### 三、问题升级\n\n问题一升级，就到了Google的经典面试题了：给你一个长度为N的链表。N很大，但你不知道N有多大。你的任务是从这N个元素中随机取出k个元素。你只能遍历这个链表一次。你的算法必须保证取出的元素恰好有k个，且它们是完全随机的（出现概率均等）？\n\n### 四、升级问题解法\n\n不知道N的长度，就不满足古典概型的有限个基本问题的要求。如题目没要求只能遍历一遍，可否求出长度再转换成古典概型呢？怕也不行，如果数据是动态增加的，两次遍历依然无法解决问题。\n\n那么问题就转换成蓄水池抽样（Reservoir Sampling）问题了。算法描述如下：\n\n1）先选择前k个元素，放入结果集中；\n\n2）从k+1开始，取[1, k+1]的random，如果得到的随机数小于k，就替换掉结果集中的相应元素；\n\n3）重复第2步，直到迭代完毕。\n\n伪代码：\n\n```\n// random() -> (0,1)\nfor i in [0, n):\n    if i < k then: // 将前k个元素放入结果集中\n        results[i] := data[i];\n    else:\n        tmp := random() * i;\n        if tmp < k then:\n            swap(results, tmp, data, i); // 交换结果集与数据集的元素\n```\n\n#### 证明1：\n\n1）前k个被选中的概率均为1；\n\n2）第k+1个元素被选中概率为k/k+1，结果集中的元素没被剔出的概率为： 1-p， p = （k / k+1）×（1 / k）。k / k+1为第k+1被选中的概率，1 / k为不幸被剔出的概率。综上，结果集中元素没被剔出的概率也为k / k+1；\n\n3）归纳法，即可证明。\n\n#### 证明2：\n\n同样也可以这样证明，元素a被选中的概率为：\n\n被选中的概率×（后面元素没被选中 + 后面元素被选中 × 没被替换的概率）\n\n![image](/images/2012/02/image2.png)\n\n证明2简明扼要啊！\n\n### 五、总结\n\n其实没啥好总结的，上面所提到仅仅只是概率运用的很小一部分。对概率理解还是得从古典概率模型出发，一般情况下，书上将概率算法大致分为四类：数值概率算法、蒙特卡罗（Monte Carlo）算法、拉斯维加斯（Las Vegas）算法和舍伍德（Sherwood）算法，我也没想通上面的是属于上面四种的哪一种，或许哪种都不是，算法嘛，思想在即可，不必纠结于概念。\n","source":"_posts/2012/02/algorithm-probability.md","raw":"title: 算法设计 - 概率初探\ntags:\n  - 算法\nid: 320\ncategories:\n  - 技术分享\ndate: 2012-02-16 12:36:40\n---\n\n### 一、问题描述\n\n有n个元素，需要随机选择m个，且要保证每个元素被选的概率相同。\n\n <!--more-->  \n\n### 二、解决思路\n\n#### 1）解法一\n\n拿这题问朋友的时候，很多人都是说：“一个m长度的for循环，取m次random。”\n\n这个方法看似可以，简单方便，代码也一目了然：\n\n```\n// random() -> (0,1)\nfor i in [0, m):\n    results[i] = data[random()*(n-1)]\n```\n\n但是，这段代码有什么问题吗？结果集中会不会出现重复的结果呢？既然是随机的，那肯定就有可能重复，如何消重就得看解法二了。\n\n#### 2）解法二\n\n要消重的话，就需要将已经选择的元素剔出，不进入下一次迭代过程。这个就和中国古代的“抓阄”是一个概念了，用高中知识解释就是“古典概率模型”——在有限多个基本结果、每个结果出现的可能性相同的条件下，先选与后选概率相同。\n\n将已经选择的元素剔出看似很方便，直接删除即可。但是数据集采用数组形式存储，删除的时间复杂度为线性，有人又说可以改成链表，但是链表存储查找又是线性时间了。那么我们能做的就是不删除元素，只是改变一下它的位置：将已选择的元素移到数组末尾。如下图：\n\n![image](/images/2012/02/image1.png)\n\n代码如下：\n\n```\n// random() -> (0,1)\ntmp := n-1;\nfor i in [0, m):\n    swap(data, random()*tmp, tmp);\n    tmp–;\n```\n\n题外话：以上问题源自实际编码中，同时在编程珠玑 II中也有提到，不过书中所提到是现实生活问题：N个地区，随机选择M个做样本。书中提到了一个很有趣的解法：“将N个地区名字打印出来，剪成均等的字条，放入纸篓摇乱，抓取M条。”作者称之为haha!算法，的确，看到这个算法我也哈哈大笑了很久。有时候，我们如果打破我们的概念壁垒，换一种思维思考问题，是一种解法更是一种乐趣。\n\n### 三、问题升级\n\n问题一升级，就到了Google的经典面试题了：给你一个长度为N的链表。N很大，但你不知道N有多大。你的任务是从这N个元素中随机取出k个元素。你只能遍历这个链表一次。你的算法必须保证取出的元素恰好有k个，且它们是完全随机的（出现概率均等）？\n\n### 四、升级问题解法\n\n不知道N的长度，就不满足古典概型的有限个基本问题的要求。如题目没要求只能遍历一遍，可否求出长度再转换成古典概型呢？怕也不行，如果数据是动态增加的，两次遍历依然无法解决问题。\n\n那么问题就转换成蓄水池抽样（Reservoir Sampling）问题了。算法描述如下：\n\n1）先选择前k个元素，放入结果集中；\n\n2）从k+1开始，取[1, k+1]的random，如果得到的随机数小于k，就替换掉结果集中的相应元素；\n\n3）重复第2步，直到迭代完毕。\n\n伪代码：\n\n```\n// random() -> (0,1)\nfor i in [0, n):\n    if i < k then: // 将前k个元素放入结果集中\n        results[i] := data[i];\n    else:\n        tmp := random() * i;\n        if tmp < k then:\n            swap(results, tmp, data, i); // 交换结果集与数据集的元素\n```\n\n#### 证明1：\n\n1）前k个被选中的概率均为1；\n\n2）第k+1个元素被选中概率为k/k+1，结果集中的元素没被剔出的概率为： 1-p， p = （k / k+1）×（1 / k）。k / k+1为第k+1被选中的概率，1 / k为不幸被剔出的概率。综上，结果集中元素没被剔出的概率也为k / k+1；\n\n3）归纳法，即可证明。\n\n#### 证明2：\n\n同样也可以这样证明，元素a被选中的概率为：\n\n被选中的概率×（后面元素没被选中 + 后面元素被选中 × 没被替换的概率）\n\n![image](/images/2012/02/image2.png)\n\n证明2简明扼要啊！\n\n### 五、总结\n\n其实没啥好总结的，上面所提到仅仅只是概率运用的很小一部分。对概率理解还是得从古典概率模型出发，一般情况下，书上将概率算法大致分为四类：数值概率算法、蒙特卡罗（Monte Carlo）算法、拉斯维加斯（Las Vegas）算法和舍伍德（Sherwood）算法，我也没想通上面的是属于上面四种的哪一种，或许哪种都不是，算法嘛，思想在即可，不必纠结于概念。\n","slug":"2012/02/algorithm-probability","published":1,"updated":"2015-12-29T15:30:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba943007i3x8fvxjoblvh"},{"title":"算法设计 - 动态规划","id":"317","date":"2012-02-14T12:13:50.000Z","_content":"\n### 一、定义说明\n\n动态规划（Dynamic Programming）是通过组合子问题的解而解决整个问题的。分治法算是是将问题划分成一些独立的子问题，递归地求解各子问题，递归地求解个子问题，然后合并子问题的解而得到原问题的解。而动态规划与此不同，它是用于子问题不是独立的情况，就是各子问题包含公共的子的子问题。\n\n<!--more-->\n\n动态规划常用于求最优解问题，此类问题可能有很多可行解，每个解都有一个值，而我们希望找到一个最优（最大、最小）值的解。需要注意，这样的问题可能有多个最优解，但是我们只求解一个。\n\n动态规划算法的设计分为如下4个步骤：\n\n1. 描述最优解的结构；\n2. 递归定义最优解的值；\n3. 按自底向上的方式计算最优解的值；\n4. 由计算出的结果构造一个最优解。\n\n### 二、简单示例\n\n斐波那契（Fibonacci）数列。相信所有大学生都对这个名字不会陌生，大计基（大学计算机基础）课上老师会给大家一个这样的公式：\n\n> f(n) = f(n-2) + f(n-1);  f(0) =0, f(1) = 1\n\n同时，老师就会给一个递归的解，几行代码了事。期末或国家计算机考试的时候，写出递归解就算过关了。但对于我们程序猿来说，可不是这么回事了，递归算f(20)都算不出来。\n\n不知道拿Fibonacci说事合理不合理，因为Fibonacci数列不存在最优不最优，我们姑且算求得答案算最优解吧。\n\n动态规划有两个要素：**最优子结构与重叠子问题**。\n\n* 最优子结构要素：如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构要素。\n* 子问题重叠要素：子问题重叠要素是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。\n\nFibonacci数列满足最优子结构，那子问题重叠呢？f(n-1)、f(n-2)均要用到f(n-3)的解，但是每次都要重复计算一次，所以也满足子问题重叠性质。\n\n已经描述了Fibonacci的最优解结构，公式也就是其递归解，现在需要做的就是自底向上计算最优解了，伪代码如下：\n\n```\nf[] := new bigint[n];\nf[0] := f[1] := 1;\nfor i in [2, n):\n     f[i] := f[i-1] + f[i-2];\n```\n\n构造出的最优解就是上述代码中的f(n)了。动态规划利用了子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。\n\n### 三、经典例子\n\n接下来汇总一些经典的动态规划问题，但是只给出状态转移方程（递归解）和伪代码。\n\n#### 1）01背包问题\n\n有N件物品和一个容量为W的背包。第i件物品的重量是w[i]，价值是v[i]。求解将哪些物品装入背包可使这些物品的重量总和不超过背包容量，且价值总和最大。为啥叫01背包呢，因为每个物体就一件，放就是1不放就是0。\n\n子问题为`V[i][w]`，定义为：前i件物品恰放入一个容量为w的背包可以获得的最大价值。\n\n状态转移方程：\n\n`V[i][w] = MAX{V[i-1][w], V[i-1][w-w[i]]+v[i]}`\n\n其中，`V[i-1][w]`代表第i件物品没有加入到背包中（为0），`V[i-1][w-w[i]]+v[i]`表示第i件物品加入了（为1），在01中取最大值。\n\n伪代码：\n\n```\nV[][] := new int[n][W]\nfor i in [1, n):\n    for w in (W, w[i-1]]: // 保证w-w[i-1]不越界\n        V[i][w] := MAX{V[i-1][w], V[i-1][w-w[i]]+v[i]}\n```\n\n上面的时间复杂度为O(n*W)，空间复杂度也为O(n*W)。我们可以压缩一下空间，因为我们只需要袋子装满没有，装满后的价值是多少，设V[w]：在w重量下，袋子中物品的最大价值。\n\n状态转移方程：\n\n> `V [w] = MAX{V[w], V[w-w[i]]+v[i]}``\n\n伪代码：\n\n```\nV[]:= new int [W+1]\nfor i in [0, n):\n    for w in [W, w[i]-1]: // 保证w-w[i-1]不越界\n        V[w] := MAX{V[w], V[w-w[i]]+v[i]}\n```\n\n#### 2）最长公共子序列（Longest Common Sequence）\n\n求两数组相同的最长子序列，子序列可以不连续的。状态转移方程：\n\n![clip_image002](/images/2012/02/clip_image0021.jpg)\n\n`c[i][j]`表示在字符串x的i位前与字符串y的j位前最长公共子序列。很容易理解，如果一个x[i]、y[j]相等，`c[i-1][j-1]`加1，不等的话，`c[i][j]`等于`c[i-1][j]`、`c[i][j-1]`中较大的数。\n\n伪代码：\n\n```\nc[][] := new int[len(x)][len(y)]\nfor i in [0, len(x)):\n    for j in [0, len(y));\n        if i==0 || j==0 then:\n            c[i][j] :=0;\n        else if x[i] == y[j] then:\n            c[i-1][j-1] += 1;\n        else:\n            c[i][j]:= MAX{c[i][j-1], c[i-1][j]};\n```\n\n#### 3）最长递增子序列（Longest Increase Sequence）\n\n求一个一维数组（N个元素）中最长递增子序列的长度。状态转移方程：\n\n> `LIS[i+1] = MAX{1, LIS[k] +1}, array[i+1] > array[k], for k in [0, i]`\n\n伪代码：\n\n```\nL[] :=new int[len(array)];\nfor i in [0, len(array)):\n    L[i] := 1 // 初始默认为1\n    for j in [0, j):\n        if array[i] > array[j] && L[j] +1 > L[i]:\n            L[i] = L[j] +1;\n```\n\n### 四、小结\n\n以上整理自**算法导论**、**编程之美**;，要掌握动态规划主要是理解两个要素：最优子结构和重叠子问题。要懂得找到一个好的表格来装子问题，将大问题逐步简化，直到得到状态转移方程。表格即子问题最优解，如背包问题：V[w]为背包重量为w的时候最大价值；LCS问题：`c[i][j]`为x[0, i]、y[0, j]的最大子序列；LIS问题：L[i]为array[0, i]的最大递增序列。\n\n但要完全掌握动态规划仅靠以上几道经典问题是不够的，需要不断思考、分析、实践，才可逐渐掌握这一算法分析设计方法。以上经典问题的解答也是经典的，还有许多需要优化以及修改的地方，但不写在博客里了，有兴趣的可以参考相关书籍，也欢迎留言讨论。\n","source":"_posts/2012/02/algorithm-dynammic-programming.md","raw":"title: 算法设计 - 动态规划\ntags:\n  - 算法\nid: 317\ncategories:\n  - 技术分享\ndate: 2012-02-14 20:13:50\n---\n\n### 一、定义说明\n\n动态规划（Dynamic Programming）是通过组合子问题的解而解决整个问题的。分治法算是是将问题划分成一些独立的子问题，递归地求解各子问题，递归地求解个子问题，然后合并子问题的解而得到原问题的解。而动态规划与此不同，它是用于子问题不是独立的情况，就是各子问题包含公共的子的子问题。\n\n<!--more-->\n\n动态规划常用于求最优解问题，此类问题可能有很多可行解，每个解都有一个值，而我们希望找到一个最优（最大、最小）值的解。需要注意，这样的问题可能有多个最优解，但是我们只求解一个。\n\n动态规划算法的设计分为如下4个步骤：\n\n1. 描述最优解的结构；\n2. 递归定义最优解的值；\n3. 按自底向上的方式计算最优解的值；\n4. 由计算出的结果构造一个最优解。\n\n### 二、简单示例\n\n斐波那契（Fibonacci）数列。相信所有大学生都对这个名字不会陌生，大计基（大学计算机基础）课上老师会给大家一个这样的公式：\n\n> f(n) = f(n-2) + f(n-1);  f(0) =0, f(1) = 1\n\n同时，老师就会给一个递归的解，几行代码了事。期末或国家计算机考试的时候，写出递归解就算过关了。但对于我们程序猿来说，可不是这么回事了，递归算f(20)都算不出来。\n\n不知道拿Fibonacci说事合理不合理，因为Fibonacci数列不存在最优不最优，我们姑且算求得答案算最优解吧。\n\n动态规划有两个要素：**最优子结构与重叠子问题**。\n\n* 最优子结构要素：如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构要素。\n* 子问题重叠要素：子问题重叠要素是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。\n\nFibonacci数列满足最优子结构，那子问题重叠呢？f(n-1)、f(n-2)均要用到f(n-3)的解，但是每次都要重复计算一次，所以也满足子问题重叠性质。\n\n已经描述了Fibonacci的最优解结构，公式也就是其递归解，现在需要做的就是自底向上计算最优解了，伪代码如下：\n\n```\nf[] := new bigint[n];\nf[0] := f[1] := 1;\nfor i in [2, n):\n     f[i] := f[i-1] + f[i-2];\n```\n\n构造出的最优解就是上述代码中的f(n)了。动态规划利用了子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。\n\n### 三、经典例子\n\n接下来汇总一些经典的动态规划问题，但是只给出状态转移方程（递归解）和伪代码。\n\n#### 1）01背包问题\n\n有N件物品和一个容量为W的背包。第i件物品的重量是w[i]，价值是v[i]。求解将哪些物品装入背包可使这些物品的重量总和不超过背包容量，且价值总和最大。为啥叫01背包呢，因为每个物体就一件，放就是1不放就是0。\n\n子问题为`V[i][w]`，定义为：前i件物品恰放入一个容量为w的背包可以获得的最大价值。\n\n状态转移方程：\n\n`V[i][w] = MAX{V[i-1][w], V[i-1][w-w[i]]+v[i]}`\n\n其中，`V[i-1][w]`代表第i件物品没有加入到背包中（为0），`V[i-1][w-w[i]]+v[i]`表示第i件物品加入了（为1），在01中取最大值。\n\n伪代码：\n\n```\nV[][] := new int[n][W]\nfor i in [1, n):\n    for w in (W, w[i-1]]: // 保证w-w[i-1]不越界\n        V[i][w] := MAX{V[i-1][w], V[i-1][w-w[i]]+v[i]}\n```\n\n上面的时间复杂度为O(n*W)，空间复杂度也为O(n*W)。我们可以压缩一下空间，因为我们只需要袋子装满没有，装满后的价值是多少，设V[w]：在w重量下，袋子中物品的最大价值。\n\n状态转移方程：\n\n> `V [w] = MAX{V[w], V[w-w[i]]+v[i]}``\n\n伪代码：\n\n```\nV[]:= new int [W+1]\nfor i in [0, n):\n    for w in [W, w[i]-1]: // 保证w-w[i-1]不越界\n        V[w] := MAX{V[w], V[w-w[i]]+v[i]}\n```\n\n#### 2）最长公共子序列（Longest Common Sequence）\n\n求两数组相同的最长子序列，子序列可以不连续的。状态转移方程：\n\n![clip_image002](/images/2012/02/clip_image0021.jpg)\n\n`c[i][j]`表示在字符串x的i位前与字符串y的j位前最长公共子序列。很容易理解，如果一个x[i]、y[j]相等，`c[i-1][j-1]`加1，不等的话，`c[i][j]`等于`c[i-1][j]`、`c[i][j-1]`中较大的数。\n\n伪代码：\n\n```\nc[][] := new int[len(x)][len(y)]\nfor i in [0, len(x)):\n    for j in [0, len(y));\n        if i==0 || j==0 then:\n            c[i][j] :=0;\n        else if x[i] == y[j] then:\n            c[i-1][j-1] += 1;\n        else:\n            c[i][j]:= MAX{c[i][j-1], c[i-1][j]};\n```\n\n#### 3）最长递增子序列（Longest Increase Sequence）\n\n求一个一维数组（N个元素）中最长递增子序列的长度。状态转移方程：\n\n> `LIS[i+1] = MAX{1, LIS[k] +1}, array[i+1] > array[k], for k in [0, i]`\n\n伪代码：\n\n```\nL[] :=new int[len(array)];\nfor i in [0, len(array)):\n    L[i] := 1 // 初始默认为1\n    for j in [0, j):\n        if array[i] > array[j] && L[j] +1 > L[i]:\n            L[i] = L[j] +1;\n```\n\n### 四、小结\n\n以上整理自**算法导论**、**编程之美**;，要掌握动态规划主要是理解两个要素：最优子结构和重叠子问题。要懂得找到一个好的表格来装子问题，将大问题逐步简化，直到得到状态转移方程。表格即子问题最优解，如背包问题：V[w]为背包重量为w的时候最大价值；LCS问题：`c[i][j]`为x[0, i]、y[0, j]的最大子序列；LIS问题：L[i]为array[0, i]的最大递增序列。\n\n但要完全掌握动态规划仅靠以上几道经典问题是不够的，需要不断思考、分析、实践，才可逐渐掌握这一算法分析设计方法。以上经典问题的解答也是经典的，还有许多需要优化以及修改的地方，但不写在博客里了，有兴趣的可以参考相关书籍，也欢迎留言讨论。\n","slug":"2012/02/algorithm-dynammic-programming","published":1,"updated":"2015-12-29T15:28:14.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba945007l3x8fbpb2zu84"},{"title":"人生苦短，我用Python","id":"243","date":"2012-01-20T08:40:51.000Z","_content":"\n用脚本语言的原因就是因为其语法简单，使用方便，但是好像一直没有把握其中的精华。虽然仔细读过网上转载过N次的[Python少打字小技巧](http://www.joynb.net/blog/archives/120)，但还是无法很好的运用。\n\n <!--more-->  \n\n今天逛论坛的时候，看到了一个网友发了一个[ip地址排序](http://bbs.chinaunix.net/thread-823862-1-1.html)的帖子，代码行数60行，核心代码30多行。没有仔细读，看看评论的时候，看到了另一个解法，总代码6行，核心代码4行。用了列表解析、匿名函数等特性，还有内置函数，代码如下：\n\n``` python\niplist=[‘192.168.1.33′,’10.5.1.3′,’10.5.2.4′,’202.98.96.68′,’13.12.1.1’]  \ndef ip2int(s):  \n    l = [int(i) for i in s.split(‘.’)]  \n    return (l[0] << 24) | (l[1] << 16) | (l[2] << 8 ) | l[3]  \n      \niplist.sort(lambda x, y: cmp(ip2int(x), ip2int(y)))  \nprint iplist \n```\n\n按照[Python少打字小技巧](http://www.joynb.net/blog/archives/120)里的例子，这个代码核心部分可以缩减到2行（其实可以只有1行），不过，可读性就不敢恭维咯。\n\n\n``` python\niplist=[‘192.168.1.33′,’10.5.1.3′,’10.5.2.4′,’202.98.96.68′,’13.12.1.1’]  \np = lambda ip: sum( [ int(k)*v for k, v in zip(ip.split(‘.’), [1<<24, 65536, 256, 1])]);  \niplist.sort(lambda x, y: cmp(p(x), p(y)))  \nprint iplist \n```\n\n既然说到短小精悍，就写写Python的一些特性和函数吧：lambda、map、reduce、filter\n\n**lambda**\n\n匿名函数，和c/c++中的宏定义类似，格式为：lambda [参数] : [表达式]，如：lambda x : x+2，调用的时候可以用一个变量接收这个函数，并传入参数。\n\n``` python\nlam = lambda x : x+2  \nprint lam(2)  \nprint (lambda x : x+2)(2) \n```\n\n有些人觉得lambda也就这点功能，但笔者认为，这才是脚本语言，点题：人生苦短，我用Python！这位[**博主**](http://www.cnblogs.com/coderzh/archive/2010/04/30/python-cookbook-lambda.html)写得不错，可以参考参考。\n\n**map，reduce，filter**\n\n其实lambda很多地方都是用在上面这三个函数上面，map(func,seq)、reduce(func,seq,initial=None)、filter(bool_func,seq)。\n\n* map：对seq中的每一个对象都执行func函数，并返回一个列表；\n* reduce：对seq中的每两个对象依次执行func函数，并返回一个值，如1+2+3+4；\n* filter：过滤掉seq中不符合bool_func的对象。\n\n虽然这三个函数中func都可以使用def来定义，但是如果程序需要的逻辑比较简单的话，用lambda就足够了。\n\n``` python\nls = [1,2,3,4,5]  \n# every item in ls plus 2  \nprint map(lambda x : x+2, ls)  \n# sum(ls)  \nprint reduce(lambda x, y : x+y, ls)  \n# filter the item which less than 2  \nprint filter(lambda x : x > 2 and True or False, ls)  \n```\n","source":"_posts/2012/01/python-chat.md","raw":"title: 人生苦短，我用Python\ntags:\n  - Python\nid: 243\ncategories:\n  - 技术分享\ndate: 2012-01-20 16:40:51\n---\n\n用脚本语言的原因就是因为其语法简单，使用方便，但是好像一直没有把握其中的精华。虽然仔细读过网上转载过N次的[Python少打字小技巧](http://www.joynb.net/blog/archives/120)，但还是无法很好的运用。\n\n <!--more-->  \n\n今天逛论坛的时候，看到了一个网友发了一个[ip地址排序](http://bbs.chinaunix.net/thread-823862-1-1.html)的帖子，代码行数60行，核心代码30多行。没有仔细读，看看评论的时候，看到了另一个解法，总代码6行，核心代码4行。用了列表解析、匿名函数等特性，还有内置函数，代码如下：\n\n``` python\niplist=[‘192.168.1.33′,’10.5.1.3′,’10.5.2.4′,’202.98.96.68′,’13.12.1.1’]  \ndef ip2int(s):  \n    l = [int(i) for i in s.split(‘.’)]  \n    return (l[0] << 24) | (l[1] << 16) | (l[2] << 8 ) | l[3]  \n      \niplist.sort(lambda x, y: cmp(ip2int(x), ip2int(y)))  \nprint iplist \n```\n\n按照[Python少打字小技巧](http://www.joynb.net/blog/archives/120)里的例子，这个代码核心部分可以缩减到2行（其实可以只有1行），不过，可读性就不敢恭维咯。\n\n\n``` python\niplist=[‘192.168.1.33′,’10.5.1.3′,’10.5.2.4′,’202.98.96.68′,’13.12.1.1’]  \np = lambda ip: sum( [ int(k)*v for k, v in zip(ip.split(‘.’), [1<<24, 65536, 256, 1])]);  \niplist.sort(lambda x, y: cmp(p(x), p(y)))  \nprint iplist \n```\n\n既然说到短小精悍，就写写Python的一些特性和函数吧：lambda、map、reduce、filter\n\n**lambda**\n\n匿名函数，和c/c++中的宏定义类似，格式为：lambda [参数] : [表达式]，如：lambda x : x+2，调用的时候可以用一个变量接收这个函数，并传入参数。\n\n``` python\nlam = lambda x : x+2  \nprint lam(2)  \nprint (lambda x : x+2)(2) \n```\n\n有些人觉得lambda也就这点功能，但笔者认为，这才是脚本语言，点题：人生苦短，我用Python！这位[**博主**](http://www.cnblogs.com/coderzh/archive/2010/04/30/python-cookbook-lambda.html)写得不错，可以参考参考。\n\n**map，reduce，filter**\n\n其实lambda很多地方都是用在上面这三个函数上面，map(func,seq)、reduce(func,seq,initial=None)、filter(bool_func,seq)。\n\n* map：对seq中的每一个对象都执行func函数，并返回一个列表；\n* reduce：对seq中的每两个对象依次执行func函数，并返回一个值，如1+2+3+4；\n* filter：过滤掉seq中不符合bool_func的对象。\n\n虽然这三个函数中func都可以使用def来定义，但是如果程序需要的逻辑比较简单的话，用lambda就足够了。\n\n``` python\nls = [1,2,3,4,5]  \n# every item in ls plus 2  \nprint map(lambda x : x+2, ls)  \n# sum(ls)  \nprint reduce(lambda x, y : x+y, ls)  \n# filter the item which less than 2  \nprint filter(lambda x : x > 2 and True or False, ls)  \n```\n","slug":"2012/01/python-chat","published":1,"updated":"2015-12-29T15:22:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba946007o3x8fg1rmkigw"},{"title":"分布式文件系统元数据服务器模型","id":"163","date":"2011-10-01T02:21:35.000Z","_content":"\n随着非结构化数据的爆炸，分布式文件系统进入了发展的黄金时期，从高性能计算到数据中心，从数据共享到互联网应用，已经渗透到数据应用的各方各面。对于大多数分布式文件系统(或集群文件系统，或并行文件系统)而言，通常将元数据与数据两者独立开来，即控制流与数据流进行分离，从而获得更高的系统扩展性和I/O并发性。因而，元数据管理模型显得至关重要，直接影响到系统的扩展性、性能、可靠性和稳定性等。存储系统要具有很高的Scale-Out特性，最大的挑战之一就是记录数据逻辑与物理位置的映像关系即数据元数据，还包括诸如属性和访问权限等信息。特别是对于海量小文件的应用，元数据问题是个非常大的挑战。总体来说，分布式文件系统的元数据管理方式大致可以分为三种模型，即集中式元数据服务模型、分布式元数据服务模型和无元数据服务模型。在学术界和工业界，这三种模型一直存在争议，各有优势和不足之处，实际系统实现中也难分优劣。实际上，设计出一个能够适用各种数据应用负载的通用分布式文件系统，这种想法本来就是不现实的。从这个意义上看，这三种元数据服务模型都有各自存在的理由，至少是在它适用的数据存储应用领域之内。\n\n<!--more-->\n\n**集中式元数据服务模型**\n\n分布式文件系统中，数据和I/O访问负载被分散到多个物理独立的存储和计算节点，从而实现系统的高扩展性和高性能。对于一组文件，如果以文件为单位进行调度，则不同的文件会存储在不同的节点上；或者以Stripe方式进行存储，则一个文件会分成多个部分存放在多个节点。显然，我们面临的一个关键问题就是如何确保对数据进行正确定位和访问，元数据服务正是用来解决这个问题的。元数据服务记录数据逻辑名字与物理信息的映射关系，包含文件访问控制所需要的所有元数据，对文件进行访问时，先向元数据服务请求查询对应的元数据，然后通过获得的元数据进行后续的文件读写等I/O操作。\n\n出于简化系统设计复杂性的考虑，并且由于大量的历史遗留系统等原因，大多数分布式文件系统采用了集中式的元数据服务，如Lustre, PVFS, StorNext, GFS等。集中式元数据服务模型，通常提供一个中央元数据服务器负责元数据的存储和客户端查询请求，它提供统一的文件系统命名空间，并处理名字解析和数据定位等访问控制功能。传统的NAS系统中，I/O数据流需要经过服务器，而分布式文件系统中，I/O数据流不需要经过元数据服务器，由客户端与存储节点直接交互。这个架构上的变革，使得控制流与数据流分离开来，元数据服务器和存储服务器各司其职，系统扩展性和性能上获得了极大的提升。显而易见，集中式元数据服务模型的最大优点就是设计实现简单，本质上相当于设计一个单机应用程序，对外提供网络访问接口即可，如Socket, RPC, HTTP REST或SOAP等。元数据服务设计实现的关键是OPS吞吐量，即单位时间处理的操作数，这对集中式元数据服务模型尤其关键，因为会受到系统Scale-Up方面的限制。为了优化OPS，该模型对CPU、内存、磁盘要求较高，条件允许的情况下尽量使用高性能CPU、大内存和高速磁盘，甚至后端存储可考虑使用高端磁盘阵列或SSD。在软件架构方面设计，应该考虑多进程/线程(池)、异步通信、Cache、事件驱动等实现机制。至于分布式文件系统名字空间的设计实现，请参考“[分布式文件系统名字空间实现研究](http://blog.csdn.net/liuben/article/details/5993604)”一文，这里不再讨论。实际上，集中式元数据服务模型的缺点同样突出，其中两个最为关键的是性能瓶颈和单点故障问题。\n\n性能瓶颈，这种模型下元数据服务器在负载不断增大时将很快成为整个系统性能的瓶颈。根据Amdahl定律，系统性能加速比最终受制于串行部分的比重，这决定了系统使用并行手段所能改进性能的潜力。这里，元数据服务器就是串行的部分，它直接决定着系统的扩展规模和性能。文件元数据的基本特性要求它必须同步地进行维护和更新，任何时候对文件数据或元数据进行操作时，都需要同步更新元数据。例如，文件的访问时间，即使是读操作或列目录都需要对它进行更新。客户端访问分布式文件系统时，都需要先与元数据服务器进行交互，这包括命名空间解析、数据定位、访问控制等，然后才直接与存储节点进行I/O交互。随着系统规模不断扩大，存储节点、磁盘数量、文件数量、客户端数据、文件操作数量等都将急剧增加，而运行元数据服务器的物理服务器性能毕竟终究有限，因此集中式元数据服务器将最终成为性能瓶颈。对于众所周知的LOSF(Lots of Small Files)应用，文件数量众多而且文件很小，通常都是几KB至几十KB的小文件，比如CDN和生命科学DNA数据应用，集中式元数据服务模型的性能瓶颈问题更加严重。LOSF应用主要是大量的元数据操作，元数据服务器一旦出现性能问题，直接导致极低的OPS和I/O吞吐量。目前，以这种模型实现的分布式文件系统都不适合LOSF应用，比如Lustre, PVFS, GFS。\n\n实际上，性能瓶颈问题没有想像中的那么严重，Lustre, StorNext, GFS等在大文件应用下性能极高，StorNext甚至在小文件应该下性能也表现良好。一方面，首先应该尽量避免应用于LOSF，除非对性能要求极低。其次，对于大文件应用，更加强调I/O数据吞吐量，元数据操作所占比例非常小。文件很大时，元数据数量将显著降低，而且系统更多时间是在进行数据传输，元数据服务器压力大幅下降。这种情形下，基本上不存在性能瓶颈问题了。再者，如果出现性能瓶颈问题，在系统可以承载的最大负载前提下，可以对元数据服务器进行性能优化。优化最为直接的方法是升级硬件，比如CPU、内存、存储、网络，摩尔定律目前仍然是有效的。系统级优化通常也是有效的，包括OS裁剪和参数优化，这方面有很大提升空间。元数据服务器设计本身的优化才是最为关键的，它可以帮助用户节约成本、简化维护和管理，优化的方法主要包括数据局部性、Cache、异步I/O等，旨在提高并发性、减少磁盘I/O访问、降低请求处理时间。因此，在非常多的数据应用下，集中式元数据服务器的性能并不是大问题，或者通过性能优化可以解决的。\n\n单点故障(SPOF，Single Point of Failure)，这个问题看上去要比性能瓶颈更加严重。整个系统严重依赖于元数据服务器，一旦出现问题，系统将变得完全不可用，直接导致应用 中断并影响业务连续性。物理服务器所涉及的网络、计算和存储部件以及软件都有可能发生故障，因此单点故障问题潜在的，采用更优的硬件和软件只能降低发生的概率而无法避免。目前，SPOF问题主要是采用HA机制来解决，根据可用性要求的高低，镜像一个或多个元数据服务器(逻辑的或物理的均可)，构成一个元数据服务HA集群。集群中一台作为主元数据服务器，接受和处理来自客户端的请求，并与其他服务器保持同步。当主元数据服务器发生问题时，自动选择一台可用服务器作为新的主服务器，这一过程对上层应用是透明的，不会产生业务中断。HA机制能够解决SPOF问题，但同时增加了成本开销，只有主服务器是活动的，其他服务器均处于非活动状态，对性能提升没有任何帮助。\n\n**分布式元数据服务模型**\n\n自然地有人提出了分布式元数据服务模型，顾名思义就是使用多台服务器构成集群协同为分布式文件系统提供元数据服务，从而消除集中式元数据服务模型的性能瓶颈和单点故障问题。这种模型可以细分为两类，一为全对等模式，即集群中的每个元数据服务器是完全对等的，每个都可以独立对外提供元数据服务，然后集群内部进行元数据同步，保持数据一致性，比如ISILON、LoongStore、CZSS等。另一类为全分布模式，集群中的每个元数据服务器负责部分元数据服务(分区可以重叠)，共同构成完整的元数据服务，比如PanFS, GPFS, Ceph等。分布式元数据服务模型，将负载分散到多台服务器解决了性能瓶颈问题，利用对等的服务器或冗余元数据服务分区解决了单点故障问题。分布式看似非常完善，然而它大大增加了设计实现上的复杂性，同时可能会引入了新的问题，即性能开销和数据一致性问题。\n\n性能开销，分布式系统通常会引由于节点之间的数据同步而引入额外开销，这是因为同步过程中需要使用各种锁和同步机制，以保证数据一致性。如果节点同步问题处理不当，性能开销将对系统扩展性和性能产生较大影响，和集中式元数据模型一样形成性能瓶颈，这就对分布式元数据服务器的设计提出了更高的要求。这种性能开销会抵消一部分采用分布式所带来的性能提升，而且随着元数据服务器数量、文件数量、文件操作、存储系统规模、磁盘数量、文件大小变小、I/O操作随机性等增加而加剧。另外，元数据服务器规模较大时，高并发性元数据访问会导致同步性能开销更加显著。目前，一些分布式文件系统采用高性能网络(如InfiniBand, GibE等)、SSD固态硬盘或SAN磁盘阵列、分布式共享内存(SMP或ccNUMA)等技术进行集群内部的元数据同步和通信。这的确可以明显提高系统性能以抵消同步开销，不过成本方面也徒然增加许多。\n\n数据一致性，这是分布式系统必须面对的难题。分布式元数据服务模型同样面临潜在的系统发生错误的风险，虽然一部分元数据节点发生故障不会致使整个系统宕机，但却可能影响整个系统正常运行或出现访问错误。为了保证高可用性，元数据会被复制到多个节点位置，维护多个副本之间的同步具有很高的风险。如果元数据没有及时同步或者遭受意外破坏，同一个文件的元数据就会出现不一致，从而导致访问文件数据的不一致，直接影响到上层数据应用的正确性。这种风险发生的概率随着系统规模的扩大而大幅增加，因此分布式元数据的同步和并发访问是个巨大的挑战。使用同步方法对元数据进行同步，再结合事务或日志，自然可以解决数据一致性问题，然而这大大降低了系统的并发性，违背了分布式系统的设计初衷。在保证元数据一致性的前提下，尽可能地提高并发性，这就对同步机制和算法设计方面提出了严格要求，复杂性和挑战性不言而喻。\n\n**无元数据服务模型**\n\n既然集中式或分布式元数据服务模型都不能彻底地解决问题，那么直接去掉元数据服务器，是否就可以避免这些问题呢？理论上，无元数据服务模型是完全可行的，寻找到元数据查询定位的替代方法即可。理想情况下，这种模型消除了元数据的性能瓶颈、单点故障、数据一致性等一系列相关问题，系统扩展性显著提高，系统并发性和性能将实现线性扩展增长。目前，基于无元数据服务模型的分布式文件系统可谓凤毛麟角，Glusterfs是其中最为典型的代表。\n\n对于分布式系统而言，元数据处理是决定系统扩展性、性能以及稳定性的关键。GlusterFS另辟蹊径，彻底摒弃了元数据服务，使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务。这根本性解决了元数据这一难题，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问。换句话说，GlusterFS不需要将元数据与数据进行分离，因为文件定位可独立并行化进行。GlusterFS独特地采用无元数据服务的设计，取而代之使用算法来定位文件，元数据和数据没有分离而是一起存储。集群中的所有存储系统服务器都可以智能地对文件数据分片进行定位，仅仅根据文件名和路径并运用算法即可，而不需要查询索引或者其他服务器。这使得数据访问完全并行化，从而实现真正的线性性能扩展。无元数据服务器极大提高了GlusterFS的性能、可靠性和稳定性。(Glusterfs更深入地分析请参考“[Glusterfs集群文件系统研究](http://blog.csdn.net/liuben/article/details/6284551)”一文)。\n\n无元数据服务器设计的好处是没有单点故障和性能瓶颈问题，可提高系统扩展性、性能、可靠性和稳定性。对于海量小文件应用，这种设计能够有效解决元数据的难点问题。它的负面影响是，数据一致问题更加复杂，文件目录遍历操作效率低下，缺乏全局监控管理功能。同时也导致客户端承担了更多的职能，比如文件定位、名字空间缓存、逻辑卷视图维护等等，这些都增加了客户端的负载，占用相当的CPU和内存。\n\n**三种元数据服务模型比较**\n\n对Scale-Out存储系统而言，最大的挑战之一就是记录数据逻辑与物理位置的映像关系，即数据元数据。传统分布式存储系统使用集中式或布式元数据服务来维护元数据，集中式元数据服务会导致单点故障和性能瓶颈问题，而分布式元数据服务存在性能开销、元数据同步一致性和设计复杂性等问题。无元数据服务模型，消除了元数据访问问题，但同时增加了数据本身管理的复杂性，缺乏全局监控管理功能，并增加了客户端的负载。由此可见，这三种模型都不是完美的，分别有各自的优点和不足，没有绝对的优劣与好坏之分，实际选型要根据具体情况选择合适的模型，并想方设法完善其不足之处，从而提高分布式文件系统的扩展性、高性能、可用性等特性。集中式元数据服务模型的代表是Lustre, StorNext, GFS等，分布式元数据服务模型的典型案例有ISILON, GPFS, Ceph等，Glustrefs是无元数据服务模型的经典。以上这些都是非常强大的分布式文件系统，它们是非常好的设计典范。这也足以说明，架构固然非常关键，但具体实现技术却往往决定最后的结局。\n\n**补充阅读**\n[1] Ceph, [http://www.ibm.com/developerworks/cn/linux/l-ceph/index.html?ca=drs-](http://www.ibm.com/developerworks/cn/linux/l-ceph/index.html?ca=drs-)\n[2] Glusterfs, [http://blog.csdn.net/liuben/article/details/6284551](http://blog.csdn.net/liuben/article/details/6284551)\n[3] 集群NAS技术架构，[http://blog.csdn.net/liuben/article/details/6422700](http://blog.csdn.net/liuben/article/details/6422700)\n> **转载：**[**刘爱贵的专栏**](http://blog.csdn.net/liuben/article/details/6749188)\n","source":"_posts/2011/10/dist-filesystem-metadata-server.md","raw":"title: 分布式文件系统元数据服务器模型\ntags:\n  - 分布式\nid: 163\ncategories:\n  - 技术分享\ndate: 2011-10-01 10:21:35\n---\n\n随着非结构化数据的爆炸，分布式文件系统进入了发展的黄金时期，从高性能计算到数据中心，从数据共享到互联网应用，已经渗透到数据应用的各方各面。对于大多数分布式文件系统(或集群文件系统，或并行文件系统)而言，通常将元数据与数据两者独立开来，即控制流与数据流进行分离，从而获得更高的系统扩展性和I/O并发性。因而，元数据管理模型显得至关重要，直接影响到系统的扩展性、性能、可靠性和稳定性等。存储系统要具有很高的Scale-Out特性，最大的挑战之一就是记录数据逻辑与物理位置的映像关系即数据元数据，还包括诸如属性和访问权限等信息。特别是对于海量小文件的应用，元数据问题是个非常大的挑战。总体来说，分布式文件系统的元数据管理方式大致可以分为三种模型，即集中式元数据服务模型、分布式元数据服务模型和无元数据服务模型。在学术界和工业界，这三种模型一直存在争议，各有优势和不足之处，实际系统实现中也难分优劣。实际上，设计出一个能够适用各种数据应用负载的通用分布式文件系统，这种想法本来就是不现实的。从这个意义上看，这三种元数据服务模型都有各自存在的理由，至少是在它适用的数据存储应用领域之内。\n\n<!--more-->\n\n**集中式元数据服务模型**\n\n分布式文件系统中，数据和I/O访问负载被分散到多个物理独立的存储和计算节点，从而实现系统的高扩展性和高性能。对于一组文件，如果以文件为单位进行调度，则不同的文件会存储在不同的节点上；或者以Stripe方式进行存储，则一个文件会分成多个部分存放在多个节点。显然，我们面临的一个关键问题就是如何确保对数据进行正确定位和访问，元数据服务正是用来解决这个问题的。元数据服务记录数据逻辑名字与物理信息的映射关系，包含文件访问控制所需要的所有元数据，对文件进行访问时，先向元数据服务请求查询对应的元数据，然后通过获得的元数据进行后续的文件读写等I/O操作。\n\n出于简化系统设计复杂性的考虑，并且由于大量的历史遗留系统等原因，大多数分布式文件系统采用了集中式的元数据服务，如Lustre, PVFS, StorNext, GFS等。集中式元数据服务模型，通常提供一个中央元数据服务器负责元数据的存储和客户端查询请求，它提供统一的文件系统命名空间，并处理名字解析和数据定位等访问控制功能。传统的NAS系统中，I/O数据流需要经过服务器，而分布式文件系统中，I/O数据流不需要经过元数据服务器，由客户端与存储节点直接交互。这个架构上的变革，使得控制流与数据流分离开来，元数据服务器和存储服务器各司其职，系统扩展性和性能上获得了极大的提升。显而易见，集中式元数据服务模型的最大优点就是设计实现简单，本质上相当于设计一个单机应用程序，对外提供网络访问接口即可，如Socket, RPC, HTTP REST或SOAP等。元数据服务设计实现的关键是OPS吞吐量，即单位时间处理的操作数，这对集中式元数据服务模型尤其关键，因为会受到系统Scale-Up方面的限制。为了优化OPS，该模型对CPU、内存、磁盘要求较高，条件允许的情况下尽量使用高性能CPU、大内存和高速磁盘，甚至后端存储可考虑使用高端磁盘阵列或SSD。在软件架构方面设计，应该考虑多进程/线程(池)、异步通信、Cache、事件驱动等实现机制。至于分布式文件系统名字空间的设计实现，请参考“[分布式文件系统名字空间实现研究](http://blog.csdn.net/liuben/article/details/5993604)”一文，这里不再讨论。实际上，集中式元数据服务模型的缺点同样突出，其中两个最为关键的是性能瓶颈和单点故障问题。\n\n性能瓶颈，这种模型下元数据服务器在负载不断增大时将很快成为整个系统性能的瓶颈。根据Amdahl定律，系统性能加速比最终受制于串行部分的比重，这决定了系统使用并行手段所能改进性能的潜力。这里，元数据服务器就是串行的部分，它直接决定着系统的扩展规模和性能。文件元数据的基本特性要求它必须同步地进行维护和更新，任何时候对文件数据或元数据进行操作时，都需要同步更新元数据。例如，文件的访问时间，即使是读操作或列目录都需要对它进行更新。客户端访问分布式文件系统时，都需要先与元数据服务器进行交互，这包括命名空间解析、数据定位、访问控制等，然后才直接与存储节点进行I/O交互。随着系统规模不断扩大，存储节点、磁盘数量、文件数量、客户端数据、文件操作数量等都将急剧增加，而运行元数据服务器的物理服务器性能毕竟终究有限，因此集中式元数据服务器将最终成为性能瓶颈。对于众所周知的LOSF(Lots of Small Files)应用，文件数量众多而且文件很小，通常都是几KB至几十KB的小文件，比如CDN和生命科学DNA数据应用，集中式元数据服务模型的性能瓶颈问题更加严重。LOSF应用主要是大量的元数据操作，元数据服务器一旦出现性能问题，直接导致极低的OPS和I/O吞吐量。目前，以这种模型实现的分布式文件系统都不适合LOSF应用，比如Lustre, PVFS, GFS。\n\n实际上，性能瓶颈问题没有想像中的那么严重，Lustre, StorNext, GFS等在大文件应用下性能极高，StorNext甚至在小文件应该下性能也表现良好。一方面，首先应该尽量避免应用于LOSF，除非对性能要求极低。其次，对于大文件应用，更加强调I/O数据吞吐量，元数据操作所占比例非常小。文件很大时，元数据数量将显著降低，而且系统更多时间是在进行数据传输，元数据服务器压力大幅下降。这种情形下，基本上不存在性能瓶颈问题了。再者，如果出现性能瓶颈问题，在系统可以承载的最大负载前提下，可以对元数据服务器进行性能优化。优化最为直接的方法是升级硬件，比如CPU、内存、存储、网络，摩尔定律目前仍然是有效的。系统级优化通常也是有效的，包括OS裁剪和参数优化，这方面有很大提升空间。元数据服务器设计本身的优化才是最为关键的，它可以帮助用户节约成本、简化维护和管理，优化的方法主要包括数据局部性、Cache、异步I/O等，旨在提高并发性、减少磁盘I/O访问、降低请求处理时间。因此，在非常多的数据应用下，集中式元数据服务器的性能并不是大问题，或者通过性能优化可以解决的。\n\n单点故障(SPOF，Single Point of Failure)，这个问题看上去要比性能瓶颈更加严重。整个系统严重依赖于元数据服务器，一旦出现问题，系统将变得完全不可用，直接导致应用 中断并影响业务连续性。物理服务器所涉及的网络、计算和存储部件以及软件都有可能发生故障，因此单点故障问题潜在的，采用更优的硬件和软件只能降低发生的概率而无法避免。目前，SPOF问题主要是采用HA机制来解决，根据可用性要求的高低，镜像一个或多个元数据服务器(逻辑的或物理的均可)，构成一个元数据服务HA集群。集群中一台作为主元数据服务器，接受和处理来自客户端的请求，并与其他服务器保持同步。当主元数据服务器发生问题时，自动选择一台可用服务器作为新的主服务器，这一过程对上层应用是透明的，不会产生业务中断。HA机制能够解决SPOF问题，但同时增加了成本开销，只有主服务器是活动的，其他服务器均处于非活动状态，对性能提升没有任何帮助。\n\n**分布式元数据服务模型**\n\n自然地有人提出了分布式元数据服务模型，顾名思义就是使用多台服务器构成集群协同为分布式文件系统提供元数据服务，从而消除集中式元数据服务模型的性能瓶颈和单点故障问题。这种模型可以细分为两类，一为全对等模式，即集群中的每个元数据服务器是完全对等的，每个都可以独立对外提供元数据服务，然后集群内部进行元数据同步，保持数据一致性，比如ISILON、LoongStore、CZSS等。另一类为全分布模式，集群中的每个元数据服务器负责部分元数据服务(分区可以重叠)，共同构成完整的元数据服务，比如PanFS, GPFS, Ceph等。分布式元数据服务模型，将负载分散到多台服务器解决了性能瓶颈问题，利用对等的服务器或冗余元数据服务分区解决了单点故障问题。分布式看似非常完善，然而它大大增加了设计实现上的复杂性，同时可能会引入了新的问题，即性能开销和数据一致性问题。\n\n性能开销，分布式系统通常会引由于节点之间的数据同步而引入额外开销，这是因为同步过程中需要使用各种锁和同步机制，以保证数据一致性。如果节点同步问题处理不当，性能开销将对系统扩展性和性能产生较大影响，和集中式元数据模型一样形成性能瓶颈，这就对分布式元数据服务器的设计提出了更高的要求。这种性能开销会抵消一部分采用分布式所带来的性能提升，而且随着元数据服务器数量、文件数量、文件操作、存储系统规模、磁盘数量、文件大小变小、I/O操作随机性等增加而加剧。另外，元数据服务器规模较大时，高并发性元数据访问会导致同步性能开销更加显著。目前，一些分布式文件系统采用高性能网络(如InfiniBand, GibE等)、SSD固态硬盘或SAN磁盘阵列、分布式共享内存(SMP或ccNUMA)等技术进行集群内部的元数据同步和通信。这的确可以明显提高系统性能以抵消同步开销，不过成本方面也徒然增加许多。\n\n数据一致性，这是分布式系统必须面对的难题。分布式元数据服务模型同样面临潜在的系统发生错误的风险，虽然一部分元数据节点发生故障不会致使整个系统宕机，但却可能影响整个系统正常运行或出现访问错误。为了保证高可用性，元数据会被复制到多个节点位置，维护多个副本之间的同步具有很高的风险。如果元数据没有及时同步或者遭受意外破坏，同一个文件的元数据就会出现不一致，从而导致访问文件数据的不一致，直接影响到上层数据应用的正确性。这种风险发生的概率随着系统规模的扩大而大幅增加，因此分布式元数据的同步和并发访问是个巨大的挑战。使用同步方法对元数据进行同步，再结合事务或日志，自然可以解决数据一致性问题，然而这大大降低了系统的并发性，违背了分布式系统的设计初衷。在保证元数据一致性的前提下，尽可能地提高并发性，这就对同步机制和算法设计方面提出了严格要求，复杂性和挑战性不言而喻。\n\n**无元数据服务模型**\n\n既然集中式或分布式元数据服务模型都不能彻底地解决问题，那么直接去掉元数据服务器，是否就可以避免这些问题呢？理论上，无元数据服务模型是完全可行的，寻找到元数据查询定位的替代方法即可。理想情况下，这种模型消除了元数据的性能瓶颈、单点故障、数据一致性等一系列相关问题，系统扩展性显著提高，系统并发性和性能将实现线性扩展增长。目前，基于无元数据服务模型的分布式文件系统可谓凤毛麟角，Glusterfs是其中最为典型的代表。\n\n对于分布式系统而言，元数据处理是决定系统扩展性、性能以及稳定性的关键。GlusterFS另辟蹊径，彻底摒弃了元数据服务，使用弹性哈希算法代替传统分布式文件系统中的集中或分布式元数据服务。这根本性解决了元数据这一难题，从而获得了接近线性的高扩展性，同时也提高了系统性能和可靠性。GlusterFS使用算法进行数据定位，集群中的任何服务器和客户端只需根据路径和文件名就可以对数据进行定位和读写访问。换句话说，GlusterFS不需要将元数据与数据进行分离，因为文件定位可独立并行化进行。GlusterFS独特地采用无元数据服务的设计，取而代之使用算法来定位文件，元数据和数据没有分离而是一起存储。集群中的所有存储系统服务器都可以智能地对文件数据分片进行定位，仅仅根据文件名和路径并运用算法即可，而不需要查询索引或者其他服务器。这使得数据访问完全并行化，从而实现真正的线性性能扩展。无元数据服务器极大提高了GlusterFS的性能、可靠性和稳定性。(Glusterfs更深入地分析请参考“[Glusterfs集群文件系统研究](http://blog.csdn.net/liuben/article/details/6284551)”一文)。\n\n无元数据服务器设计的好处是没有单点故障和性能瓶颈问题，可提高系统扩展性、性能、可靠性和稳定性。对于海量小文件应用，这种设计能够有效解决元数据的难点问题。它的负面影响是，数据一致问题更加复杂，文件目录遍历操作效率低下，缺乏全局监控管理功能。同时也导致客户端承担了更多的职能，比如文件定位、名字空间缓存、逻辑卷视图维护等等，这些都增加了客户端的负载，占用相当的CPU和内存。\n\n**三种元数据服务模型比较**\n\n对Scale-Out存储系统而言，最大的挑战之一就是记录数据逻辑与物理位置的映像关系，即数据元数据。传统分布式存储系统使用集中式或布式元数据服务来维护元数据，集中式元数据服务会导致单点故障和性能瓶颈问题，而分布式元数据服务存在性能开销、元数据同步一致性和设计复杂性等问题。无元数据服务模型，消除了元数据访问问题，但同时增加了数据本身管理的复杂性，缺乏全局监控管理功能，并增加了客户端的负载。由此可见，这三种模型都不是完美的，分别有各自的优点和不足，没有绝对的优劣与好坏之分，实际选型要根据具体情况选择合适的模型，并想方设法完善其不足之处，从而提高分布式文件系统的扩展性、高性能、可用性等特性。集中式元数据服务模型的代表是Lustre, StorNext, GFS等，分布式元数据服务模型的典型案例有ISILON, GPFS, Ceph等，Glustrefs是无元数据服务模型的经典。以上这些都是非常强大的分布式文件系统，它们是非常好的设计典范。这也足以说明，架构固然非常关键，但具体实现技术却往往决定最后的结局。\n\n**补充阅读**\n[1] Ceph, [http://www.ibm.com/developerworks/cn/linux/l-ceph/index.html?ca=drs-](http://www.ibm.com/developerworks/cn/linux/l-ceph/index.html?ca=drs-)\n[2] Glusterfs, [http://blog.csdn.net/liuben/article/details/6284551](http://blog.csdn.net/liuben/article/details/6284551)\n[3] 集群NAS技术架构，[http://blog.csdn.net/liuben/article/details/6422700](http://blog.csdn.net/liuben/article/details/6422700)\n> **转载：**[**刘爱贵的专栏**](http://blog.csdn.net/liuben/article/details/6749188)\n","slug":"2011/10/dist-filesystem-metadata-server","published":1,"updated":"2015-12-29T14:26:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba948007s3x8f26bz42ly"},{"title":"学校与社会学习的不同","id":"118","date":"2011-03-08T16:59:51.000Z","_content":"\n我现在谈这个似乎还挺嫩，但是还是表达一下自己的看法吧，给以后更成熟的我做一个对比。\n\n从小受我爸的熏陶，官场商场酒桌歌厅到处溜达，也见识了不少社会的“陋习”。酒桌文化、烟文化等。可以说，大学前我还算一个小混混吧，留着近似齐肩的头发，穿着长似连衣裙的衣服，踩着牛腿粗的裤脚，带着蝙蝠侠的白色镜框，不过还好我不抽烟不嗜酒不打牌。\n\n<!--more-->\n\n很幸运，我考到了湖南师范大学这个“人文”气息很浓的学校，让我这理科生受了不少熏陶。同时，在这四年的学习生活中，我也开始思考人生，思考我的价值观。我不爱金钱，可能是因为我不“太缺”；我不爱购物，可能是我没相关“品位”；我不爱享乐，可能是因为我一直生活在快乐中。\n\n我一个很好的朋友向往纸醉金迷的生活，我也向往，我向往我内心的“纸醉金迷”。那么，如何让我内心“纸醉金迷”呢？琢磨了这么多年，想到了几种方式。第一，多读书，充实自己；第二，如GZ所说，朋友好，才是真的好；第三，达到自己改变世界的梦想，很难，但是很有诱惑力。\n\n那么，回到我的标题，学校与社会学习的不同。我在思考，如果我没有读大学，那么现在的我是什么样的？可能也有点小成就，但是那时候的我会真正快乐吗？\n\n根据我上面说的，不知道你有没有看出来，我要说的不同在哪。我认为的不同就是这两种知识的吸收内容以及方式不同。大学吸收的基础的知识，社会吸收的应用的知识；大学吸收方方面面的知识，社会吸收为人处世交际的知识；大学扎实的吸收，社会浮躁的吸收。\n\n为什么这么说呢，大学是百科知识的大熔炉，而社会是分门别类的机器工厂，社会只看重利益，人容易迷失自己的方向，而喜欢走捷径去达到想到的目标，这样的方式本身并没有错，但是我觉得会导致精神的空虚，但是想摆脱这样的环境的确很难，有几人能做到出污泥而不染呢？我不能，我能做的，就是尽量少“染”一些污泥，少染的方法只能多读书多见识多交流，以从精神上充实自己。\n\n这也算一个愿意在学校里面再待几年的原因，我想在学校里面再扎实的吸收一些基础的方方面面的知识，让精神在可能的境况下尽可能的多吸收。\n\n现在，你能谈谈你对学校与社会学习的不同吗？顺带问句，你空虚吗？\n","source":"_posts/2011/03/study-school-sociey.md","raw":"title: 学校与社会学习的不同\ntags:\n  - 价值观\nid: 118\ncategories:\n  - 生活分享\ndate: 2011-03-09 00:59:51\n---\n\n我现在谈这个似乎还挺嫩，但是还是表达一下自己的看法吧，给以后更成熟的我做一个对比。\n\n从小受我爸的熏陶，官场商场酒桌歌厅到处溜达，也见识了不少社会的“陋习”。酒桌文化、烟文化等。可以说，大学前我还算一个小混混吧，留着近似齐肩的头发，穿着长似连衣裙的衣服，踩着牛腿粗的裤脚，带着蝙蝠侠的白色镜框，不过还好我不抽烟不嗜酒不打牌。\n\n<!--more-->\n\n很幸运，我考到了湖南师范大学这个“人文”气息很浓的学校，让我这理科生受了不少熏陶。同时，在这四年的学习生活中，我也开始思考人生，思考我的价值观。我不爱金钱，可能是因为我不“太缺”；我不爱购物，可能是我没相关“品位”；我不爱享乐，可能是因为我一直生活在快乐中。\n\n我一个很好的朋友向往纸醉金迷的生活，我也向往，我向往我内心的“纸醉金迷”。那么，如何让我内心“纸醉金迷”呢？琢磨了这么多年，想到了几种方式。第一，多读书，充实自己；第二，如GZ所说，朋友好，才是真的好；第三，达到自己改变世界的梦想，很难，但是很有诱惑力。\n\n那么，回到我的标题，学校与社会学习的不同。我在思考，如果我没有读大学，那么现在的我是什么样的？可能也有点小成就，但是那时候的我会真正快乐吗？\n\n根据我上面说的，不知道你有没有看出来，我要说的不同在哪。我认为的不同就是这两种知识的吸收内容以及方式不同。大学吸收的基础的知识，社会吸收的应用的知识；大学吸收方方面面的知识，社会吸收为人处世交际的知识；大学扎实的吸收，社会浮躁的吸收。\n\n为什么这么说呢，大学是百科知识的大熔炉，而社会是分门别类的机器工厂，社会只看重利益，人容易迷失自己的方向，而喜欢走捷径去达到想到的目标，这样的方式本身并没有错，但是我觉得会导致精神的空虚，但是想摆脱这样的环境的确很难，有几人能做到出污泥而不染呢？我不能，我能做的，就是尽量少“染”一些污泥，少染的方法只能多读书多见识多交流，以从精神上充实自己。\n\n这也算一个愿意在学校里面再待几年的原因，我想在学校里面再扎实的吸收一些基础的方方面面的知识，让精神在可能的境况下尽可能的多吸收。\n\n现在，你能谈谈你对学校与社会学习的不同吗？顺带问句，你空虚吗？\n","slug":"2011/03/study-school-sociey","published":1,"updated":"2015-12-29T14:35:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba949007w3x8f975lu7w9"},{"title":"从《社交网络》以及美女排名系统看问题","id":"112","date":"2011-03-06T15:33:20.000Z","_content":"\n![image](/images/ranksystem-in-social-network.png)\n\n大概是2个月前看的这部电影，没有过多关注拍摄手法、演技等电影元素，关注的是[Jesse Eisenberg](http://people.mtime.com/914798/)对Mark的人物演绎。人和人是有差距的，这点我很赞同。那么，我和Mark之间的差距在哪？\n\n<!--more-->\n\n来分析下：\n\n1. Mark是一个Geek（褒义的），我是一个伪Geek。\r2. Mark对技术狂热，热到可以不吃不睡以及禁欲，我对技术热爱，爱到可以不吃晚睡但不会禁欲。\n3. Mark可以心无旁骛的钻研，我可以左顾右盼的学习。\n\n是的，差距就是这样来的，可能我较Mark好的就是，我对衣着的品位还是好一点，毕竟Mark去年被英国时尚杂志《Esquire》评为十大着装品位最差男人之一嘛，哈哈，玩笑话……\n\n来说说正事吧，一直对Mark的Facemash对女孩评分的算法感兴趣，今天琢磨了一下，写写心得体会。\n\n该排名系统出自Elo Rating System，根据维基百科的介绍：\n\nThe Elo rating system is a method for calculating the _relative skill levels_ of players in two-player games such as [chess](http://en.wikipedia.org/wiki/Chess). It is named after its creator [Arpad Elo](http://en.wikipedia.org/wiki/Arpad_Elo), a [Hungarian](http://en.wikipedia.org/wiki/Hungary)-born [American](http://en.wikipedia.org/wiki/United_States) [physics](http://en.wikipedia.org/wiki/Physics) professor.\n\nElo rating system是一个用于计算两人对战模式中参赛者相对技术水平的方法，如象棋。是根据它的创造者匈牙利裔美国物理学家Arpad Elo命名的。\n\nThe Elo system was invented as an improved [chess rating system](http://en.wikipedia.org/wiki/Chess_rating_system), but today it is also used in many other games. It is also used as a rating system for multiplayer competition in a number of [computer games](http://en.wikipedia.org/wiki/Computer_game),[<sup>[1]</sup>](http://en.wikipedia.org/wiki/Elo_rating_system#cite_note-0) and has been adapted to team sports including [association football](http://en.wikipedia.org/wiki/Association_football), American college football and basketball, and [Major League Baseball](http://en.wikipedia.org/wiki/Major_League_Baseball).\n\n这个系统最初设计用来改善国际象棋排名系统，但是现在也用在其它比赛中。同样也可以用在多人竞技的电脑游戏的排名系统，也被团队运动所采纳，如足球比赛、美国大学足球和篮球比赛和棒球联盟比赛。\n\n**Elo假设：**\n\n1.参赛选手在每次比赛中的表现成正态分布；后来普遍认为Logistic分布更为合理（抱歉，由于专业和知识限制，无法解释以及理解Logistic分布）\n\n2.在一局比赛中，赢的一方被认为表现较好，输的一方被认为表现较差；若平局，则双方表现大致相当。虽然这个假设貌似很稀松平常。\n\n**算法如图：**\n\n![clip_image003](/2011/03/clip_image003.png)\n\nEa为选手A的期望表现，Ra为选手A当前的等级分排名。\n\n当选手A和B进行比赛时，可根据公式算出两选手的期望表现。\n\nEa + Eb=1\n\n胜方得1分，负方得0分。（在电影中，不会出现平局）\n\n如果选手的表现比期望要好，那么此选手的排名应该上升。相反，若表现不如期望，则排名会下降。\n\n![clip_image004](/images/2011/03/clip_image004.jpg)\n\nSa为选手A本局的得分（1或0），Ra为选手A的期望表现。K为常数，在大师级象棋赛中通常取16。得到的Ra’为选手本局比赛后的等级分排名。\n\n初始可认为每个人的等级分排名为0。\n\n第一局是A和B进行比赛。此时Ra=Rb=0，Ea=Eb=0.5。\n\n假设本局A胜B负，则A的得分为1，B的得分为0。\n\nRa'=0+16*(1-0.5)=8\n\nRb'=0+16*(0-0.5)=-8\n\n上面的算法过程主要是转载的**豆瓣网友**的，原文请看**参考资料**。\n\n通过Mark创建Facemash给我的启发很大：**第一**，数学非常重要。**第二**，其实看似很高深的东东，放到生活中就会那么有趣。不过，前提得是你知识的深厚与渊博，这也是我考研的一个目的，尽管落榜了T_T。\n\n> **参考资料:**\r>\n> * [http://en.wikipedia.org/wiki/Elo_rating_system](http://en.wikipedia.org/wiki/Elo_rating_system)\r> * [http://www.douban.com/note/122191956/](http://www.douban.com/note/122191956/)\n","source":"_posts/2011/03/ranksystem-in-social-network.md","raw":"title: 从《社交网络》以及美女排名系统看问题\ntags:\n  - 电影\n  - 算法\nid: 112\ncategories:\n  - 技术分享\ndate: 2011-03-06 23:33:20\n---\n\n![image](/images/ranksystem-in-social-network.png)\n\n大概是2个月前看的这部电影，没有过多关注拍摄手法、演技等电影元素，关注的是[Jesse Eisenberg](http://people.mtime.com/914798/)对Mark的人物演绎。人和人是有差距的，这点我很赞同。那么，我和Mark之间的差距在哪？\n\n<!--more-->\n\n来分析下：\n\n1. Mark是一个Geek（褒义的），我是一个伪Geek。\r2. Mark对技术狂热，热到可以不吃不睡以及禁欲，我对技术热爱，爱到可以不吃晚睡但不会禁欲。\n3. Mark可以心无旁骛的钻研，我可以左顾右盼的学习。\n\n是的，差距就是这样来的，可能我较Mark好的就是，我对衣着的品位还是好一点，毕竟Mark去年被英国时尚杂志《Esquire》评为十大着装品位最差男人之一嘛，哈哈，玩笑话……\n\n来说说正事吧，一直对Mark的Facemash对女孩评分的算法感兴趣，今天琢磨了一下，写写心得体会。\n\n该排名系统出自Elo Rating System，根据维基百科的介绍：\n\nThe Elo rating system is a method for calculating the _relative skill levels_ of players in two-player games such as [chess](http://en.wikipedia.org/wiki/Chess). It is named after its creator [Arpad Elo](http://en.wikipedia.org/wiki/Arpad_Elo), a [Hungarian](http://en.wikipedia.org/wiki/Hungary)-born [American](http://en.wikipedia.org/wiki/United_States) [physics](http://en.wikipedia.org/wiki/Physics) professor.\n\nElo rating system是一个用于计算两人对战模式中参赛者相对技术水平的方法，如象棋。是根据它的创造者匈牙利裔美国物理学家Arpad Elo命名的。\n\nThe Elo system was invented as an improved [chess rating system](http://en.wikipedia.org/wiki/Chess_rating_system), but today it is also used in many other games. It is also used as a rating system for multiplayer competition in a number of [computer games](http://en.wikipedia.org/wiki/Computer_game),[<sup>[1]</sup>](http://en.wikipedia.org/wiki/Elo_rating_system#cite_note-0) and has been adapted to team sports including [association football](http://en.wikipedia.org/wiki/Association_football), American college football and basketball, and [Major League Baseball](http://en.wikipedia.org/wiki/Major_League_Baseball).\n\n这个系统最初设计用来改善国际象棋排名系统，但是现在也用在其它比赛中。同样也可以用在多人竞技的电脑游戏的排名系统，也被团队运动所采纳，如足球比赛、美国大学足球和篮球比赛和棒球联盟比赛。\n\n**Elo假设：**\n\n1.参赛选手在每次比赛中的表现成正态分布；后来普遍认为Logistic分布更为合理（抱歉，由于专业和知识限制，无法解释以及理解Logistic分布）\n\n2.在一局比赛中，赢的一方被认为表现较好，输的一方被认为表现较差；若平局，则双方表现大致相当。虽然这个假设貌似很稀松平常。\n\n**算法如图：**\n\n![clip_image003](/2011/03/clip_image003.png)\n\nEa为选手A的期望表现，Ra为选手A当前的等级分排名。\n\n当选手A和B进行比赛时，可根据公式算出两选手的期望表现。\n\nEa + Eb=1\n\n胜方得1分，负方得0分。（在电影中，不会出现平局）\n\n如果选手的表现比期望要好，那么此选手的排名应该上升。相反，若表现不如期望，则排名会下降。\n\n![clip_image004](/images/2011/03/clip_image004.jpg)\n\nSa为选手A本局的得分（1或0），Ra为选手A的期望表现。K为常数，在大师级象棋赛中通常取16。得到的Ra’为选手本局比赛后的等级分排名。\n\n初始可认为每个人的等级分排名为0。\n\n第一局是A和B进行比赛。此时Ra=Rb=0，Ea=Eb=0.5。\n\n假设本局A胜B负，则A的得分为1，B的得分为0。\n\nRa'=0+16*(1-0.5)=8\n\nRb'=0+16*(0-0.5)=-8\n\n上面的算法过程主要是转载的**豆瓣网友**的，原文请看**参考资料**。\n\n通过Mark创建Facemash给我的启发很大：**第一**，数学非常重要。**第二**，其实看似很高深的东东，放到生活中就会那么有趣。不过，前提得是你知识的深厚与渊博，这也是我考研的一个目的，尽管落榜了T_T。\n\n> **参考资料:**\r>\n> * [http://en.wikipedia.org/wiki/Elo_rating_system](http://en.wikipedia.org/wiki/Elo_rating_system)\r> * [http://www.douban.com/note/122191956/](http://www.douban.com/note/122191956/)\n","slug":"2011/03/ranksystem-in-social-network","published":1,"updated":"2015-12-29T15:15:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba94b007z3x8fdktiw8rl"},{"title":"Java中随机数生成是等概率的吗？","id":"127","date":"2011-03-17T14:32:43.000Z","_content":"\n今天群里面问了一个问题：\n> Java里的随机数生成是等概率的吗？\r\r例如：下面这两行代码将从0-99之间随机生成一个数\n\n```java\njava.util.Random ran = new java.util.Random();\nint ConnectID=ran.nextInt(100);\n```\n> 问：生成0-99之间任意一个数字的概率是相等的吗？\n\n没怎么用过Random类，于是乎随意的浏览了一下Random类和nextInt()方法源代码，无奈水平有限，短时间内无法参透其中的奥秘啊。所以来了一个快速验证的方法，统计学中不是说过随机事件的概率，一般可以通过大量重复试验求得其近似值么。于是乎，我也通过大量重复试验来求求其概率吧。\n\n<!--more-->\n\n代码如下：\n\n``` java\njava.util.Random ran = new java.util.Random();\nint connectID;\nint[] ids = new int[100];\nfor (int i = 0; i < 100; i++) {\n  ids[i] = 0;\n}\nfor (int i = 0; i < 10000000; i++) {\n  connectID = ran.nextInt(100);\n  ids[connectID]++;\n}\nfor (int i = 0; i < 100; i++) {\n  System.out.println(i + “: “ + ids[i]);\n}\n```\n\n截一小段结果：\n\n> 0: 99127\r> 1: 99941\n> 2: 100295\n> 3: 100187\n> 4: 100255\n> 5: 100266\n> 6: 100280\n> 7: 99840\n> 8: 100466\n> 9: 99616\n> 10: 100016\n\n剩下的90个数出现的频率和上面的基本一致，大约为10万次。通过上面的实验，基本可以确定为随机数生成是等概率的\n","source":"_posts/2011/03/java-probability.md","raw":"title: Java中随机数生成是等概率的吗？\ntags:\n  - Java\nid: 127\ncategories:\n  - 技术分享\ndate: 2011-03-17 22:32:43\n---\n\n今天群里面问了一个问题：\n> Java里的随机数生成是等概率的吗？\r\r例如：下面这两行代码将从0-99之间随机生成一个数\n\n```java\njava.util.Random ran = new java.util.Random();\nint ConnectID=ran.nextInt(100);\n```\n> 问：生成0-99之间任意一个数字的概率是相等的吗？\n\n没怎么用过Random类，于是乎随意的浏览了一下Random类和nextInt()方法源代码，无奈水平有限，短时间内无法参透其中的奥秘啊。所以来了一个快速验证的方法，统计学中不是说过随机事件的概率，一般可以通过大量重复试验求得其近似值么。于是乎，我也通过大量重复试验来求求其概率吧。\n\n<!--more-->\n\n代码如下：\n\n``` java\njava.util.Random ran = new java.util.Random();\nint connectID;\nint[] ids = new int[100];\nfor (int i = 0; i < 100; i++) {\n  ids[i] = 0;\n}\nfor (int i = 0; i < 10000000; i++) {\n  connectID = ran.nextInt(100);\n  ids[connectID]++;\n}\nfor (int i = 0; i < 100; i++) {\n  System.out.println(i + “: “ + ids[i]);\n}\n```\n\n截一小段结果：\n\n> 0: 99127\r> 1: 99941\n> 2: 100295\n> 3: 100187\n> 4: 100255\n> 5: 100266\n> 6: 100280\n> 7: 99840\n> 8: 100466\n> 9: 99616\n> 10: 100016\n\n剩下的90个数出现的频率和上面的基本一致，大约为10万次。通过上面的实验，基本可以确定为随机数生成是等概率的\n","slug":"2011/03/java-probability","published":1,"updated":"2015-12-30T12:10:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba94d00843x8f8wel8e9n"},{"title":"那些英语标点怎么读","date":"2011-03-08T07:52:59.000Z","_content":"\n```\n+　 plus　加号；正号\n-　 minus　减号；负号\n±　plus or minus　正负号\n×　is multiplied by　乘号\n÷　is divided by　除号\n＝　is equal to　等于号\n≠　is not equal to　不等于号\n≡　is equivalent to　全等于号\n≌　is equal to or approximately equal to　等于或约等于号\n≈　is approximately equal to　约等于号\n＜　is less than　小于号\n＞　is greater than　大于号\n≮　is not less than　不小于号\n≯　is not more than　不大于号\n≤　is less than or equal to　小于或等于号\n≥　is more than or equal to　大于或等于号\n<!--more-->\n%　 per cent　百分之…\n‰　per mill　千分之…\n∞　infinity　无限大号\n∝　varies as　与…成比例\n√　(square) root　平方根\n∵　since; because　因为\n∴　hence　所以\n∷　equals, as (proportion)　等于，成比例\n∠　angle　角\n⌒　semicircle　半圆\n⊙　circle　圆\n○　circumference　圆周\nπ　pi 圆周率\n△　triangle　三角形\n⊥　perpendicular to　垂直于\n∪　union of　并，合集\n∩　intersection of 交，通集\n∫　the integral of …的积分\n∑　(sigma) summation of　总和\n°　degree　度\n′　minute　分\n″　second　秒\n℃　Celsius system　摄氏度\n{　open brace, open curly　左花括号\n}　close brace, close curly　右花括号\n(　open parenthesis, open paren　左圆括号\n)　close parenthesis, close paren　右圆括号\n() brakets/ parentheses　括号\n[　open bracket 左方括号\n]　close bracket 右方括号\n[] square brackets　方括号\n.　period, dot　句号，点\n|　vertical bar, vertical virgule　竖线\n&　ampersand, and, reference, ref　和，引用\n*　asterisk, multiply, star, pointer　星号，乘号，星，指针\n/　slash, divide, oblique 斜线，斜杠，除号\n//　slash-slash, comment 双斜线，注释符\n#　pound　井号\n\\　backslash, sometimes escape　反斜线转义符，有时表示转义符或续行符\n~　tilde　波浪符\n.　full stop　句号\n,　comma　逗号\n:　colon　冒号\n;　semicolon　分号\n?　question mark　问号\n!　exclamation mark (英式英语) exclamation point (美式英语)\n'　apostrophe　撇号\n-　hyphen　连字号\n-- dash 破折号\n...　dots/ ellipsis　省略号\n\"　single quotation marks 单引号\n\"\"　double quotation marks 双引号\n‖ parallel 双线号\n&　ampersand = and\n～　swung dash 代字号\n§　section; division 分节号\n→　arrow 箭号；参见号\n```\n\n> [原文链接](http://blog.sina.com.cn/s/blog_515dc9340100gy95.html)\n","source":"_posts/2011/03/english-punctuate.md","raw":"title: 那些英语标点怎么读\ntags:\n  - 英语\ncategories:\n  - 生活分享\ndate: 2011-03-08 15:52:59\n---\n\n```\n+　 plus　加号；正号\n-　 minus　减号；负号\n±　plus or minus　正负号\n×　is multiplied by　乘号\n÷　is divided by　除号\n＝　is equal to　等于号\n≠　is not equal to　不等于号\n≡　is equivalent to　全等于号\n≌　is equal to or approximately equal to　等于或约等于号\n≈　is approximately equal to　约等于号\n＜　is less than　小于号\n＞　is greater than　大于号\n≮　is not less than　不小于号\n≯　is not more than　不大于号\n≤　is less than or equal to　小于或等于号\n≥　is more than or equal to　大于或等于号\n<!--more-->\n%　 per cent　百分之…\n‰　per mill　千分之…\n∞　infinity　无限大号\n∝　varies as　与…成比例\n√　(square) root　平方根\n∵　since; because　因为\n∴　hence　所以\n∷　equals, as (proportion)　等于，成比例\n∠　angle　角\n⌒　semicircle　半圆\n⊙　circle　圆\n○　circumference　圆周\nπ　pi 圆周率\n△　triangle　三角形\n⊥　perpendicular to　垂直于\n∪　union of　并，合集\n∩　intersection of 交，通集\n∫　the integral of …的积分\n∑　(sigma) summation of　总和\n°　degree　度\n′　minute　分\n″　second　秒\n℃　Celsius system　摄氏度\n{　open brace, open curly　左花括号\n}　close brace, close curly　右花括号\n(　open parenthesis, open paren　左圆括号\n)　close parenthesis, close paren　右圆括号\n() brakets/ parentheses　括号\n[　open bracket 左方括号\n]　close bracket 右方括号\n[] square brackets　方括号\n.　period, dot　句号，点\n|　vertical bar, vertical virgule　竖线\n&　ampersand, and, reference, ref　和，引用\n*　asterisk, multiply, star, pointer　星号，乘号，星，指针\n/　slash, divide, oblique 斜线，斜杠，除号\n//　slash-slash, comment 双斜线，注释符\n#　pound　井号\n\\　backslash, sometimes escape　反斜线转义符，有时表示转义符或续行符\n~　tilde　波浪符\n.　full stop　句号\n,　comma　逗号\n:　colon　冒号\n;　semicolon　分号\n?　question mark　问号\n!　exclamation mark (英式英语) exclamation point (美式英语)\n'　apostrophe　撇号\n-　hyphen　连字号\n-- dash 破折号\n...　dots/ ellipsis　省略号\n\"　single quotation marks 单引号\n\"\"　double quotation marks 双引号\n‖ parallel 双线号\n&　ampersand = and\n～　swung dash 代字号\n§　section; division 分节号\n→　arrow 箭号；参见号\n```\n\n> [原文链接](http://blog.sina.com.cn/s/blog_515dc9340100gy95.html)\n","slug":"2011/03/english-punctuate","published":1,"updated":"2015-12-29T15:12:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciixba94e00873x8fu11d1qfe"}],"PostAsset":[],"PostCategory":[{"post_id":"ciixba91400023x8fjhea3hiw","category_id":"ciixba91600033x8fst7yyl58","_id":"ciixba91800063x8fe8z0eues"},{"post_id":"ciixba91900073x8fg46h5uvz","category_id":"ciixba91600033x8fst7yyl58","_id":"ciixba91900083x8fqhdo83pp"},{"post_id":"ciixba91b000f3x8f5at6t47y","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91b000j3x8fkm9s2ahe"},{"post_id":"ciixba91c000l3x8frog3l52g","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91d000m3x8fwgro1jkt"},{"post_id":"ciixba91e000r3x8fg6cm4qqq","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91f000s3x8fg0j07620"},{"post_id":"ciixba91g000w3x8fizaap2wx","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91g000x3x8f52koy40h"},{"post_id":"ciixba91i000z3x8fxh74snht","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91i00103x8ftlch27m3"},{"post_id":"ciixba91k00133x8fiuoohec4","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91k00143x8f8wop4gw1"},{"post_id":"ciixba91m00193x8f7xbhre62","category_id":"ciixba91b000g3x8fk3t3bvpx","_id":"ciixba91m001a3x8fp6aetckd"},{"post_id":"ciixba91n001g3x8fc6uue4jh","category_id":"ciixba91o001h3x8fb87criik","_id":"ciixba91p001k3x8fldli9ndi"},{"post_id":"ciixba91q001p3x8fqygs3v42","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba91r001t3x8fio6mf039"},{"post_id":"ciixba91s001x3x8fzd532v71","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba91t001y3x8f9qqa0kzt"},{"post_id":"ciixba91u00243x8ffljdbbw3","category_id":"ciixba91o001h3x8fb87criik","_id":"ciixba91v00253x8fwyrerl4o"},{"post_id":"ciixba91x002c3x8ffayl09am","category_id":"ciixba91o001h3x8fb87criik","_id":"ciixba91y002d3x8ftf4a7yiq"},{"post_id":"ciixba920002h3x8f12pn122x","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba920002i3x8fvt9l34mq"},{"post_id":"ciixba921002l3x8fzjtnij0q","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba922002m3x8f8yzd4lqh"},{"post_id":"ciixba923002o3x8fspszqe9m","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba923002p3x8fp2ogx733"},{"post_id":"ciixba926002r3x8fb3ujanur","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba927002s3x8ft1vik4yg"},{"post_id":"ciixba928002v3x8fivkzerg8","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba929002w3x8fnr77f6dz"},{"post_id":"ciixba92a002z3x8flunhwmw0","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92b00303x8futxav3yb"},{"post_id":"ciixba92c00333x8f5rsbmknu","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92c00343x8f94abucrf"},{"post_id":"ciixba92d00363x8f2c7cacf2","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92e00373x8f5s92odil"},{"post_id":"ciixba92f003b3x8fo1obvqla","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92f003c3x8fcf1491r3"},{"post_id":"ciixba92g003f3x8fugob9x2z","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92h003g3x8fdth0d17j"},{"post_id":"ciixba92h003k3x8ff96p6t4n","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92i003l3x8f5172fee1"},{"post_id":"ciixba92k003o3x8f706hbn6w","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92k003p3x8f73cdfdrv"},{"post_id":"ciixba92l003r3x8fraw7plxd","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92m003s3x8fs613xk7h"},{"post_id":"ciixba92n003u3x8ft9onmyvf","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92n003v3x8fa5uh6yip"},{"post_id":"ciixba92o00403x8f6pmfhcof","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92p00413x8fsvau7dyx"},{"post_id":"ciixba92q00443x8fc98tuxba","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92q00453x8fdv6t27bu"},{"post_id":"ciixba92u00483x8fa5z2sw25","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92v00493x8fkxfvxpxj"},{"post_id":"ciixba92w004c3x8fh2ojdyxf","category_id":"ciixba92w004d3x8fky8i432j","_id":"ciixba92w004f3x8fc1bi911i"},{"post_id":"ciixba92x004g3x8f69chd0lm","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92x004h3x8f91515nwj"},{"post_id":"ciixba92y004k3x8ff38avcou","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba92z004l3x8fgz208c6u"},{"post_id":"ciixba930004q3x8fv69ihm4a","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba930004r3x8fumsh4yuv"},{"post_id":"ciixba931004u3x8fianviynf","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba931004v3x8ffjvz1uwq"},{"post_id":"ciixba932004x3x8fq7entvla","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba933004y3x8fkeuieo74"},{"post_id":"ciixba93400533x8fvzmqtrc2","category_id":"ciixba92w004d3x8fky8i432j","_id":"ciixba93400543x8f0h504u1n"},{"post_id":"ciixba93500573x8flv87dcot","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93600583x8fgcqxkub9"},{"post_id":"ciixba937005d3x8f3jdsygqk","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba937005e3x8fyqbvvjlz"},{"post_id":"ciixba938005i3x8fsucb768a","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba939005j3x8fzn0wrpnl"},{"post_id":"ciixba93a005m3x8f7rgkta39","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93a005n3x8fwero30zf"},{"post_id":"ciixba93c005t3x8ff9z19p5v","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93d005u3x8fpnhbt94d"},{"post_id":"ciixba93e005y3x8f1b14ubcz","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93f005z3x8f82hfe1zd"},{"post_id":"ciixba93h00653x8fvpecf1l8","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93i00663x8fdlfz768v"},{"post_id":"ciixba93j00683x8f4x6awurv","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93j00693x8f1n6ymurg"},{"post_id":"ciixba93k006b3x8fvwoogfg3","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93l006c3x8fy1k7athf"},{"post_id":"ciixba93l006e3x8f72kgd4iy","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93m006f3x8flqc10gf9"},{"post_id":"ciixba93n006h3x8f2zj104f0","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93n006i3x8fx702glkn"},{"post_id":"ciixba93o006l3x8fdo11rek9","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93p006m3x8f6ic5nb8f"},{"post_id":"ciixba93q006o3x8fq7frllex","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93q006p3x8fs3t2963w"},{"post_id":"ciixba93r006s3x8f6xj4ufr4","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93r006t3x8f8iabom27"},{"post_id":"ciixba93s006y3x8f95j31wof","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93t006z3x8fdp6v7olq"},{"post_id":"ciixba93u00723x8fln18ucz0","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93u00733x8fbc458ika"},{"post_id":"ciixba93v00773x8f3d5pi7bm","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93w00783x8fkae1tc8s"},{"post_id":"ciixba93x007b3x8fx9pac8zx","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba93z007c3x8ffiog77tv"},{"post_id":"ciixba940007f3x8f9e6iz1zo","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba942007g3x8fmp2rg5i1"},{"post_id":"ciixba943007i3x8fvxjoblvh","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba944007j3x8f9lfsffq3"},{"post_id":"ciixba945007l3x8fbpb2zu84","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba945007m3x8fd6w27rh4"},{"post_id":"ciixba946007o3x8fg1rmkigw","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba947007p3x8fk3qbhvmi"},{"post_id":"ciixba948007s3x8f26bz42ly","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba948007t3x8fl81w3lun"},{"post_id":"ciixba949007w3x8f975lu7w9","category_id":"ciixba92w004d3x8fky8i432j","_id":"ciixba94a007x3x8fk6sbpwsn"},{"post_id":"ciixba94b007z3x8fdktiw8rl","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba94c00803x8f777zl1yn"},{"post_id":"ciixba94d00843x8f8wel8e9n","category_id":"ciixba91q001q3x8fh9x4ielv","_id":"ciixba94e00853x8fw3favl5v"},{"post_id":"ciixba94e00873x8fu11d1qfe","category_id":"ciixba92w004d3x8fky8i432j","_id":"ciixba94g00883x8f83b25pdr"}],"PostTag":[{"post_id":"ciixba91400023x8fjhea3hiw","tag_id":"ciixba91600043x8f5ijo8oj8","_id":"ciixba91700053x8ft3cpwc31"},{"post_id":"ciixba91900073x8fg46h5uvz","tag_id":"ciixba91900093x8ffskezy4o","_id":"ciixba91a000c3x8fhxvi8tnx"},{"post_id":"ciixba91900073x8fg46h5uvz","tag_id":"ciixba91a000a3x8f1pobfwy5","_id":"ciixba91a000d3x8fyf95yczx"},{"post_id":"ciixba91900073x8fg46h5uvz","tag_id":"ciixba91a000b3x8fkhmimyer","_id":"ciixba91a000e3x8fd7gsqda5"},{"post_id":"ciixba91b000f3x8f5at6t47y","tag_id":"ciixba91900093x8ffskezy4o","_id":"ciixba91b000i3x8f3mhvxf56"},{"post_id":"ciixba91b000f3x8f5at6t47y","tag_id":"ciixba91b000h3x8flzrs53b3","_id":"ciixba91b000k3x8ffqb9lyhj"},{"post_id":"ciixba91c000l3x8frog3l52g","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba91d000p3x8fr3itova1"},{"post_id":"ciixba91c000l3x8frog3l52g","tag_id":"ciixba91d000o3x8fwcf9grgy","_id":"ciixba91d000q3x8fsbtzhdcp"},{"post_id":"ciixba91e000r3x8fg6cm4qqq","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba91f000u3x8fz2fb4xgb"},{"post_id":"ciixba91e000r3x8fg6cm4qqq","tag_id":"ciixba91f000t3x8fss4f3gth","_id":"ciixba91f000v3x8f6k295dni"},{"post_id":"ciixba91g000w3x8fizaap2wx","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba91h000y3x8f7dnenqmt"},{"post_id":"ciixba91i000z3x8fxh74snht","tag_id":"ciixba91i00113x8fnpq7jvbo","_id":"ciixba91j00123x8f8alkcjo0"},{"post_id":"ciixba91k00133x8fiuoohec4","tag_id":"ciixba91l00153x8fo93tjdap","_id":"ciixba91l00173x8fnlb4axeu"},{"post_id":"ciixba91k00133x8fiuoohec4","tag_id":"ciixba91l00163x8f3zhpt33o","_id":"ciixba91l00183x8fpqqe9qeg"},{"post_id":"ciixba91m00193x8f7xbhre62","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba91n001d3x8fxxwctpfm"},{"post_id":"ciixba91m00193x8f7xbhre62","tag_id":"ciixba91m001b3x8fjfc0aca9","_id":"ciixba91n001e3x8f1vji7cwt"},{"post_id":"ciixba91m00193x8f7xbhre62","tag_id":"ciixba91n001c3x8frcbp391b","_id":"ciixba91n001f3x8f500scqva"},{"post_id":"ciixba91n001g3x8fc6uue4jh","tag_id":"ciixba91o001i3x8f5r9gq9di","_id":"ciixba91p001m3x8f5eha5p2r"},{"post_id":"ciixba91n001g3x8fc6uue4jh","tag_id":"ciixba91o001j3x8f3alyym3m","_id":"ciixba91p001n3x8fvy93308j"},{"post_id":"ciixba91n001g3x8fc6uue4jh","tag_id":"ciixba91p001l3x8f6edzea6p","_id":"ciixba91p001o3x8foxu9c2ly"},{"post_id":"ciixba91q001p3x8fqygs3v42","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba91r001u3x8fi5mhtb90"},{"post_id":"ciixba91q001p3x8fqygs3v42","tag_id":"ciixba91r001r3x8fmkmwazyu","_id":"ciixba91s001v3x8fkjr9ovf9"},{"post_id":"ciixba91q001p3x8fqygs3v42","tag_id":"ciixba91r001s3x8fqtqkwhla","_id":"ciixba91s001w3x8fwalcg8xs"},{"post_id":"ciixba91s001x3x8fzd532v71","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba91t00203x8fjaw29ryq"},{"post_id":"ciixba91s001x3x8fzd532v71","tag_id":"ciixba91r001r3x8fmkmwazyu","_id":"ciixba91u00213x8f4lffglz4"},{"post_id":"ciixba91s001x3x8fzd532v71","tag_id":"ciixba91r001s3x8fqtqkwhla","_id":"ciixba91u00223x8f8f4zlbfo"},{"post_id":"ciixba91s001x3x8fzd532v71","tag_id":"ciixba91t001z3x8f7ktbb5lr","_id":"ciixba91u00233x8fboyzyjqr"},{"post_id":"ciixba91u00243x8ffljdbbw3","tag_id":"ciixba91v00263x8fx9e8mcba","_id":"ciixba91w00283x8f75964jo1"},{"post_id":"ciixba91u00243x8ffljdbbw3","tag_id":"ciixba91w00273x8f3so2wykg","_id":"ciixba91w00293x8fwk9szjly"},{"post_id":"ciixba91u00243x8ffljdbbw3","tag_id":"ciixba91o001j3x8f3alyym3m","_id":"ciixba91w002a3x8f3ouy7dg2"},{"post_id":"ciixba91u00243x8ffljdbbw3","tag_id":"ciixba91p001l3x8f6edzea6p","_id":"ciixba91w002b3x8fehg7k05e"},{"post_id":"ciixba91x002c3x8ffayl09am","tag_id":"ciixba91y002e3x8fjrkcko5g","_id":"ciixba91z002f3x8fvzi09zjx"},{"post_id":"ciixba91x002c3x8ffayl09am","tag_id":"ciixba91p001l3x8f6edzea6p","_id":"ciixba91z002g3x8f82znzsu6"},{"post_id":"ciixba920002h3x8f12pn122x","tag_id":"ciixba91r001s3x8fqtqkwhla","_id":"ciixba920002j3x8f0bfqz41k"},{"post_id":"ciixba920002h3x8f12pn122x","tag_id":"ciixba91t001z3x8f7ktbb5lr","_id":"ciixba921002k3x8fwmmyw435"},{"post_id":"ciixba921002l3x8fzjtnij0q","tag_id":"ciixba91r001s3x8fqtqkwhla","_id":"ciixba922002n3x8ftm09dnk8"},{"post_id":"ciixba923002o3x8fspszqe9m","tag_id":"ciixba91r001s3x8fqtqkwhla","_id":"ciixba924002q3x8fbs5giv5x"},{"post_id":"ciixba926002r3x8fb3ujanur","tag_id":"ciixba927002t3x8fzm0w2gml","_id":"ciixba928002u3x8fm62rvgjn"},{"post_id":"ciixba928002v3x8fivkzerg8","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba929002x3x8fkyp9deh3"},{"post_id":"ciixba928002v3x8fivkzerg8","tag_id":"ciixba91m001b3x8fjfc0aca9","_id":"ciixba929002y3x8fbyo50cxl"},{"post_id":"ciixba92a002z3x8flunhwmw0","tag_id":"ciixba92b00313x8ff86vi09r","_id":"ciixba92b00323x8fgp5h37em"},{"post_id":"ciixba92c00333x8f5rsbmknu","tag_id":"ciixba92b00313x8ff86vi09r","_id":"ciixba92c00353x8fve5qd1bq"},{"post_id":"ciixba92d00363x8f2c7cacf2","tag_id":"ciixba92b00313x8ff86vi09r","_id":"ciixba92e00393x8f0kgjnnsk"},{"post_id":"ciixba92d00363x8f2c7cacf2","tag_id":"ciixba92e00383x8f6226fige","_id":"ciixba92e003a3x8f57mxivth"},{"post_id":"ciixba92f003b3x8fo1obvqla","tag_id":"ciixba92b00313x8ff86vi09r","_id":"ciixba92f003d3x8fk9mshwju"},{"post_id":"ciixba92f003b3x8fo1obvqla","tag_id":"ciixba92e00383x8f6226fige","_id":"ciixba92f003e3x8fuggspgvs"},{"post_id":"ciixba92g003f3x8fugob9x2z","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba92h003i3x8fm5jhps6b"},{"post_id":"ciixba92g003f3x8fugob9x2z","tag_id":"ciixba92h003h3x8fgqm6b6sx","_id":"ciixba92h003j3x8fs4dc7ji1"},{"post_id":"ciixba92h003k3x8ff96p6t4n","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba92i003m3x8f4lm081mo"},{"post_id":"ciixba92h003k3x8ff96p6t4n","tag_id":"ciixba92h003h3x8fgqm6b6sx","_id":"ciixba92j003n3x8fzqlyw829"},{"post_id":"ciixba92k003o3x8f706hbn6w","tag_id":"ciixba92b00313x8ff86vi09r","_id":"ciixba92k003q3x8fi07hp50b"},{"post_id":"ciixba92l003r3x8fraw7plxd","tag_id":"ciixba92b00313x8ff86vi09r","_id":"ciixba92m003t3x8ff51y97o3"},{"post_id":"ciixba92n003u3x8ft9onmyvf","tag_id":"ciixba92n003w3x8fntic5ak2","_id":"ciixba92o003y3x8fhvpoviwn"},{"post_id":"ciixba92n003u3x8ft9onmyvf","tag_id":"ciixba92o003x3x8ffydwvq6l","_id":"ciixba92o003z3x8fwk4etquu"},{"post_id":"ciixba92o00403x8f6pmfhcof","tag_id":"ciixba92n003w3x8fntic5ak2","_id":"ciixba92p00423x8fffdh350f"},{"post_id":"ciixba92o00403x8f6pmfhcof","tag_id":"ciixba92o003x3x8ffydwvq6l","_id":"ciixba92p00433x8ftx2zpv1m"},{"post_id":"ciixba92q00443x8fc98tuxba","tag_id":"ciixba92n003w3x8fntic5ak2","_id":"ciixba92q00463x8f4da282sf"},{"post_id":"ciixba92q00443x8fc98tuxba","tag_id":"ciixba92o003x3x8ffydwvq6l","_id":"ciixba92q00473x8favjr8uts"},{"post_id":"ciixba92u00483x8fa5z2sw25","tag_id":"ciixba92n003w3x8fntic5ak2","_id":"ciixba92v004a3x8fszjkuzqa"},{"post_id":"ciixba92u00483x8fa5z2sw25","tag_id":"ciixba92o003x3x8ffydwvq6l","_id":"ciixba92v004b3x8fqnlbcclx"},{"post_id":"ciixba92w004c3x8fh2ojdyxf","tag_id":"ciixba91p001l3x8f6edzea6p","_id":"ciixba92w004e3x8facd2opf1"},{"post_id":"ciixba92x004g3x8f69chd0lm","tag_id":"ciixba92x004i3x8f97ot366l","_id":"ciixba92y004j3x8fuzq0w060"},{"post_id":"ciixba92y004k3x8ff38avcou","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba92z004o3x8f61gopr6a"},{"post_id":"ciixba92y004k3x8ff38avcou","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba92z004p3x8fn8j8y55z"},{"post_id":"ciixba930004q3x8fv69ihm4a","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba930004s3x8f0i6euxq9"},{"post_id":"ciixba930004q3x8fv69ihm4a","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba930004t3x8f70uj0ym3"},{"post_id":"ciixba931004u3x8fianviynf","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba932004w3x8fw64gvwm5"},{"post_id":"ciixba932004x3x8fq7entvla","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93300503x8febfzbmor"},{"post_id":"ciixba932004x3x8fq7entvla","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba93300513x8f565e150h"},{"post_id":"ciixba932004x3x8fq7entvla","tag_id":"ciixba933004z3x8f8wwaio4e","_id":"ciixba93300523x8f1iakdoxd"},{"post_id":"ciixba93400533x8fvzmqtrc2","tag_id":"ciixba93400553x8fv22xczhm","_id":"ciixba93500563x8f1d68a3x5"},{"post_id":"ciixba93500573x8flv87dcot","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba936005a3x8fwa7cmwif"},{"post_id":"ciixba93500573x8flv87dcot","tag_id":"ciixba93600593x8fus88f61l","_id":"ciixba936005b3x8f20goqq64"},{"post_id":"ciixba93500573x8flv87dcot","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba936005c3x8f29eyxtzm"},{"post_id":"ciixba937005d3x8f3jdsygqk","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba937005f3x8fskpmxo36"},{"post_id":"ciixba937005d3x8f3jdsygqk","tag_id":"ciixba93600593x8fus88f61l","_id":"ciixba938005g3x8fnkt05qkc"},{"post_id":"ciixba937005d3x8f3jdsygqk","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba938005h3x8flaouc3m9"},{"post_id":"ciixba938005i3x8fsucb768a","tag_id":"ciixba939005k3x8fq4dpndn3","_id":"ciixba939005l3x8fygzv4631"},{"post_id":"ciixba93a005m3x8f7rgkta39","tag_id":"ciixba93a005o3x8f7zlr3j81","_id":"ciixba93b005q3x8forz4tckl"},{"post_id":"ciixba93a005m3x8f7rgkta39","tag_id":"ciixba939005k3x8fq4dpndn3","_id":"ciixba93c005r3x8fdfemc6ny"},{"post_id":"ciixba93a005m3x8f7rgkta39","tag_id":"ciixba93b005p3x8fbfi84j5d","_id":"ciixba93c005s3x8f60hsg5op"},{"post_id":"ciixba93c005t3x8ff9z19p5v","tag_id":"ciixba93a005o3x8f7zlr3j81","_id":"ciixba93d005v3x8f60xy0neb"},{"post_id":"ciixba93c005t3x8ff9z19p5v","tag_id":"ciixba939005k3x8fq4dpndn3","_id":"ciixba93e005w3x8fr7o77nsr"},{"post_id":"ciixba93c005t3x8ff9z19p5v","tag_id":"ciixba93b005p3x8fbfi84j5d","_id":"ciixba93e005x3x8fjra9j1hw"},{"post_id":"ciixba93e005y3x8f1b14ubcz","tag_id":"ciixba93f00603x8fssba09sv","_id":"ciixba93g00623x8f82m5rcyw"},{"post_id":"ciixba93e005y3x8f1b14ubcz","tag_id":"ciixba93g00613x8f1511056d","_id":"ciixba93g00633x8f4uauhikb"},{"post_id":"ciixba93e005y3x8f1b14ubcz","tag_id":"ciixba92e00383x8f6226fige","_id":"ciixba93g00643x8fqjtiyj6m"},{"post_id":"ciixba93h00653x8fvpecf1l8","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba93i00673x8f7xmp8x8b"},{"post_id":"ciixba93j00683x8f4x6awurv","tag_id":"ciixba92e00383x8f6226fige","_id":"ciixba93j006a3x8frpxai7q3"},{"post_id":"ciixba93k006b3x8fvwoogfg3","tag_id":"ciixba92e00383x8f6226fige","_id":"ciixba93l006d3x8fm2gb5alf"},{"post_id":"ciixba93l006e3x8f72kgd4iy","tag_id":"ciixba92e00383x8f6226fige","_id":"ciixba93m006g3x8fit2bdx66"},{"post_id":"ciixba93n006h3x8f2zj104f0","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93n006j3x8fo442j5a9"},{"post_id":"ciixba93n006h3x8f2zj104f0","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba93n006k3x8f3bevjpct"},{"post_id":"ciixba93o006l3x8fdo11rek9","tag_id":"ciixba91m001b3x8fjfc0aca9","_id":"ciixba93p006n3x8f4ygbb798"},{"post_id":"ciixba93q006o3x8fq7frllex","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93q006q3x8f1smgfihe"},{"post_id":"ciixba93q006o3x8fq7frllex","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba93q006r3x8f9f1czazi"},{"post_id":"ciixba93r006s3x8f6xj4ufr4","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93s006v3x8f5m63q8sj"},{"post_id":"ciixba93r006s3x8f6xj4ufr4","tag_id":"ciixba93s006u3x8fc6gkj9lb","_id":"ciixba93s006w3x8f7oxilir7"},{"post_id":"ciixba93r006s3x8f6xj4ufr4","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba93s006x3x8frgnp3lgm"},{"post_id":"ciixba93s006y3x8f95j31wof","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93t00703x8flhw6596q"},{"post_id":"ciixba93s006y3x8f95j31wof","tag_id":"ciixba92z004n3x8fozwpo32h","_id":"ciixba93t00713x8fbq8r6wwj"},{"post_id":"ciixba93u00723x8fln18ucz0","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93v00753x8fcn8bkfn0"},{"post_id":"ciixba93u00723x8fln18ucz0","tag_id":"ciixba93v00743x8fsd0cpnkm","_id":"ciixba93v00763x8fcv2aphtb"},{"post_id":"ciixba93v00773x8f3d5pi7bm","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93w00793x8f6hb8lti6"},{"post_id":"ciixba93v00773x8f3d5pi7bm","tag_id":"ciixba93v00743x8fsd0cpnkm","_id":"ciixba93w007a3x8fqikqcumq"},{"post_id":"ciixba93x007b3x8fx9pac8zx","tag_id":"ciixba92z004m3x8fgv3wae2h","_id":"ciixba93z007d3x8f8kshm0m5"},{"post_id":"ciixba93x007b3x8fx9pac8zx","tag_id":"ciixba93v00743x8fsd0cpnkm","_id":"ciixba93z007e3x8fzn0hycxs"},{"post_id":"ciixba940007f3x8f9e6iz1zo","tag_id":"ciixba93a005o3x8f7zlr3j81","_id":"ciixba943007h3x8f6lv2eemm"},{"post_id":"ciixba943007i3x8fvxjoblvh","tag_id":"ciixba93a005o3x8f7zlr3j81","_id":"ciixba944007k3x8frvv2ten3"},{"post_id":"ciixba945007l3x8fbpb2zu84","tag_id":"ciixba93a005o3x8f7zlr3j81","_id":"ciixba946007n3x8f5tzroj98"},{"post_id":"ciixba946007o3x8fg1rmkigw","tag_id":"ciixba947007q3x8fj4511a4q","_id":"ciixba947007r3x8f5md057l6"},{"post_id":"ciixba948007s3x8f26bz42ly","tag_id":"ciixba948007u3x8fuocjfgav","_id":"ciixba949007v3x8ftbqonr91"},{"post_id":"ciixba949007w3x8f975lu7w9","tag_id":"ciixba91y002e3x8fjrkcko5g","_id":"ciixba94a007y3x8fcxnratn9"},{"post_id":"ciixba94b007z3x8fdktiw8rl","tag_id":"ciixba94c00813x8fbe301oov","_id":"ciixba94c00823x8fnqeb6w3c"},{"post_id":"ciixba94b007z3x8fdktiw8rl","tag_id":"ciixba93a005o3x8f7zlr3j81","_id":"ciixba94c00833x8fyjco118r"},{"post_id":"ciixba94d00843x8f8wel8e9n","tag_id":"ciixba91d000n3x8fmq26zo9v","_id":"ciixba94e00863x8fpfrqm32e"},{"post_id":"ciixba94e00873x8fu11d1qfe","tag_id":"ciixba94g00893x8fpuoa2bhf","_id":"ciixba94g008a3x8fmlmojgd4"}],"Tag":[{"name":"SSH","_id":"ciixba91600043x8f5ijo8oj8"},{"name":"Docker","_id":"ciixba91900093x8ffskezy4o"},{"name":"Docker plugins","_id":"ciixba91a000a3x8f1pobfwy5"},{"name":"Docker volume","_id":"ciixba91a000b3x8fkhmimyer"},{"name":"Docker Compose","_id":"ciixba91b000h3x8flzrs53b3"},{"name":"Java","_id":"ciixba91d000n3x8fmq26zo9v"},{"name":"Network","_id":"ciixba91d000o3x8fwcf9grgy"},{"name":"源码","_id":"ciixba91f000t3x8fss4f3gth"},{"name":"GIT","_id":"ciixba91i00113x8fnpq7jvbo"},{"name":"CSS","_id":"ciixba91l00153x8fo93tjdap"},{"name":"Frontend","_id":"ciixba91l00163x8f3zhpt33o"},{"name":"JVM","_id":"ciixba91m001b3x8fjfc0aca9"},{"name":"OOM Killer","_id":"ciixba91n001c3x8frcbp391b"},{"name":"Remote","_id":"ciixba91o001i3x8f5r9gq9di"},{"name":"书摘","_id":"ciixba91o001j3x8f3alyym3m"},{"name":"生活","_id":"ciixba91p001l3x8f6edzea6p"},{"name":"Mina","_id":"ciixba91r001r3x8fmkmwazyu"},{"name":"Netty","_id":"ciixba91r001s3x8fqtqkwhla"},{"name":"线程模型","_id":"ciixba91t001z3x8f7ktbb5lr"},{"name":"REWORK","_id":"ciixba91v00263x8fx9e8mcba"},{"name":"37signals","_id":"ciixba91w00273x8f3so2wykg"},{"name":"价值观","_id":"ciixba91y002e3x8fjrkcko5g"},{"name":"双11","_id":"ciixba927002t3x8fzm0w2gml"},{"name":"Solr","_id":"ciixba92b00313x8ff86vi09r"},{"name":"数据结构","_id":"ciixba92e00383x8f6226fige"},{"name":"Maven","_id":"ciixba92h003h3x8fgqm6b6sx"},{"name":"Linux","_id":"ciixba92n003w3x8fntic5ak2"},{"name":"Shell","_id":"ciixba92o003x3x8ffydwvq6l"},{"name":"ZooKeeper","_id":"ciixba92x004i3x8f97ot366l"},{"name":"Hadoop","_id":"ciixba92z004m3x8fgv3wae2h"},{"name":"MapReduce","_id":"ciixba92z004n3x8fozwpo32h"},{"name":"YARN","_id":"ciixba933004z3x8f8wwaio4e"},{"name":"Google","_id":"ciixba93400553x8fv22xczhm"},{"name":"Hadoop Pipes","_id":"ciixba93600593x8fus88f61l"},{"name":"自然语言处理","_id":"ciixba939005k3x8fq4dpndn3"},{"name":"算法","_id":"ciixba93a005o3x8f7zlr3j81"},{"name":"语义","_id":"ciixba93b005p3x8fbfi84j5d"},{"name":"NoSQL","_id":"ciixba93f00603x8fssba09sv"},{"name":"Redis","_id":"ciixba93g00613x8f1511056d"},{"name":"Haloop","_id":"ciixba93s006u3x8fc6gkj9lb"},{"name":"RPC","_id":"ciixba93v00743x8fsd0cpnkm"},{"name":"Python","_id":"ciixba947007q3x8fj4511a4q"},{"name":"分布式","_id":"ciixba948007u3x8fuocjfgav"},{"name":"电影","_id":"ciixba94c00813x8fbe301oov"},{"name":"英语","_id":"ciixba94g00893x8fpuoa2bhf"}]}}